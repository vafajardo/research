{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (13.5, 13.5) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Frey Faces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/andrei/ml/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = scipy.io.loadmat(data_path + 'frey_rawface.mat', squeeze_me=True, struct_as_record=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for later convenience\n",
    "img_rows, img_cols = 28, 20\n",
    "ff = ff[\"ff\"].T\n",
    "# .T.reshape((-1, img_rows, img_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965, 560)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Frey Faces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://dohmatob.github.io/research/2016/10/22/VAE.html\n",
    "def show_examples(data, n=None, n_cols=20, thumbnail_cb=None):\n",
    "    if n is None:\n",
    "        n = len(data)    \n",
    "    n_rows = int(np.ceil(n / float(n_cols)))\n",
    "    figure = np.zeros((img_rows * n_rows, img_cols * n_cols))\n",
    "    for k, x in enumerate(data[:n]):\n",
    "        r = k // n_cols\n",
    "        c = k % n_cols\n",
    "        figure[r * img_rows: (r + 1) * img_rows,\n",
    "               c * img_cols: (c + 1) * img_cols] = x\n",
    "        if thumbnail_cb is not None:\n",
    "            thumbnail_cb(locals())\n",
    "        \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(figure)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  81.,  136.,  167., ...,  208.,   84.,  103.],\n",
       "       [ 105.,  139.,  165., ...,  224.,  144.,   74.],\n",
       "       [  56.,  126.,  161., ...,  225.,  180.,   98.],\n",
       "       ..., \n",
       "       [ 204.,  160.,  126., ...,  149.,  141.,  140.],\n",
       "       [ 152.,  154.,  129., ...,  159.,  137.,  141.],\n",
       "       [  93.,  144.,  138., ...,  171.,  171.,  180.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a21080eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_examples(ff, n=100, n_cols=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_reader(object):\n",
    "    \n",
    "    def __init__(self,X,batch_size=1):\n",
    "        self.X = X\n",
    "        self.num_examples = X.shape[0]\n",
    "        self.batch_number = 0 \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "    \n",
    "    def next_batch(self):\n",
    "        low_ix = self.batch_number*self.batch_size \n",
    "        up_ix = (self.batch_number + 1)*self.batch_size\n",
    "        if up_ix >= self.X.shape[0]:\n",
    "            up_ix = self.X.shape[0]\n",
    "            self.batch_number = 0 # reset batch_number to zero\n",
    "        else:\n",
    "            self.batch_number = self.batch_number + 1\n",
    "        return self.X[low_ix:up_ix,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "ff_reader = data_reader(ff, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_reader.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1965"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_reader.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196500"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50890.58524173028"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e8 / 1965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction phase\n",
    "n_inputs = 28*20\n",
    "n_hidden1 = 200\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 20 # codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected],\n",
    "        activation_fn = tf.nn.elu,\n",
    "        weights_initializer = tf.contrib.layers.variance_scaling_initializer()):\n",
    "    X = tf.placeholder(tf.float32, shape = [None, n_inputs])\n",
    "    hidden1 = fully_connected(X, n_hidden1)\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, activation_fn=tf.nn.tanh)\n",
    "    hidden2_mean = fully_connected(hidden1, n_hidden2)\n",
    "    hidden3_mean = fully_connected(hidden2, n_hidden3, activation_fn=None)\n",
    "    hidden3_gamma = fully_connected(hidden2, n_hidden3, activation_fn=None)\n",
    "    hidden3_sigma = tf.exp(0.5 * hidden3_gamma)\n",
    "    noise1 = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)\n",
    "    hidden3 = hidden3_mean + hidden3_sigma * noise1\n",
    "    hidden4 = fully_connected(hidden3, n_hidden4)\n",
    "    hidden5 = fully_connected(hidden4, n_hidden5, activation_fn=tf.nn.tanh)\n",
    "    hidden6_mean = fully_connected(hidden5, n_outputs, activation_fn=None)\n",
    "    hidden6_gamma = fully_connected(hidden5, n_outputs, activation_fn=None)\n",
    "    hidden6_sigma = tf.exp(0.5 * hidden6_gamma)\n",
    "    noise2 = tf.random_normal(tf.shape(hidden6_sigma), dtype=tf.float32)\n",
    "    outputs = hidden6_mean + hidden6_sigma * noise2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "    tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)\n",
    "\n",
    "reconstruction_loss = 0.5 * tf.reduce_sum(tf.square((X - hidden6_mean) / (eps + tf.exp(hidden6_gamma)))\n",
    "                                  + tf.log(2*np.pi) + hidden6_gamma)\n",
    "cost = reconstruction_loss + latent_loss\n",
    "# display_cost = tf.reduce_mean(reconstruction_loss) + tf.reduce_mean(latent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train total loss: 104526.0 \tReconstruction loss: 104525.0 \tLatent loss: 1.26524\n",
      "1 Train total loss: 51553.2 \tReconstruction loss: 51551.8 \tLatent loss: 1.39176\n",
      "2 Train total loss: 30836.0 \tReconstruction loss: 30834.5 \tLatent loss: 1.50664\n",
      "3 Train total loss: 21107.3 \tReconstruction loss: 21105.6 \tLatent loss: 1.689\n",
      "4 Train total loss: 13683.8 \tReconstruction loss: 13682.0 \tLatent loss: 1.76517\n",
      "5 Train total loss: 10822.8 \tReconstruction loss: 10820.9 \tLatent loss: 1.9505\n",
      "6 Train total loss: 8354.94 \tReconstruction loss: 8352.92 \tLatent loss: 2.02174\n",
      "7 Train total loss: 7115.53 \tReconstruction loss: 7113.49 \tLatent loss: 2.04047\n",
      "8 Train total loss: 5779.65 \tReconstruction loss: 5777.43 \tLatent loss: 2.21212\n",
      "9 Train total loss: 4995.61 \tReconstruction loss: 4993.41 \tLatent loss: 2.20239\n",
      "10 Train total loss: 4456.75 \tReconstruction loss: 4454.52 \tLatent loss: 2.23322\n",
      "11 Train total loss: 3583.04 \tReconstruction loss: 3580.7 \tLatent loss: 2.34063\n",
      "12 Train total loss: 3509.67 \tReconstruction loss: 3507.3 \tLatent loss: 2.36485\n",
      "13 Train total loss: 3489.35 \tReconstruction loss: 3486.91 \tLatent loss: 2.4351\n",
      "14 Train total loss: 3199.9 \tReconstruction loss: 3197.44 \tLatent loss: 2.45512\n",
      "15 Train total loss: 2536.7 \tReconstruction loss: 2534.13 \tLatent loss: 2.57118\n",
      "16 Train total loss: 2261.98 \tReconstruction loss: 2259.34 \tLatent loss: 2.63841\n",
      "17 Train total loss: 2114.64 \tReconstruction loss: 2112.03 \tLatent loss: 2.61257\n",
      "18 Train total loss: 1891.26 \tReconstruction loss: 1888.61 \tLatent loss: 2.6508\n",
      "19 Train total loss: 1798.91 \tReconstruction loss: 1796.28 \tLatent loss: 2.62829\n",
      "20 Train total loss: 1827.01 \tReconstruction loss: 1824.4 \tLatent loss: 2.61705\n",
      "21 Train total loss: 1598.13 \tReconstruction loss: 1595.44 \tLatent loss: 2.69349\n",
      "22 Train total loss: 1507.88 \tReconstruction loss: 1505.19 \tLatent loss: 2.69757\n",
      "23 Train total loss: 1412.31 \tReconstruction loss: 1409.58 \tLatent loss: 2.73797\n",
      "24 Train total loss: 1287.61 \tReconstruction loss: 1284.91 \tLatent loss: 2.69737\n",
      "25 Train total loss: 1239.58 \tReconstruction loss: 1236.84 \tLatent loss: 2.74788\n",
      "26 Train total loss: 1096.08 \tReconstruction loss: 1093.26 \tLatent loss: 2.81878\n",
      "27 Train total loss: 1102.99 \tReconstruction loss: 1100.17 \tLatent loss: 2.81371\n",
      "28 Train total loss: 1020.52 \tReconstruction loss: 1017.63 \tLatent loss: 2.88305\n",
      "29 Train total loss: 985.44 \tReconstruction loss: 982.557 \tLatent loss: 2.88315\n",
      "30 Train total loss: 958.908 \tReconstruction loss: 956.094 \tLatent loss: 2.81447\n",
      "31 Train total loss: 851.626 \tReconstruction loss: 848.774 \tLatent loss: 2.85127\n",
      "32 Train total loss: 873.554 \tReconstruction loss: 870.678 \tLatent loss: 2.87527\n",
      "33 Train total loss: 933.116 \tReconstruction loss: 930.26 \tLatent loss: 2.85569\n",
      "34 Train total loss: 896.944 \tReconstruction loss: 894.051 \tLatent loss: 2.89251\n",
      "35 Train total loss: 748.623 \tReconstruction loss: 745.65 \tLatent loss: 2.97281\n",
      "36 Train total loss: 744.365 \tReconstruction loss: 741.368 \tLatent loss: 2.99688\n",
      "37 Train total loss: 706.131 \tReconstruction loss: 703.154 \tLatent loss: 2.97733\n",
      "38 Train total loss: 660.636 \tReconstruction loss: 657.627 \tLatent loss: 3.00899\n",
      "39 Train total loss: 665.554 \tReconstruction loss: 662.593 \tLatent loss: 2.96043\n",
      "40 Train total loss: 676.941 \tReconstruction loss: 674.001 \tLatent loss: 2.94061\n",
      "41 Train total loss: 619.75 \tReconstruction loss: 616.725 \tLatent loss: 3.025\n",
      "42 Train total loss: 593.838 \tReconstruction loss: 590.813 \tLatent loss: 3.02473\n",
      "43 Train total loss: 572.531 \tReconstruction loss: 569.499 \tLatent loss: 3.03211\n",
      "44 Train total loss: 524.535 \tReconstruction loss: 521.546 \tLatent loss: 2.98892\n",
      "45 Train total loss: 516.722 \tReconstruction loss: 513.694 \tLatent loss: 3.02789\n",
      "46 Train total loss: 469.766 \tReconstruction loss: 466.682 \tLatent loss: 3.08378\n",
      "47 Train total loss: 482.588 \tReconstruction loss: 479.511 \tLatent loss: 3.077\n",
      "48 Train total loss: 464.501 \tReconstruction loss: 461.389 \tLatent loss: 3.11119\n",
      "49 Train total loss: 455.994 \tReconstruction loss: 452.884 \tLatent loss: 3.10988\n",
      "50 Train total loss: 429.099 \tReconstruction loss: 426.054 \tLatent loss: 3.04526\n",
      "51 Train total loss: 411.553 \tReconstruction loss: 408.481 \tLatent loss: 3.07258\n",
      "52 Train total loss: 415.907 \tReconstruction loss: 412.818 \tLatent loss: 3.08942\n",
      "53 Train total loss: 450.897 \tReconstruction loss: 447.838 \tLatent loss: 3.05869\n",
      "54 Train total loss: 447.557 \tReconstruction loss: 444.471 \tLatent loss: 3.08565\n",
      "55 Train total loss: 385.014 \tReconstruction loss: 381.851 \tLatent loss: 3.16302\n",
      "56 Train total loss: 379.518 \tReconstruction loss: 376.348 \tLatent loss: 3.16962\n",
      "57 Train total loss: 371.987 \tReconstruction loss: 368.827 \tLatent loss: 3.16001\n",
      "58 Train total loss: 346.299 \tReconstruction loss: 343.117 \tLatent loss: 3.18237\n",
      "59 Train total loss: 349.956 \tReconstruction loss: 346.812 \tLatent loss: 3.14487\n",
      "60 Train total loss: 358.584 \tReconstruction loss: 355.475 \tLatent loss: 3.10853\n",
      "61 Train total loss: 337.018 \tReconstruction loss: 333.833 \tLatent loss: 3.18527\n",
      "62 Train total loss: 331.358 \tReconstruction loss: 328.161 \tLatent loss: 3.197\n",
      "63 Train total loss: 317.366 \tReconstruction loss: 314.171 \tLatent loss: 3.19429\n",
      "64 Train total loss: 298.939 \tReconstruction loss: 295.79 \tLatent loss: 3.14902\n",
      "65 Train total loss: 292.141 \tReconstruction loss: 288.954 \tLatent loss: 3.18677\n",
      "66 Train total loss: 269.389 \tReconstruction loss: 266.155 \tLatent loss: 3.23352\n",
      "67 Train total loss: 277.994 \tReconstruction loss: 274.778 \tLatent loss: 3.21569\n",
      "68 Train total loss: 272.492 \tReconstruction loss: 269.252 \tLatent loss: 3.24033\n",
      "69 Train total loss: 265.185 \tReconstruction loss: 261.939 \tLatent loss: 3.24541\n",
      "70 Train total loss: 257.993 \tReconstruction loss: 254.81 \tLatent loss: 3.18287\n",
      "71 Train total loss: 243.804 \tReconstruction loss: 240.608 \tLatent loss: 3.19552\n",
      "72 Train total loss: 252.829 \tReconstruction loss: 249.607 \tLatent loss: 3.22174\n",
      "73 Train total loss: 270.981 \tReconstruction loss: 267.803 \tLatent loss: 3.17804\n",
      "74 Train total loss: 276.427 \tReconstruction loss: 273.222 \tLatent loss: 3.20473\n",
      "75 Train total loss: 234.816 \tReconstruction loss: 231.539 \tLatent loss: 3.27642\n",
      "76 Train total loss: 235.255 \tReconstruction loss: 231.975 \tLatent loss: 3.27966\n",
      "77 Train total loss: 228.714 \tReconstruction loss: 225.447 \tLatent loss: 3.26707\n",
      "78 Train total loss: 216.553 \tReconstruction loss: 213.261 \tLatent loss: 3.29231\n",
      "79 Train total loss: 222.342 \tReconstruction loss: 219.078 \tLatent loss: 3.26347\n",
      "80 Train total loss: 229.346 \tReconstruction loss: 226.129 \tLatent loss: 3.21724\n",
      "81 Train total loss: 213.585 \tReconstruction loss: 210.298 \tLatent loss: 3.28629\n",
      "82 Train total loss: 207.279 \tReconstruction loss: 203.98 \tLatent loss: 3.29909\n",
      "83 Train total loss: 205.867 \tReconstruction loss: 202.573 \tLatent loss: 3.29365\n",
      "84 Train total loss: 190.839 \tReconstruction loss: 187.575 \tLatent loss: 3.26416\n",
      "85 Train total loss: 193.091 \tReconstruction loss: 189.806 \tLatent loss: 3.28478\n",
      "86 Train total loss: 175.953 \tReconstruction loss: 172.626 \tLatent loss: 3.32693\n",
      "87 Train total loss: 185.621 \tReconstruction loss: 182.307 \tLatent loss: 3.31423\n",
      "88 Train total loss: 179.176 \tReconstruction loss: 175.846 \tLatent loss: 3.32989\n",
      "89 Train total loss: 177.286 \tReconstruction loss: 173.946 \tLatent loss: 3.33945\n",
      "90 Train total loss: 169.638 \tReconstruction loss: 166.362 \tLatent loss: 3.27586\n",
      "91 Train total loss: 163.776 \tReconstruction loss: 160.494 \tLatent loss: 3.282\n",
      "92 Train total loss: 172.437 \tReconstruction loss: 169.124 \tLatent loss: 3.31222\n",
      "93 Train total loss: 182.928 \tReconstruction loss: 179.664 \tLatent loss: 3.26394\n",
      "94 Train total loss: 184.574 \tReconstruction loss: 181.283 \tLatent loss: 3.2911\n",
      "95 Train total loss: 162.551 \tReconstruction loss: 159.194 \tLatent loss: 3.35679\n",
      "96 Train total loss: 162.217 \tReconstruction loss: 158.858 \tLatent loss: 3.35862\n",
      "97 Train total loss: 158.549 \tReconstruction loss: 155.202 \tLatent loss: 3.34695\n",
      "98 Train total loss: 148.5 \tReconstruction loss: 145.131 \tLatent loss: 3.36909\n",
      "99 Train total loss: 152.528 \tReconstruction loss: 149.185 \tLatent loss: 3.34237\n",
      "100 Train total loss: 156.487 \tReconstruction loss: 153.193 \tLatent loss: 3.29424\n",
      "101 Train total loss: 147.173 \tReconstruction loss: 143.812 \tLatent loss: 3.36096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 Train total loss: 143.109 \tReconstruction loss: 139.738 \tLatent loss: 3.37107\n",
      "103 Train total loss: 143.724 \tReconstruction loss: 140.359 \tLatent loss: 3.36491\n",
      "104 Train total loss: 133.331 \tReconstruction loss: 129.988 \tLatent loss: 3.34311\n",
      "105 Train total loss: 134.938 \tReconstruction loss: 131.584 \tLatent loss: 3.3539\n",
      "106 Train total loss: 126.835 \tReconstruction loss: 123.44 \tLatent loss: 3.39522\n",
      "107 Train total loss: 131.933 \tReconstruction loss: 128.545 \tLatent loss: 3.3886\n",
      "108 Train total loss: 127.368 \tReconstruction loss: 123.972 \tLatent loss: 3.39628\n",
      "109 Train total loss: 125.269 \tReconstruction loss: 121.863 \tLatent loss: 3.40615\n",
      "110 Train total loss: 122.384 \tReconstruction loss: 119.04 \tLatent loss: 3.3436\n",
      "111 Train total loss: 117.288 \tReconstruction loss: 113.942 \tLatent loss: 3.34674\n",
      "112 Train total loss: 123.85 \tReconstruction loss: 120.472 \tLatent loss: 3.37781\n",
      "113 Train total loss: 130.161 \tReconstruction loss: 126.831 \tLatent loss: 3.33052\n",
      "114 Train total loss: 131.87 \tReconstruction loss: 128.507 \tLatent loss: 3.36256\n",
      "115 Train total loss: 116.463 \tReconstruction loss: 113.044 \tLatent loss: 3.41909\n",
      "116 Train total loss: 117.486 \tReconstruction loss: 114.066 \tLatent loss: 3.4198\n",
      "117 Train total loss: 114.145 \tReconstruction loss: 110.738 \tLatent loss: 3.40782\n",
      "118 Train total loss: 108.74 \tReconstruction loss: 105.311 \tLatent loss: 3.42854\n",
      "119 Train total loss: 111.328 \tReconstruction loss: 107.925 \tLatent loss: 3.40352\n",
      "120 Train total loss: 115.285 \tReconstruction loss: 111.93 \tLatent loss: 3.35536\n",
      "121 Train total loss: 108.604 \tReconstruction loss: 105.185 \tLatent loss: 3.41862\n",
      "122 Train total loss: 105.338 \tReconstruction loss: 101.911 \tLatent loss: 3.42688\n",
      "123 Train total loss: 104.816 \tReconstruction loss: 101.392 \tLatent loss: 3.42468\n",
      "124 Train total loss: 99.0115 \tReconstruction loss: 95.6081 \tLatent loss: 3.4034\n",
      "125 Train total loss: 98.4594 \tReconstruction loss: 95.052 \tLatent loss: 3.40736\n",
      "126 Train total loss: 92.5556 \tReconstruction loss: 89.1043 \tLatent loss: 3.45128\n",
      "127 Train total loss: 96.6186 \tReconstruction loss: 93.1732 \tLatent loss: 3.44533\n",
      "128 Train total loss: 95.074 \tReconstruction loss: 91.6262 \tLatent loss: 3.44783\n",
      "129 Train total loss: 93.3688 \tReconstruction loss: 89.9104 \tLatent loss: 3.45838\n",
      "130 Train total loss: 90.9214 \tReconstruction loss: 87.5198 \tLatent loss: 3.40167\n",
      "131 Train total loss: 87.5848 \tReconstruction loss: 84.1877 \tLatent loss: 3.39712\n",
      "132 Train total loss: 91.2115 \tReconstruction loss: 87.7796 \tLatent loss: 3.43189\n",
      "133 Train total loss: 97.8103 \tReconstruction loss: 94.4278 \tLatent loss: 3.38254\n",
      "134 Train total loss: 99.4675 \tReconstruction loss: 96.0543 \tLatent loss: 3.41315\n",
      "135 Train total loss: 88.5394 \tReconstruction loss: 85.0719 \tLatent loss: 3.46745\n",
      "136 Train total loss: 88.9888 \tReconstruction loss: 85.5214 \tLatent loss: 3.46739\n",
      "137 Train total loss: 86.5951 \tReconstruction loss: 83.1387 \tLatent loss: 3.45647\n",
      "138 Train total loss: 82.8099 \tReconstruction loss: 79.3361 \tLatent loss: 3.47377\n",
      "139 Train total loss: 84.3304 \tReconstruction loss: 80.8775 \tLatent loss: 3.45287\n",
      "140 Train total loss: 86.6629 \tReconstruction loss: 83.2588 \tLatent loss: 3.4041\n",
      "141 Train total loss: 82.2991 \tReconstruction loss: 78.8344 \tLatent loss: 3.46471\n",
      "142 Train total loss: 81.1292 \tReconstruction loss: 77.6574 \tLatent loss: 3.47185\n",
      "143 Train total loss: 79.6909 \tReconstruction loss: 76.2209 \tLatent loss: 3.47002\n",
      "144 Train total loss: 74.8604 \tReconstruction loss: 71.4087 \tLatent loss: 3.45176\n",
      "145 Train total loss: 75.5457 \tReconstruction loss: 72.0961 \tLatent loss: 3.44959\n",
      "146 Train total loss: 71.5801 \tReconstruction loss: 68.0868 \tLatent loss: 3.49326\n",
      "147 Train total loss: 74.5978 \tReconstruction loss: 71.1107 \tLatent loss: 3.4871\n",
      "148 Train total loss: 73.2442 \tReconstruction loss: 69.7563 \tLatent loss: 3.48791\n",
      "149 Train total loss: 72.6247 \tReconstruction loss: 69.1249 \tLatent loss: 3.49973\n",
      "150 Train total loss: 69.7945 \tReconstruction loss: 66.3511 \tLatent loss: 3.44339\n",
      "151 Train total loss: 67.2506 \tReconstruction loss: 63.813 \tLatent loss: 3.43752\n",
      "152 Train total loss: 70.8073 \tReconstruction loss: 67.3349 \tLatent loss: 3.47239\n",
      "153 Train total loss: 76.0957 \tReconstruction loss: 72.6758 \tLatent loss: 3.41995\n",
      "154 Train total loss: 76.4678 \tReconstruction loss: 73.0174 \tLatent loss: 3.45043\n",
      "155 Train total loss: 68.4126 \tReconstruction loss: 64.9079 \tLatent loss: 3.50471\n",
      "156 Train total loss: 68.4848 \tReconstruction loss: 64.9805 \tLatent loss: 3.50434\n",
      "157 Train total loss: 67.1376 \tReconstruction loss: 63.6437 \tLatent loss: 3.49388\n",
      "158 Train total loss: 64.4199 \tReconstruction loss: 60.9115 \tLatent loss: 3.50847\n",
      "159 Train total loss: 66.2645 \tReconstruction loss: 62.775 \tLatent loss: 3.48945\n",
      "160 Train total loss: 67.6721 \tReconstruction loss: 64.2296 \tLatent loss: 3.44249\n",
      "161 Train total loss: 64.4161 \tReconstruction loss: 60.9148 \tLatent loss: 3.50134\n",
      "162 Train total loss: 63.5082 \tReconstruction loss: 60.0021 \tLatent loss: 3.50611\n",
      "163 Train total loss: 63.0745 \tReconstruction loss: 59.5708 \tLatent loss: 3.50362\n",
      "164 Train total loss: 58.7826 \tReconstruction loss: 55.2969 \tLatent loss: 3.48568\n",
      "165 Train total loss: 59.5438 \tReconstruction loss: 56.061 \tLatent loss: 3.48276\n",
      "166 Train total loss: 56.451 \tReconstruction loss: 52.9256 \tLatent loss: 3.52536\n",
      "167 Train total loss: 58.7214 \tReconstruction loss: 55.2034 \tLatent loss: 3.51796\n",
      "168 Train total loss: 58.0382 \tReconstruction loss: 54.52 \tLatent loss: 3.51825\n",
      "169 Train total loss: 57.0487 \tReconstruction loss: 53.518 \tLatent loss: 3.53079\n",
      "170 Train total loss: 55.1244 \tReconstruction loss: 51.6524 \tLatent loss: 3.47204\n",
      "171 Train total loss: 53.7793 \tReconstruction loss: 50.3101 \tLatent loss: 3.46922\n",
      "172 Train total loss: 56.1287 \tReconstruction loss: 52.6271 \tLatent loss: 3.50156\n",
      "173 Train total loss: 59.9038 \tReconstruction loss: 56.456 \tLatent loss: 3.44778\n",
      "174 Train total loss: 60.9772 \tReconstruction loss: 57.4983 \tLatent loss: 3.47889\n",
      "175 Train total loss: 54.3976 \tReconstruction loss: 50.8658 \tLatent loss: 3.53182\n",
      "176 Train total loss: 54.8784 \tReconstruction loss: 51.3469 \tLatent loss: 3.53145\n",
      "177 Train total loss: 53.5198 \tReconstruction loss: 49.9994 \tLatent loss: 3.52038\n",
      "178 Train total loss: 51.4458 \tReconstruction loss: 47.9104 \tLatent loss: 3.53531\n",
      "179 Train total loss: 52.7434 \tReconstruction loss: 49.227 \tLatent loss: 3.51642\n",
      "180 Train total loss: 53.706 \tReconstruction loss: 50.2354 \tLatent loss: 3.47064\n",
      "181 Train total loss: 51.9472 \tReconstruction loss: 48.4196 \tLatent loss: 3.52762\n",
      "182 Train total loss: 50.7168 \tReconstruction loss: 47.1851 \tLatent loss: 3.53163\n",
      "183 Train total loss: 50.4201 \tReconstruction loss: 46.892 \tLatent loss: 3.52807\n",
      "184 Train total loss: 46.8768 \tReconstruction loss: 43.3656 \tLatent loss: 3.51121\n",
      "185 Train total loss: 47.5533 \tReconstruction loss: 44.0466 \tLatent loss: 3.50668\n",
      "186 Train total loss: 45.3931 \tReconstruction loss: 41.8445 \tLatent loss: 3.54864\n",
      "187 Train total loss: 47.6698 \tReconstruction loss: 44.128 \tLatent loss: 3.54177\n",
      "188 Train total loss: 46.6947 \tReconstruction loss: 43.1546 \tLatent loss: 3.54004\n",
      "189 Train total loss: 45.9477 \tReconstruction loss: 42.3952 \tLatent loss: 3.55255\n",
      "190 Train total loss: 44.5175 \tReconstruction loss: 41.0239 \tLatent loss: 3.49357\n",
      "191 Train total loss: 43.5046 \tReconstruction loss: 40.0137 \tLatent loss: 3.49086\n",
      "192 Train total loss: 45.265 \tReconstruction loss: 41.7425 \tLatent loss: 3.52244\n",
      "193 Train total loss: 48.5031 \tReconstruction loss: 45.0339 \tLatent loss: 3.46922\n",
      "194 Train total loss: 49.3924 \tReconstruction loss: 45.893 \tLatent loss: 3.4994\n",
      "195 Train total loss: 44.4626 \tReconstruction loss: 40.9113 \tLatent loss: 3.55132\n",
      "196 Train total loss: 44.8248 \tReconstruction loss: 41.2744 \tLatent loss: 3.5504\n",
      "197 Train total loss: 43.5219 \tReconstruction loss: 39.9828 \tLatent loss: 3.53909\n",
      "198 Train total loss: 42.1158 \tReconstruction loss: 38.5619 \tLatent loss: 3.55395\n",
      "199 Train total loss: 42.779 \tReconstruction loss: 39.2442 \tLatent loss: 3.53485\n",
      "200 Train total loss: 43.9993 \tReconstruction loss: 40.5105 \tLatent loss: 3.48881\n",
      "201 Train total loss: 42.4421 \tReconstruction loss: 38.8975 \tLatent loss: 3.54464\n",
      "202 Train total loss: 41.3889 \tReconstruction loss: 37.8409 \tLatent loss: 3.54806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 Train total loss: 41.1787 \tReconstruction loss: 37.6347 \tLatent loss: 3.54402\n",
      "204 Train total loss: 38.7664 \tReconstruction loss: 35.2392 \tLatent loss: 3.52726\n",
      "205 Train total loss: 39.0923 \tReconstruction loss: 35.5707 \tLatent loss: 3.52164\n",
      "206 Train total loss: 37.3655 \tReconstruction loss: 33.8027 \tLatent loss: 3.56278\n",
      "207 Train total loss: 39.0135 \tReconstruction loss: 35.4556 \tLatent loss: 3.55782\n",
      "208 Train total loss: 38.4566 \tReconstruction loss: 34.9032 \tLatent loss: 3.55338\n",
      "209 Train total loss: 38.1087 \tReconstruction loss: 34.5436 \tLatent loss: 3.5651\n",
      "210 Train total loss: 36.8804 \tReconstruction loss: 33.3735 \tLatent loss: 3.50697\n",
      "211 Train total loss: 35.9035 \tReconstruction loss: 32.3998 \tLatent loss: 3.50371\n",
      "212 Train total loss: 37.3862 \tReconstruction loss: 33.8518 \tLatent loss: 3.53443\n",
      "213 Train total loss: 39.8638 \tReconstruction loss: 36.3806 \tLatent loss: 3.48321\n",
      "214 Train total loss: 40.6786 \tReconstruction loss: 37.1682 \tLatent loss: 3.51041\n",
      "215 Train total loss: 36.5166 \tReconstruction loss: 32.9546 \tLatent loss: 3.56193\n",
      "216 Train total loss: 36.7775 \tReconstruction loss: 33.217 \tLatent loss: 3.56047\n",
      "217 Train total loss: 35.8807 \tReconstruction loss: 32.3317 \tLatent loss: 3.54901\n",
      "218 Train total loss: 34.7225 \tReconstruction loss: 31.1586 \tLatent loss: 3.56396\n",
      "219 Train total loss: 35.4293 \tReconstruction loss: 31.8846 \tLatent loss: 3.54472\n",
      "220 Train total loss: 36.4424 \tReconstruction loss: 32.9445 \tLatent loss: 3.49791\n",
      "221 Train total loss: 35.1198 \tReconstruction loss: 31.5668 \tLatent loss: 3.55304\n",
      "222 Train total loss: 34.3186 \tReconstruction loss: 30.7637 \tLatent loss: 3.55489\n",
      "223 Train total loss: 34.2527 \tReconstruction loss: 30.7011 \tLatent loss: 3.55158\n",
      "224 Train total loss: 32.192 \tReconstruction loss: 28.6577 \tLatent loss: 3.5343\n",
      "225 Train total loss: 32.5847 \tReconstruction loss: 29.0571 \tLatent loss: 3.52767\n",
      "226 Train total loss: 31.1972 \tReconstruction loss: 27.6292 \tLatent loss: 3.56804\n",
      "227 Train total loss: 32.5283 \tReconstruction loss: 28.9644 \tLatent loss: 3.56386\n",
      "228 Train total loss: 31.9055 \tReconstruction loss: 28.3472 \tLatent loss: 3.55838\n",
      "229 Train total loss: 31.6942 \tReconstruction loss: 28.1247 \tLatent loss: 3.56949\n",
      "230 Train total loss: 30.8332 \tReconstruction loss: 27.3204 \tLatent loss: 3.5128\n",
      "231 Train total loss: 30.0537 \tReconstruction loss: 26.5459 \tLatent loss: 3.5078\n",
      "232 Train total loss: 31.2423 \tReconstruction loss: 27.7039 \tLatent loss: 3.53839\n",
      "233 Train total loss: 33.3885 \tReconstruction loss: 29.9029 \tLatent loss: 3.48565\n",
      "234 Train total loss: 33.7531 \tReconstruction loss: 30.2413 \tLatent loss: 3.51174\n",
      "235 Train total loss: 30.5565 \tReconstruction loss: 26.9938 \tLatent loss: 3.5627\n",
      "236 Train total loss: 31.066 \tReconstruction loss: 27.5043 \tLatent loss: 3.56166\n",
      "237 Train total loss: 30.1049 \tReconstruction loss: 26.5553 \tLatent loss: 3.54966\n",
      "238 Train total loss: 29.2999 \tReconstruction loss: 25.7355 \tLatent loss: 3.56442\n",
      "239 Train total loss: 29.8607 \tReconstruction loss: 26.315 \tLatent loss: 3.54571\n",
      "240 Train total loss: 30.3988 \tReconstruction loss: 26.9015 \tLatent loss: 3.49726\n",
      "241 Train total loss: 29.4026 \tReconstruction loss: 25.8516 \tLatent loss: 3.55107\n",
      "242 Train total loss: 28.8214 \tReconstruction loss: 25.2682 \tLatent loss: 3.55321\n",
      "243 Train total loss: 28.8398 \tReconstruction loss: 25.2901 \tLatent loss: 3.54972\n",
      "244 Train total loss: 27.0781 \tReconstruction loss: 23.5447 \tLatent loss: 3.53343\n",
      "245 Train total loss: 27.3272 \tReconstruction loss: 23.8024 \tLatent loss: 3.52481\n",
      "246 Train total loss: 26.3086 \tReconstruction loss: 22.7443 \tLatent loss: 3.5643\n",
      "247 Train total loss: 27.3827 \tReconstruction loss: 23.8224 \tLatent loss: 3.56034\n",
      "248 Train total loss: 26.9754 \tReconstruction loss: 23.4217 \tLatent loss: 3.55372\n",
      "249 Train total loss: 26.7653 \tReconstruction loss: 23.2015 \tLatent loss: 3.56383\n",
      "250 Train total loss: 26.0215 \tReconstruction loss: 22.5135 \tLatent loss: 3.50797\n",
      "251 Train total loss: 25.2841 \tReconstruction loss: 21.7815 \tLatent loss: 3.50258\n",
      "252 Train total loss: 26.5483 \tReconstruction loss: 23.0154 \tLatent loss: 3.53289\n",
      "253 Train total loss: 28.1128 \tReconstruction loss: 24.6346 \tLatent loss: 3.47822\n",
      "254 Train total loss: 28.5062 \tReconstruction loss: 25.0021 \tLatent loss: 3.50406\n",
      "255 Train total loss: 25.8851 \tReconstruction loss: 22.3306 \tLatent loss: 3.5545\n",
      "256 Train total loss: 26.1206 \tReconstruction loss: 22.5674 \tLatent loss: 3.55322\n",
      "257 Train total loss: 25.5128 \tReconstruction loss: 21.9721 \tLatent loss: 3.54069\n",
      "258 Train total loss: 24.8697 \tReconstruction loss: 21.3146 \tLatent loss: 3.55505\n",
      "259 Train total loss: 25.3029 \tReconstruction loss: 21.7663 \tLatent loss: 3.53662\n",
      "260 Train total loss: 25.8832 \tReconstruction loss: 22.3965 \tLatent loss: 3.48671\n",
      "261 Train total loss: 25.0622 \tReconstruction loss: 21.5224 \tLatent loss: 3.53981\n",
      "262 Train total loss: 24.7582 \tReconstruction loss: 21.2172 \tLatent loss: 3.54102\n",
      "263 Train total loss: 24.5536 \tReconstruction loss: 21.0159 \tLatent loss: 3.53769\n",
      "264 Train total loss: 23.1027 \tReconstruction loss: 19.581 \tLatent loss: 3.52173\n",
      "265 Train total loss: 23.5318 \tReconstruction loss: 20.0201 \tLatent loss: 3.5117\n",
      "266 Train total loss: 22.6133 \tReconstruction loss: 19.0629 \tLatent loss: 3.55034\n",
      "267 Train total loss: 23.4916 \tReconstruction loss: 19.9456 \tLatent loss: 3.54599\n",
      "268 Train total loss: 23.0983 \tReconstruction loss: 19.5593 \tLatent loss: 3.53903\n",
      "269 Train total loss: 22.9293 \tReconstruction loss: 19.3808 \tLatent loss: 3.54856\n",
      "270 Train total loss: 22.3839 \tReconstruction loss: 18.8912 \tLatent loss: 3.49271\n",
      "271 Train total loss: 21.8395 \tReconstruction loss: 18.3524 \tLatent loss: 3.48706\n",
      "272 Train total loss: 22.6564 \tReconstruction loss: 19.1402 \tLatent loss: 3.51622\n",
      "273 Train total loss: 24.1337 \tReconstruction loss: 20.6738 \tLatent loss: 3.45995\n",
      "274 Train total loss: 24.4879 \tReconstruction loss: 21.001 \tLatent loss: 3.48681\n",
      "275 Train total loss: 22.2291 \tReconstruction loss: 18.6929 \tLatent loss: 3.53619\n",
      "276 Train total loss: 22.531 \tReconstruction loss: 18.9966 \tLatent loss: 3.53438\n",
      "277 Train total loss: 22.0831 \tReconstruction loss: 18.5618 \tLatent loss: 3.52128\n",
      "278 Train total loss: 21.374 \tReconstruction loss: 17.8388 \tLatent loss: 3.53515\n",
      "279 Train total loss: 21.8074 \tReconstruction loss: 18.2909 \tLatent loss: 3.51646\n",
      "280 Train total loss: 22.2198 \tReconstruction loss: 18.7536 \tLatent loss: 3.46621\n",
      "281 Train total loss: 21.5857 \tReconstruction loss: 18.0681 \tLatent loss: 3.51767\n",
      "282 Train total loss: 21.1963 \tReconstruction loss: 17.6787 \tLatent loss: 3.51764\n",
      "283 Train total loss: 21.1268 \tReconstruction loss: 17.6138 \tLatent loss: 3.51296\n",
      "284 Train total loss: 20.0328 \tReconstruction loss: 16.5349 \tLatent loss: 3.49786\n",
      "285 Train total loss: 20.2986 \tReconstruction loss: 16.8109 \tLatent loss: 3.48765\n",
      "286 Train total loss: 19.5419 \tReconstruction loss: 16.0167 \tLatent loss: 3.52512\n",
      "287 Train total loss: 20.3244 \tReconstruction loss: 16.8039 \tLatent loss: 3.52047\n",
      "288 Train total loss: 20.0145 \tReconstruction loss: 16.5013 \tLatent loss: 3.51319\n",
      "289 Train total loss: 19.8545 \tReconstruction loss: 16.3325 \tLatent loss: 3.52198\n",
      "290 Train total loss: 19.4411 \tReconstruction loss: 15.9756 \tLatent loss: 3.46546\n",
      "291 Train total loss: 18.9417 \tReconstruction loss: 15.4812 \tLatent loss: 3.46053\n",
      "292 Train total loss: 19.6755 \tReconstruction loss: 16.1882 \tLatent loss: 3.48727\n",
      "293 Train total loss: 20.7763 \tReconstruction loss: 17.346 \tLatent loss: 3.43029\n",
      "294 Train total loss: 21.0365 \tReconstruction loss: 17.5786 \tLatent loss: 3.45792\n",
      "295 Train total loss: 19.3975 \tReconstruction loss: 15.8913 \tLatent loss: 3.50614\n",
      "296 Train total loss: 19.4819 \tReconstruction loss: 15.9782 \tLatent loss: 3.5037\n",
      "297 Train total loss: 19.272 \tReconstruction loss: 15.7824 \tLatent loss: 3.48963\n",
      "298 Train total loss: 18.5725 \tReconstruction loss: 15.0694 \tLatent loss: 3.50305\n",
      "299 Train total loss: 18.9663 \tReconstruction loss: 15.4832 \tLatent loss: 3.48316\n",
      "300 Train total loss: 19.2314 \tReconstruction loss: 15.7986 \tLatent loss: 3.43281\n",
      "301 Train total loss: 18.7981 \tReconstruction loss: 15.3146 \tLatent loss: 3.48353\n",
      "302 Train total loss: 18.428 \tReconstruction loss: 14.945 \tLatent loss: 3.48301\n",
      "303 Train total loss: 18.4214 \tReconstruction loss: 14.9451 \tLatent loss: 3.47631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 Train total loss: 17.5391 \tReconstruction loss: 14.0767 \tLatent loss: 3.46236\n",
      "305 Train total loss: 17.6689 \tReconstruction loss: 14.2175 \tLatent loss: 3.45141\n",
      "306 Train total loss: 17.2264 \tReconstruction loss: 13.7387 \tLatent loss: 3.4877\n",
      "307 Train total loss: 17.7396 \tReconstruction loss: 14.2568 \tLatent loss: 3.48277\n",
      "308 Train total loss: 17.5521 \tReconstruction loss: 14.0771 \tLatent loss: 3.475\n",
      "309 Train total loss: 17.4537 \tReconstruction loss: 13.972 \tLatent loss: 3.48175\n",
      "310 Train total loss: 17.03 \tReconstruction loss: 13.6036 \tLatent loss: 3.42645\n",
      "311 Train total loss: 16.6784 \tReconstruction loss: 13.2578 \tLatent loss: 3.4206\n",
      "312 Train total loss: 17.2077 \tReconstruction loss: 13.762 \tLatent loss: 3.44567\n",
      "313 Train total loss: 18.1434 \tReconstruction loss: 14.7548 \tLatent loss: 3.38855\n",
      "314 Train total loss: 18.4365 \tReconstruction loss: 15.0219 \tLatent loss: 3.4146\n",
      "315 Train total loss: 16.9541 \tReconstruction loss: 13.4913 \tLatent loss: 3.46283\n",
      "316 Train total loss: 17.1085 \tReconstruction loss: 13.6488 \tLatent loss: 3.45968\n",
      "317 Train total loss: 16.7838 \tReconstruction loss: 13.3396 \tLatent loss: 3.44418\n",
      "318 Train total loss: 16.4085 \tReconstruction loss: 12.9512 \tLatent loss: 3.45731\n",
      "319 Train total loss: 16.6423 \tReconstruction loss: 13.2054 \tLatent loss: 3.43685\n",
      "320 Train total loss: 17.1018 \tReconstruction loss: 13.7166 \tLatent loss: 3.38517\n",
      "321 Train total loss: 16.5491 \tReconstruction loss: 13.1136 \tLatent loss: 3.43549\n",
      "322 Train total loss: 16.2539 \tReconstruction loss: 12.8189 \tLatent loss: 3.43504\n",
      "323 Train total loss: 16.2427 \tReconstruction loss: 12.816 \tLatent loss: 3.42661\n",
      "324 Train total loss: 15.5905 \tReconstruction loss: 12.1776 \tLatent loss: 3.41281\n",
      "325 Train total loss: 15.6927 \tReconstruction loss: 12.2916 \tLatent loss: 3.40107\n",
      "326 Train total loss: 15.1516 \tReconstruction loss: 11.7154 \tLatent loss: 3.43614\n",
      "327 Train total loss: 15.6467 \tReconstruction loss: 12.2165 \tLatent loss: 3.43021\n",
      "328 Train total loss: 15.4724 \tReconstruction loss: 12.0495 \tLatent loss: 3.42292\n",
      "329 Train total loss: 15.397 \tReconstruction loss: 11.9696 \tLatent loss: 3.42738\n",
      "330 Train total loss: 15.0945 \tReconstruction loss: 11.7217 \tLatent loss: 3.3728\n",
      "331 Train total loss: 14.7888 \tReconstruction loss: 11.4226 \tLatent loss: 3.36617\n",
      "332 Train total loss: 15.282 \tReconstruction loss: 11.8925 \tLatent loss: 3.38949\n",
      "333 Train total loss: 15.9628 \tReconstruction loss: 12.6308 \tLatent loss: 3.33192\n",
      "334 Train total loss: 16.1527 \tReconstruction loss: 12.7958 \tLatent loss: 3.35692\n",
      "335 Train total loss: 15.0575 \tReconstruction loss: 11.6532 \tLatent loss: 3.4043\n",
      "336 Train total loss: 15.1964 \tReconstruction loss: 11.7961 \tLatent loss: 3.40034\n",
      "337 Train total loss: 14.9302 \tReconstruction loss: 11.5473 \tLatent loss: 3.38293\n",
      "338 Train total loss: 14.5396 \tReconstruction loss: 11.1437 \tLatent loss: 3.39593\n",
      "339 Train total loss: 14.8306 \tReconstruction loss: 11.4555 \tLatent loss: 3.37511\n",
      "340 Train total loss: 15.005 \tReconstruction loss: 11.6829 \tLatent loss: 3.32211\n",
      "341 Train total loss: 14.7156 \tReconstruction loss: 11.3438 \tLatent loss: 3.37184\n",
      "342 Train total loss: 14.4665 \tReconstruction loss: 11.0955 \tLatent loss: 3.37092\n",
      "343 Train total loss: 14.4501 \tReconstruction loss: 11.089 \tLatent loss: 3.36103\n",
      "344 Train total loss: 13.7623 \tReconstruction loss: 10.4149 \tLatent loss: 3.34736\n",
      "345 Train total loss: 13.9013 \tReconstruction loss: 10.5664 \tLatent loss: 3.33495\n",
      "346 Train total loss: 13.649 \tReconstruction loss: 10.2807 \tLatent loss: 3.36829\n",
      "347 Train total loss: 14.0301 \tReconstruction loss: 10.6683 \tLatent loss: 3.36177\n",
      "348 Train total loss: 13.8625 \tReconstruction loss: 10.5089 \tLatent loss: 3.35362\n",
      "349 Train total loss: 13.7793 \tReconstruction loss: 10.4221 \tLatent loss: 3.35724\n",
      "350 Train total loss: 13.5783 \tReconstruction loss: 10.2767 \tLatent loss: 3.30165\n",
      "351 Train total loss: 13.1978 \tReconstruction loss: 9.90212 \tLatent loss: 3.29568\n",
      "352 Train total loss: 13.6521 \tReconstruction loss: 10.3358 \tLatent loss: 3.31622\n",
      "353 Train total loss: 14.2069 \tReconstruction loss: 10.9487 \tLatent loss: 3.25826\n",
      "354 Train total loss: 14.4655 \tReconstruction loss: 11.1823 \tLatent loss: 3.28319\n",
      "355 Train total loss: 13.4874 \tReconstruction loss: 10.1581 \tLatent loss: 3.3293\n",
      "356 Train total loss: 13.5739 \tReconstruction loss: 10.2497 \tLatent loss: 3.32418\n",
      "357 Train total loss: 13.3933 \tReconstruction loss: 10.0891 \tLatent loss: 3.3042\n",
      "358 Train total loss: 13.1082 \tReconstruction loss: 9.7906 \tLatent loss: 3.31762\n",
      "359 Train total loss: 13.2678 \tReconstruction loss: 9.97134 \tLatent loss: 3.29645\n",
      "360 Train total loss: 13.4436 \tReconstruction loss: 10.2019 \tLatent loss: 3.24174\n",
      "361 Train total loss: 13.1468 \tReconstruction loss: 9.85683 \tLatent loss: 3.29001\n",
      "362 Train total loss: 12.9947 \tReconstruction loss: 9.70493 \tLatent loss: 3.28982\n",
      "363 Train total loss: 13.0074 \tReconstruction loss: 9.72844 \tLatent loss: 3.27899\n",
      "364 Train total loss: 12.4871 \tReconstruction loss: 9.22112 \tLatent loss: 3.26598\n",
      "365 Train total loss: 12.5572 \tReconstruction loss: 9.30539 \tLatent loss: 3.25181\n",
      "366 Train total loss: 12.307 \tReconstruction loss: 9.02431 \tLatent loss: 3.28272\n",
      "367 Train total loss: 12.6354 \tReconstruction loss: 9.35929 \tLatent loss: 3.27611\n",
      "368 Train total loss: 12.4677 \tReconstruction loss: 9.20066 \tLatent loss: 3.26701\n",
      "369 Train total loss: 12.4115 \tReconstruction loss: 9.14262 \tLatent loss: 3.26888\n",
      "370 Train total loss: 12.1792 \tReconstruction loss: 8.96606 \tLatent loss: 3.21319\n",
      "371 Train total loss: 11.9407 \tReconstruction loss: 8.73389 \tLatent loss: 3.2068\n",
      "372 Train total loss: 12.297 \tReconstruction loss: 9.07127 \tLatent loss: 3.22578\n",
      "373 Train total loss: 12.8319 \tReconstruction loss: 9.66317 \tLatent loss: 3.16873\n",
      "374 Train total loss: 12.9476 \tReconstruction loss: 9.7577 \tLatent loss: 3.18989\n",
      "375 Train total loss: 12.1783 \tReconstruction loss: 8.94247 \tLatent loss: 3.23588\n",
      "376 Train total loss: 12.2329 \tReconstruction loss: 9.0037 \tLatent loss: 3.22916\n",
      "377 Train total loss: 12.0801 \tReconstruction loss: 8.87204 \tLatent loss: 3.2081\n",
      "378 Train total loss: 11.8355 \tReconstruction loss: 8.61451 \tLatent loss: 3.221\n",
      "379 Train total loss: 11.9933 \tReconstruction loss: 8.79499 \tLatent loss: 3.19832\n",
      "380 Train total loss: 12.1341 \tReconstruction loss: 8.98979 \tLatent loss: 3.14427\n",
      "381 Train total loss: 11.9179 \tReconstruction loss: 8.72991 \tLatent loss: 3.18804\n",
      "382 Train total loss: 11.7756 \tReconstruction loss: 8.58623 \tLatent loss: 3.18938\n",
      "383 Train total loss: 11.73 \tReconstruction loss: 8.55156 \tLatent loss: 3.1784\n",
      "384 Train total loss: 11.3004 \tReconstruction loss: 8.1353 \tLatent loss: 3.16507\n",
      "385 Train total loss: 11.4019 \tReconstruction loss: 8.25239 \tLatent loss: 3.14954\n",
      "386 Train total loss: 11.2108 \tReconstruction loss: 8.03413 \tLatent loss: 3.17663\n",
      "387 Train total loss: 11.4405 \tReconstruction loss: 8.27038 \tLatent loss: 3.1701\n",
      "388 Train total loss: 11.3351 \tReconstruction loss: 8.17409 \tLatent loss: 3.161\n",
      "389 Train total loss: 11.2819 \tReconstruction loss: 8.12072 \tLatent loss: 3.16121\n",
      "390 Train total loss: 11.0679 \tReconstruction loss: 7.9627 \tLatent loss: 3.10523\n",
      "391 Train total loss: 10.9549 \tReconstruction loss: 7.85611 \tLatent loss: 3.09879\n",
      "392 Train total loss: 11.2069 \tReconstruction loss: 8.09059 \tLatent loss: 3.11629\n",
      "393 Train total loss: 11.6094 \tReconstruction loss: 8.54875 \tLatent loss: 3.06068\n",
      "394 Train total loss: 11.686 \tReconstruction loss: 8.60847 \tLatent loss: 3.0775\n",
      "395 Train total loss: 11.0681 \tReconstruction loss: 7.94713 \tLatent loss: 3.12094\n",
      "396 Train total loss: 11.1313 \tReconstruction loss: 8.02103 \tLatent loss: 3.11032\n",
      "397 Train total loss: 10.9783 \tReconstruction loss: 7.88671 \tLatent loss: 3.09162\n",
      "398 Train total loss: 10.7944 \tReconstruction loss: 7.68979 \tLatent loss: 3.10457\n",
      "399 Train total loss: 10.9414 \tReconstruction loss: 7.86246 \tLatent loss: 3.07891\n",
      "400 Train total loss: 11.0383 \tReconstruction loss: 8.01421 \tLatent loss: 3.02409\n",
      "401 Train total loss: 10.8633 \tReconstruction loss: 7.79539 \tLatent loss: 3.06791\n",
      "402 Train total loss: 10.7507 \tReconstruction loss: 7.68408 \tLatent loss: 3.06659\n",
      "403 Train total loss: 10.753 \tReconstruction loss: 7.69639 \tLatent loss: 3.05659\n",
      "404 Train total loss: 10.3801 \tReconstruction loss: 7.33835 \tLatent loss: 3.04172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 Train total loss: 10.4248 \tReconstruction loss: 7.39863 \tLatent loss: 3.0262\n",
      "406 Train total loss: 10.2352 \tReconstruction loss: 7.18769 \tLatent loss: 3.04754\n",
      "407 Train total loss: 10.4599 \tReconstruction loss: 7.41644 \tLatent loss: 3.04346\n",
      "408 Train total loss: 10.3455 \tReconstruction loss: 7.31271 \tLatent loss: 3.03279\n",
      "409 Train total loss: 10.3404 \tReconstruction loss: 7.3061 \tLatent loss: 3.03431\n",
      "410 Train total loss: 10.1502 \tReconstruction loss: 7.17462 \tLatent loss: 2.97559\n",
      "411 Train total loss: 10.011 \tReconstruction loss: 7.04035 \tLatent loss: 2.97068\n",
      "412 Train total loss: 10.2417 \tReconstruction loss: 7.25737 \tLatent loss: 2.98435\n",
      "413 Train total loss: 10.5584 \tReconstruction loss: 7.62758 \tLatent loss: 2.93081\n",
      "414 Train total loss: 10.6505 \tReconstruction loss: 7.70452 \tLatent loss: 2.94597\n",
      "415 Train total loss: 10.1616 \tReconstruction loss: 7.17501 \tLatent loss: 2.98658\n",
      "416 Train total loss: 10.1871 \tReconstruction loss: 7.21301 \tLatent loss: 2.97411\n",
      "417 Train total loss: 10.0815 \tReconstruction loss: 7.12877 \tLatent loss: 2.95271\n",
      "418 Train total loss: 9.91151 \tReconstruction loss: 6.94447 \tLatent loss: 2.96704\n",
      "419 Train total loss: 10.0099 \tReconstruction loss: 7.06951 \tLatent loss: 2.94039\n",
      "420 Train total loss: 10.0918 \tReconstruction loss: 7.20666 \tLatent loss: 2.8851\n",
      "421 Train total loss: 9.92068 \tReconstruction loss: 6.99298 \tLatent loss: 2.9277\n",
      "422 Train total loss: 9.87194 \tReconstruction loss: 6.94726 \tLatent loss: 2.92467\n",
      "423 Train total loss: 9.87768 \tReconstruction loss: 6.96502 \tLatent loss: 2.91266\n",
      "424 Train total loss: 9.57238 \tReconstruction loss: 6.67314 \tLatent loss: 2.89923\n",
      "425 Train total loss: 9.61404 \tReconstruction loss: 6.73445 \tLatent loss: 2.87958\n",
      "426 Train total loss: 9.46489 \tReconstruction loss: 6.56395 \tLatent loss: 2.90095\n",
      "427 Train total loss: 9.6137 \tReconstruction loss: 6.71934 \tLatent loss: 2.89436\n",
      "428 Train total loss: 9.55914 \tReconstruction loss: 6.67561 \tLatent loss: 2.88353\n",
      "429 Train total loss: 9.50773 \tReconstruction loss: 6.62239 \tLatent loss: 2.88534\n",
      "430 Train total loss: 9.37831 \tReconstruction loss: 6.55169 \tLatent loss: 2.82662\n",
      "431 Train total loss: 9.23393 \tReconstruction loss: 6.41207 \tLatent loss: 2.82185\n",
      "432 Train total loss: 9.44013 \tReconstruction loss: 6.60625 \tLatent loss: 2.83388\n",
      "433 Train total loss: 9.74953 \tReconstruction loss: 6.96919 \tLatent loss: 2.78035\n",
      "434 Train total loss: 9.77884 \tReconstruction loss: 6.98529 \tLatent loss: 2.79355\n",
      "435 Train total loss: 9.33739 \tReconstruction loss: 6.50735 \tLatent loss: 2.83003\n",
      "436 Train total loss: 9.35417 \tReconstruction loss: 6.53571 \tLatent loss: 2.81846\n",
      "437 Train total loss: 9.30878 \tReconstruction loss: 6.51387 \tLatent loss: 2.79491\n",
      "438 Train total loss: 9.16473 \tReconstruction loss: 6.35847 \tLatent loss: 2.80626\n",
      "439 Train total loss: 9.22069 \tReconstruction loss: 6.44019 \tLatent loss: 2.7805\n",
      "440 Train total loss: 9.30209 \tReconstruction loss: 6.5801 \tLatent loss: 2.72199\n",
      "441 Train total loss: 9.19711 \tReconstruction loss: 6.43285 \tLatent loss: 2.76426\n",
      "442 Train total loss: 9.09534 \tReconstruction loss: 6.33619 \tLatent loss: 2.75915\n",
      "443 Train total loss: 9.09617 \tReconstruction loss: 6.34796 \tLatent loss: 2.74822\n",
      "444 Train total loss: 8.84284 \tReconstruction loss: 6.10875 \tLatent loss: 2.73408\n",
      "445 Train total loss: 8.87672 \tReconstruction loss: 6.16021 \tLatent loss: 2.71651\n",
      "446 Train total loss: 8.7657 \tReconstruction loss: 6.03221 \tLatent loss: 2.73349\n",
      "447 Train total loss: 8.8793 \tReconstruction loss: 6.1574 \tLatent loss: 2.72189\n",
      "448 Train total loss: 8.82129 \tReconstruction loss: 6.11069 \tLatent loss: 2.71059\n",
      "449 Train total loss: 8.82383 \tReconstruction loss: 6.10972 \tLatent loss: 2.71411\n",
      "450 Train total loss: 8.67965 \tReconstruction loss: 6.02747 \tLatent loss: 2.65218\n",
      "451 Train total loss: 8.56297 \tReconstruction loss: 5.90917 \tLatent loss: 2.65379\n",
      "452 Train total loss: 8.74217 \tReconstruction loss: 6.08083 \tLatent loss: 2.66134\n",
      "453 Train total loss: 8.91637 \tReconstruction loss: 6.3112 \tLatent loss: 2.60516\n",
      "454 Train total loss: 9.01894 \tReconstruction loss: 6.39618 \tLatent loss: 2.62276\n",
      "455 Train total loss: 8.64673 \tReconstruction loss: 6.00427 \tLatent loss: 2.64246\n",
      "456 Train total loss: 8.6895 \tReconstruction loss: 6.0691 \tLatent loss: 2.6204\n",
      "457 Train total loss: 8.6116 \tReconstruction loss: 6.00584 \tLatent loss: 2.60576\n",
      "458 Train total loss: 8.47749 \tReconstruction loss: 5.86001 \tLatent loss: 2.61748\n",
      "459 Train total loss: 8.5653 \tReconstruction loss: 5.97819 \tLatent loss: 2.58712\n",
      "460 Train total loss: 8.58752 \tReconstruction loss: 6.05623 \tLatent loss: 2.53129\n",
      "461 Train total loss: 8.49466 \tReconstruction loss: 5.93302 \tLatent loss: 2.56163\n",
      "462 Train total loss: 8.4308 \tReconstruction loss: 5.85859 \tLatent loss: 2.57221\n",
      "463 Train total loss: 8.40812 \tReconstruction loss: 5.85101 \tLatent loss: 2.55712\n",
      "464 Train total loss: 8.1943 \tReconstruction loss: 5.64136 \tLatent loss: 2.55294\n",
      "465 Train total loss: 8.24876 \tReconstruction loss: 5.71569 \tLatent loss: 2.53307\n",
      "466 Train total loss: 8.11953 \tReconstruction loss: 5.58737 \tLatent loss: 2.53216\n",
      "467 Train total loss: 8.24751 \tReconstruction loss: 5.73221 \tLatent loss: 2.51529\n",
      "468 Train total loss: 8.18609 \tReconstruction loss: 5.68131 \tLatent loss: 2.50478\n",
      "469 Train total loss: 8.17503 \tReconstruction loss: 5.65009 \tLatent loss: 2.52494\n",
      "470 Train total loss: 8.06071 \tReconstruction loss: 5.5987 \tLatent loss: 2.46201\n",
      "471 Train total loss: 8.00192 \tReconstruction loss: 5.55589 \tLatent loss: 2.44603\n",
      "472 Train total loss: 8.09923 \tReconstruction loss: 5.63993 \tLatent loss: 2.4593\n",
      "473 Train total loss: 8.31018 \tReconstruction loss: 5.89754 \tLatent loss: 2.41264\n",
      "474 Train total loss: 8.37109 \tReconstruction loss: 5.94259 \tLatent loss: 2.42849\n",
      "475 Train total loss: 8.03254 \tReconstruction loss: 5.61817 \tLatent loss: 2.41438\n",
      "476 Train total loss: 8.02147 \tReconstruction loss: 5.63072 \tLatent loss: 2.39076\n",
      "477 Train total loss: 7.95449 \tReconstruction loss: 5.56269 \tLatent loss: 2.39179\n",
      "478 Train total loss: 7.87158 \tReconstruction loss: 5.47538 \tLatent loss: 2.3962\n",
      "479 Train total loss: 7.92999 \tReconstruction loss: 5.58754 \tLatent loss: 2.34245\n",
      "480 Train total loss: 7.98661 \tReconstruction loss: 5.67725 \tLatent loss: 2.30937\n",
      "481 Train total loss: 7.87988 \tReconstruction loss: 5.53662 \tLatent loss: 2.34325\n",
      "482 Train total loss: 7.86125 \tReconstruction loss: 5.49816 \tLatent loss: 2.36309\n",
      "483 Train total loss: 7.83905 \tReconstruction loss: 5.49855 \tLatent loss: 2.34049\n",
      "484 Train total loss: 7.70562 \tReconstruction loss: 5.34753 \tLatent loss: 2.35809\n",
      "485 Train total loss: 7.6777 \tReconstruction loss: 5.3697 \tLatent loss: 2.308\n",
      "486 Train total loss: 7.5204 \tReconstruction loss: 5.23697 \tLatent loss: 2.28343\n",
      "487 Train total loss: 7.6572 \tReconstruction loss: 5.37684 \tLatent loss: 2.28036\n",
      "488 Train total loss: 7.60274 \tReconstruction loss: 5.36352 \tLatent loss: 2.23922\n",
      "489 Train total loss: 7.60733 \tReconstruction loss: 5.32789 \tLatent loss: 2.27944\n",
      "490 Train total loss: 7.54825 \tReconstruction loss: 5.31138 \tLatent loss: 2.23688\n",
      "491 Train total loss: 7.39713 \tReconstruction loss: 5.18993 \tLatent loss: 2.2072\n",
      "492 Train total loss: 7.55621 \tReconstruction loss: 5.34115 \tLatent loss: 2.21506\n",
      "493 Train total loss: 7.73472 \tReconstruction loss: 5.5428 \tLatent loss: 2.19192\n",
      "494 Train total loss: 7.75922 \tReconstruction loss: 5.56473 \tLatent loss: 2.19449\n",
      "495 Train total loss: 7.48873 \tReconstruction loss: 5.31359 \tLatent loss: 2.17514\n",
      "496 Train total loss: 7.47668 \tReconstruction loss: 5.30342 \tLatent loss: 2.17326\n",
      "497 Train total loss: 7.42323 \tReconstruction loss: 5.26496 \tLatent loss: 2.15827\n",
      "498 Train total loss: 7.36273 \tReconstruction loss: 5.20138 \tLatent loss: 2.16135\n",
      "499 Train total loss: 7.34903 \tReconstruction loss: 5.24244 \tLatent loss: 2.10659\n",
      "500 Train total loss: 7.41517 \tReconstruction loss: 5.32793 \tLatent loss: 2.08724\n",
      "501 Train total loss: 7.35515 \tReconstruction loss: 5.23872 \tLatent loss: 2.11643\n",
      "502 Train total loss: 7.28479 \tReconstruction loss: 5.18432 \tLatent loss: 2.10047\n",
      "503 Train total loss: 7.27913 \tReconstruction loss: 5.17215 \tLatent loss: 2.10697\n",
      "504 Train total loss: 7.15527 \tReconstruction loss: 5.00681 \tLatent loss: 2.14846\n",
      "505 Train total loss: 7.12967 \tReconstruction loss: 5.05795 \tLatent loss: 2.07172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506 Train total loss: 7.08881 \tReconstruction loss: 5.02926 \tLatent loss: 2.05955\n",
      "507 Train total loss: 7.16117 \tReconstruction loss: 5.10346 \tLatent loss: 2.05771\n",
      "508 Train total loss: 7.09663 \tReconstruction loss: 5.07986 \tLatent loss: 2.01677\n",
      "509 Train total loss: 7.07077 \tReconstruction loss: 5.03604 \tLatent loss: 2.03473\n",
      "510 Train total loss: 7.03761 \tReconstruction loss: 5.00912 \tLatent loss: 2.02849\n",
      "511 Train total loss: 6.95587 \tReconstruction loss: 4.95575 \tLatent loss: 2.00013\n",
      "512 Train total loss: 7.04257 \tReconstruction loss: 5.05601 \tLatent loss: 1.98656\n",
      "513 Train total loss: 7.22073 \tReconstruction loss: 5.25262 \tLatent loss: 1.96811\n",
      "514 Train total loss: 7.22946 \tReconstruction loss: 5.26019 \tLatent loss: 1.96927\n",
      "515 Train total loss: 6.98847 \tReconstruction loss: 5.03035 \tLatent loss: 1.95813\n",
      "516 Train total loss: 7.01666 \tReconstruction loss: 5.05306 \tLatent loss: 1.9636\n",
      "517 Train total loss: 6.95041 \tReconstruction loss: 5.00144 \tLatent loss: 1.94897\n",
      "518 Train total loss: 6.837 \tReconstruction loss: 4.89169 \tLatent loss: 1.94532\n",
      "519 Train total loss: 6.91596 \tReconstruction loss: 5.00339 \tLatent loss: 1.91256\n",
      "520 Train total loss: 6.9347 \tReconstruction loss: 5.0356 \tLatent loss: 1.89911\n",
      "521 Train total loss: 6.86271 \tReconstruction loss: 4.9538 \tLatent loss: 1.90891\n",
      "522 Train total loss: 6.83167 \tReconstruction loss: 4.93727 \tLatent loss: 1.8944\n",
      "523 Train total loss: 6.82913 \tReconstruction loss: 4.93051 \tLatent loss: 1.89862\n",
      "524 Train total loss: 6.74229 \tReconstruction loss: 4.80579 \tLatent loss: 1.93649\n",
      "525 Train total loss: 6.72712 \tReconstruction loss: 4.86597 \tLatent loss: 1.86115\n",
      "526 Train total loss: 6.61488 \tReconstruction loss: 4.75108 \tLatent loss: 1.8638\n",
      "527 Train total loss: 6.73053 \tReconstruction loss: 4.88319 \tLatent loss: 1.84734\n",
      "528 Train total loss: 6.69024 \tReconstruction loss: 4.86078 \tLatent loss: 1.82946\n",
      "529 Train total loss: 6.63946 \tReconstruction loss: 4.80944 \tLatent loss: 1.83002\n",
      "530 Train total loss: 6.63311 \tReconstruction loss: 4.80432 \tLatent loss: 1.8288\n",
      "531 Train total loss: 6.5905 \tReconstruction loss: 4.78492 \tLatent loss: 1.80558\n",
      "532 Train total loss: 6.64079 \tReconstruction loss: 4.85062 \tLatent loss: 1.79017\n",
      "533 Train total loss: 6.71672 \tReconstruction loss: 4.94865 \tLatent loss: 1.76807\n",
      "534 Train total loss: 6.76154 \tReconstruction loss: 4.99801 \tLatent loss: 1.76353\n",
      "535 Train total loss: 6.55626 \tReconstruction loss: 4.78694 \tLatent loss: 1.76932\n",
      "536 Train total loss: 6.57515 \tReconstruction loss: 4.80017 \tLatent loss: 1.77498\n",
      "537 Train total loss: 6.56128 \tReconstruction loss: 4.80135 \tLatent loss: 1.75994\n",
      "538 Train total loss: 6.47516 \tReconstruction loss: 4.72179 \tLatent loss: 1.75337\n",
      "539 Train total loss: 6.50157 \tReconstruction loss: 4.77109 \tLatent loss: 1.73048\n",
      "540 Train total loss: 6.57308 \tReconstruction loss: 4.85651 \tLatent loss: 1.71656\n",
      "541 Train total loss: 6.45536 \tReconstruction loss: 4.73575 \tLatent loss: 1.71961\n",
      "542 Train total loss: 6.43287 \tReconstruction loss: 4.72607 \tLatent loss: 1.7068\n",
      "543 Train total loss: 6.44361 \tReconstruction loss: 4.73912 \tLatent loss: 1.70449\n",
      "544 Train total loss: 6.37232 \tReconstruction loss: 4.62474 \tLatent loss: 1.74758\n",
      "545 Train total loss: 6.32145 \tReconstruction loss: 4.6431 \tLatent loss: 1.67835\n",
      "546 Train total loss: 6.25403 \tReconstruction loss: 4.57386 \tLatent loss: 1.68017\n",
      "547 Train total loss: 6.31569 \tReconstruction loss: 4.65565 \tLatent loss: 1.66004\n",
      "548 Train total loss: 6.29771 \tReconstruction loss: 4.64719 \tLatent loss: 1.65052\n",
      "549 Train total loss: 6.32065 \tReconstruction loss: 4.67088 \tLatent loss: 1.64978\n",
      "550 Train total loss: 6.28013 \tReconstruction loss: 4.63246 \tLatent loss: 1.64768\n",
      "551 Train total loss: 6.18056 \tReconstruction loss: 4.54904 \tLatent loss: 1.63152\n",
      "552 Train total loss: 6.29546 \tReconstruction loss: 4.68069 \tLatent loss: 1.61476\n",
      "553 Train total loss: 6.34012 \tReconstruction loss: 4.74604 \tLatent loss: 1.59408\n",
      "554 Train total loss: 6.39849 \tReconstruction loss: 4.80919 \tLatent loss: 1.5893\n",
      "555 Train total loss: 6.22242 \tReconstruction loss: 4.62779 \tLatent loss: 1.59463\n",
      "556 Train total loss: 6.19957 \tReconstruction loss: 4.6045 \tLatent loss: 1.59507\n",
      "557 Train total loss: 6.17113 \tReconstruction loss: 4.58641 \tLatent loss: 1.58472\n",
      "558 Train total loss: 6.12137 \tReconstruction loss: 4.54777 \tLatent loss: 1.57361\n",
      "559 Train total loss: 6.13236 \tReconstruction loss: 4.57629 \tLatent loss: 1.55607\n",
      "560 Train total loss: 6.19281 \tReconstruction loss: 4.64846 \tLatent loss: 1.54435\n",
      "561 Train total loss: 6.14562 \tReconstruction loss: 4.60097 \tLatent loss: 1.54465\n",
      "562 Train total loss: 6.07534 \tReconstruction loss: 4.54305 \tLatent loss: 1.53228\n",
      "563 Train total loss: 6.07578 \tReconstruction loss: 4.54656 \tLatent loss: 1.52923\n",
      "564 Train total loss: 6.03176 \tReconstruction loss: 4.46136 \tLatent loss: 1.5704\n",
      "565 Train total loss: 6.00446 \tReconstruction loss: 4.49941 \tLatent loss: 1.50505\n",
      "566 Train total loss: 5.95098 \tReconstruction loss: 4.44327 \tLatent loss: 1.50771\n",
      "567 Train total loss: 6.01617 \tReconstruction loss: 4.52712 \tLatent loss: 1.48905\n",
      "568 Train total loss: 5.96801 \tReconstruction loss: 4.4861 \tLatent loss: 1.48192\n",
      "569 Train total loss: 5.97661 \tReconstruction loss: 4.4974 \tLatent loss: 1.47921\n",
      "570 Train total loss: 5.95125 \tReconstruction loss: 4.47384 \tLatent loss: 1.47741\n",
      "571 Train total loss: 5.91689 \tReconstruction loss: 4.4547 \tLatent loss: 1.4622\n",
      "572 Train total loss: 5.94097 \tReconstruction loss: 4.49285 \tLatent loss: 1.44812\n",
      "573 Train total loss: 6.06587 \tReconstruction loss: 4.63489 \tLatent loss: 1.43098\n",
      "574 Train total loss: 6.08208 \tReconstruction loss: 4.65464 \tLatent loss: 1.42743\n",
      "575 Train total loss: 5.90675 \tReconstruction loss: 4.47609 \tLatent loss: 1.43067\n",
      "576 Train total loss: 5.8998 \tReconstruction loss: 4.47243 \tLatent loss: 1.42737\n",
      "577 Train total loss: 5.92152 \tReconstruction loss: 4.49974 \tLatent loss: 1.42178\n",
      "578 Train total loss: 5.80895 \tReconstruction loss: 4.39717 \tLatent loss: 1.41177\n",
      "579 Train total loss: 5.85448 \tReconstruction loss: 4.457 \tLatent loss: 1.39748\n",
      "580 Train total loss: 5.90691 \tReconstruction loss: 4.52138 \tLatent loss: 1.38553\n",
      "581 Train total loss: 5.82396 \tReconstruction loss: 4.43769 \tLatent loss: 1.38627\n",
      "582 Train total loss: 5.79227 \tReconstruction loss: 4.41751 \tLatent loss: 1.37476\n",
      "583 Train total loss: 5.79623 \tReconstruction loss: 4.42524 \tLatent loss: 1.37099\n",
      "584 Train total loss: 5.79637 \tReconstruction loss: 4.38454 \tLatent loss: 1.41183\n",
      "585 Train total loss: 5.72544 \tReconstruction loss: 4.37535 \tLatent loss: 1.35009\n",
      "586 Train total loss: 5.69081 \tReconstruction loss: 4.34044 \tLatent loss: 1.35037\n",
      "587 Train total loss: 5.73247 \tReconstruction loss: 4.39756 \tLatent loss: 1.33491\n",
      "588 Train total loss: 5.6938 \tReconstruction loss: 4.36364 \tLatent loss: 1.33016\n",
      "589 Train total loss: 5.71554 \tReconstruction loss: 4.38874 \tLatent loss: 1.3268\n",
      "590 Train total loss: 5.67146 \tReconstruction loss: 4.34628 \tLatent loss: 1.32518\n",
      "591 Train total loss: 5.63163 \tReconstruction loss: 4.32267 \tLatent loss: 1.30897\n",
      "592 Train total loss: 5.72221 \tReconstruction loss: 4.42501 \tLatent loss: 1.2972\n",
      "593 Train total loss: 5.82652 \tReconstruction loss: 4.54044 \tLatent loss: 1.28608\n",
      "594 Train total loss: 5.78799 \tReconstruction loss: 4.50351 \tLatent loss: 1.28447\n",
      "595 Train total loss: 5.61799 \tReconstruction loss: 4.3329 \tLatent loss: 1.28509\n",
      "596 Train total loss: 5.64244 \tReconstruction loss: 4.36164 \tLatent loss: 1.28079\n",
      "597 Train total loss: 5.68239 \tReconstruction loss: 4.40836 \tLatent loss: 1.27403\n",
      "598 Train total loss: 5.57754 \tReconstruction loss: 4.30881 \tLatent loss: 1.26873\n",
      "599 Train total loss: 5.67543 \tReconstruction loss: 4.41913 \tLatent loss: 1.2563\n",
      "600 Train total loss: 5.65409 \tReconstruction loss: 4.40626 \tLatent loss: 1.24783\n",
      "601 Train total loss: 5.56609 \tReconstruction loss: 4.31852 \tLatent loss: 1.24757\n",
      "602 Train total loss: 5.55815 \tReconstruction loss: 4.32083 \tLatent loss: 1.23732\n",
      "603 Train total loss: 5.53898 \tReconstruction loss: 4.30472 \tLatent loss: 1.23425\n",
      "604 Train total loss: 5.52115 \tReconstruction loss: 4.24743 \tLatent loss: 1.27372\n",
      "605 Train total loss: 5.49305 \tReconstruction loss: 4.27693 \tLatent loss: 1.21612\n",
      "606 Train total loss: 5.47077 \tReconstruction loss: 4.25278 \tLatent loss: 1.21799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607 Train total loss: 5.48742 \tReconstruction loss: 4.28428 \tLatent loss: 1.20313\n",
      "608 Train total loss: 5.4916 \tReconstruction loss: 4.29088 \tLatent loss: 1.20073\n",
      "609 Train total loss: 5.42449 \tReconstruction loss: 4.22757 \tLatent loss: 1.19692\n",
      "610 Train total loss: 5.44962 \tReconstruction loss: 4.25248 \tLatent loss: 1.19714\n",
      "611 Train total loss: 5.44999 \tReconstruction loss: 4.26725 \tLatent loss: 1.18274\n",
      "612 Train total loss: 5.49847 \tReconstruction loss: 4.32774 \tLatent loss: 1.17073\n",
      "613 Train total loss: 5.55651 \tReconstruction loss: 4.39079 \tLatent loss: 1.16572\n",
      "614 Train total loss: 5.63716 \tReconstruction loss: 4.47591 \tLatent loss: 1.16124\n",
      "615 Train total loss: 5.38414 \tReconstruction loss: 4.225 \tLatent loss: 1.15914\n",
      "616 Train total loss: 5.40326 \tReconstruction loss: 4.24824 \tLatent loss: 1.15502\n",
      "617 Train total loss: 5.38691 \tReconstruction loss: 4.23823 \tLatent loss: 1.14868\n",
      "618 Train total loss: 5.35087 \tReconstruction loss: 4.20725 \tLatent loss: 1.14362\n",
      "619 Train total loss: 5.37259 \tReconstruction loss: 4.23789 \tLatent loss: 1.1347\n",
      "620 Train total loss: 5.48131 \tReconstruction loss: 4.35153 \tLatent loss: 1.12977\n",
      "621 Train total loss: 5.344 \tReconstruction loss: 4.21461 \tLatent loss: 1.12938\n",
      "622 Train total loss: 5.36316 \tReconstruction loss: 4.24357 \tLatent loss: 1.1196\n",
      "623 Train total loss: 5.38088 \tReconstruction loss: 4.26372 \tLatent loss: 1.11716\n",
      "624 Train total loss: 5.32844 \tReconstruction loss: 4.17486 \tLatent loss: 1.15358\n",
      "625 Train total loss: 5.29868 \tReconstruction loss: 4.19413 \tLatent loss: 1.10455\n",
      "626 Train total loss: 5.25444 \tReconstruction loss: 4.14803 \tLatent loss: 1.10641\n",
      "627 Train total loss: 5.29877 \tReconstruction loss: 4.20783 \tLatent loss: 1.09094\n",
      "628 Train total loss: 5.27325 \tReconstruction loss: 4.18339 \tLatent loss: 1.08986\n",
      "629 Train total loss: 5.27134 \tReconstruction loss: 4.18607 \tLatent loss: 1.08527\n",
      "630 Train total loss: 5.29173 \tReconstruction loss: 4.2049 \tLatent loss: 1.08684\n",
      "631 Train total loss: 5.24595 \tReconstruction loss: 4.17171 \tLatent loss: 1.07423\n",
      "632 Train total loss: 5.27243 \tReconstruction loss: 4.20801 \tLatent loss: 1.06442\n",
      "633 Train total loss: 5.30778 \tReconstruction loss: 4.24531 \tLatent loss: 1.06246\n",
      "634 Train total loss: 5.31394 \tReconstruction loss: 4.257 \tLatent loss: 1.05694\n",
      "635 Train total loss: 5.20781 \tReconstruction loss: 4.15456 \tLatent loss: 1.05325\n",
      "636 Train total loss: 5.22787 \tReconstruction loss: 4.17881 \tLatent loss: 1.04906\n",
      "637 Train total loss: 5.20987 \tReconstruction loss: 4.16567 \tLatent loss: 1.0442\n",
      "638 Train total loss: 5.16494 \tReconstruction loss: 4.12649 \tLatent loss: 1.03845\n",
      "639 Train total loss: 5.20099 \tReconstruction loss: 4.16951 \tLatent loss: 1.03148\n",
      "640 Train total loss: 5.23999 \tReconstruction loss: 4.21195 \tLatent loss: 1.02803\n",
      "641 Train total loss: 5.18355 \tReconstruction loss: 4.15784 \tLatent loss: 1.02571\n",
      "642 Train total loss: 5.15025 \tReconstruction loss: 4.13356 \tLatent loss: 1.01668\n",
      "643 Train total loss: 5.15924 \tReconstruction loss: 4.14531 \tLatent loss: 1.01393\n",
      "644 Train total loss: 5.12475 \tReconstruction loss: 4.08103 \tLatent loss: 1.04371\n",
      "645 Train total loss: 5.13226 \tReconstruction loss: 4.12618 \tLatent loss: 1.00608\n",
      "646 Train total loss: 5.07951 \tReconstruction loss: 4.07429 \tLatent loss: 1.00522\n",
      "647 Train total loss: 5.17155 \tReconstruction loss: 4.17844 \tLatent loss: 0.993115\n",
      "648 Train total loss: 5.11774 \tReconstruction loss: 4.12682 \tLatent loss: 0.990914\n",
      "649 Train total loss: 5.13003 \tReconstruction loss: 4.1441 \tLatent loss: 0.985932\n",
      "650 Train total loss: 5.09541 \tReconstruction loss: 4.11009 \tLatent loss: 0.985318\n",
      "651 Train total loss: 5.08776 \tReconstruction loss: 4.11373 \tLatent loss: 0.974037\n",
      "652 Train total loss: 5.16401 \tReconstruction loss: 4.19209 \tLatent loss: 0.971921\n",
      "653 Train total loss: 5.16333 \tReconstruction loss: 4.19514 \tLatent loss: 0.968189\n",
      "654 Train total loss: 5.15724 \tReconstruction loss: 4.18786 \tLatent loss: 0.969379\n",
      "655 Train total loss: 5.07853 \tReconstruction loss: 4.11325 \tLatent loss: 0.965277\n",
      "656 Train total loss: 5.07482 \tReconstruction loss: 4.1151 \tLatent loss: 0.959719\n",
      "657 Train total loss: 5.03947 \tReconstruction loss: 4.08246 \tLatent loss: 0.957015\n",
      "658 Train total loss: 5.0077 \tReconstruction loss: 4.05425 \tLatent loss: 0.953447\n",
      "659 Train total loss: 5.03285 \tReconstruction loss: 4.08472 \tLatent loss: 0.948138\n",
      "660 Train total loss: 5.07708 \tReconstruction loss: 4.13348 \tLatent loss: 0.943595\n",
      "661 Train total loss: 5.03488 \tReconstruction loss: 4.09758 \tLatent loss: 0.937301\n",
      "662 Train total loss: 5.01931 \tReconstruction loss: 4.0893 \tLatent loss: 0.930016\n",
      "663 Train total loss: 5.01161 \tReconstruction loss: 4.08501 \tLatent loss: 0.926608\n",
      "664 Train total loss: 4.98865 \tReconstruction loss: 4.04263 \tLatent loss: 0.946023\n",
      "665 Train total loss: 4.99077 \tReconstruction loss: 4.07023 \tLatent loss: 0.920546\n",
      "666 Train total loss: 4.94626 \tReconstruction loss: 4.02895 \tLatent loss: 0.917304\n",
      "667 Train total loss: 4.96057 \tReconstruction loss: 4.05339 \tLatent loss: 0.907183\n",
      "668 Train total loss: 4.95563 \tReconstruction loss: 4.04932 \tLatent loss: 0.906308\n",
      "669 Train total loss: 4.96719 \tReconstruction loss: 4.06456 \tLatent loss: 0.902633\n",
      "670 Train total loss: 4.96186 \tReconstruction loss: 4.0656 \tLatent loss: 0.896259\n",
      "671 Train total loss: 4.90732 \tReconstruction loss: 4.02132 \tLatent loss: 0.885996\n",
      "672 Train total loss: 4.95849 \tReconstruction loss: 4.0747 \tLatent loss: 0.883794\n",
      "673 Train total loss: 4.99227 \tReconstruction loss: 4.11296 \tLatent loss: 0.879315\n",
      "674 Train total loss: 5.00783 \tReconstruction loss: 4.12924 \tLatent loss: 0.878595\n",
      "675 Train total loss: 4.92715 \tReconstruction loss: 4.05138 \tLatent loss: 0.875769\n",
      "676 Train total loss: 4.90977 \tReconstruction loss: 4.0372 \tLatent loss: 0.872572\n",
      "677 Train total loss: 4.94873 \tReconstruction loss: 4.08009 \tLatent loss: 0.868643\n",
      "678 Train total loss: 4.89249 \tReconstruction loss: 4.02533 \tLatent loss: 0.867163\n",
      "679 Train total loss: 4.94689 \tReconstruction loss: 4.08419 \tLatent loss: 0.8627\n",
      "680 Train total loss: 4.95497 \tReconstruction loss: 4.09644 \tLatent loss: 0.858535\n",
      "681 Train total loss: 4.87859 \tReconstruction loss: 4.02817 \tLatent loss: 0.850421\n",
      "682 Train total loss: 4.93936 \tReconstruction loss: 4.09452 \tLatent loss: 0.844839\n",
      "683 Train total loss: 4.87872 \tReconstruction loss: 4.03586 \tLatent loss: 0.842856\n",
      "684 Train total loss: 4.8529 \tReconstruction loss: 3.99466 \tLatent loss: 0.85824\n",
      "685 Train total loss: 4.82526 \tReconstruction loss: 3.99039 \tLatent loss: 0.834866\n",
      "686 Train total loss: 4.83093 \tReconstruction loss: 3.99518 \tLatent loss: 0.835744\n",
      "687 Train total loss: 4.89863 \tReconstruction loss: 4.07248 \tLatent loss: 0.826151\n",
      "688 Train total loss: 4.83957 \tReconstruction loss: 4.01491 \tLatent loss: 0.824653\n",
      "689 Train total loss: 4.86899 \tReconstruction loss: 4.04759 \tLatent loss: 0.8214\n",
      "690 Train total loss: 4.89014 \tReconstruction loss: 4.07354 \tLatent loss: 0.816593\n",
      "691 Train total loss: 4.83759 \tReconstruction loss: 4.02532 \tLatent loss: 0.81227\n",
      "692 Train total loss: 4.84723 \tReconstruction loss: 4.03411 \tLatent loss: 0.813124\n",
      "693 Train total loss: 4.85925 \tReconstruction loss: 4.04942 \tLatent loss: 0.809834\n",
      "694 Train total loss: 4.93035 \tReconstruction loss: 4.12302 \tLatent loss: 0.807328\n",
      "695 Train total loss: 4.81175 \tReconstruction loss: 4.0064 \tLatent loss: 0.805356\n",
      "696 Train total loss: 4.81003 \tReconstruction loss: 4.00821 \tLatent loss: 0.801815\n",
      "697 Train total loss: 4.82222 \tReconstruction loss: 4.02431 \tLatent loss: 0.797912\n",
      "698 Train total loss: 4.75497 \tReconstruction loss: 3.9591 \tLatent loss: 0.795864\n",
      "699 Train total loss: 4.84327 \tReconstruction loss: 4.05238 \tLatent loss: 0.790885\n",
      "700 Train total loss: 4.81774 \tReconstruction loss: 4.03022 \tLatent loss: 0.78752\n",
      "701 Train total loss: 4.7725 \tReconstruction loss: 3.99255 \tLatent loss: 0.77995\n",
      "702 Train total loss: 4.74657 \tReconstruction loss: 3.97005 \tLatent loss: 0.776514\n",
      "703 Train total loss: 4.77667 \tReconstruction loss: 4.00005 \tLatent loss: 0.776615\n",
      "704 Train total loss: 4.77758 \tReconstruction loss: 3.98298 \tLatent loss: 0.794604\n",
      "705 Train total loss: 4.75169 \tReconstruction loss: 3.97529 \tLatent loss: 0.776393\n",
      "706 Train total loss: 4.71872 \tReconstruction loss: 3.94344 \tLatent loss: 0.775279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707 Train total loss: 4.73698 \tReconstruction loss: 3.96968 \tLatent loss: 0.767304\n",
      "708 Train total loss: 4.73164 \tReconstruction loss: 3.96958 \tLatent loss: 0.762055\n",
      "709 Train total loss: 4.72795 \tReconstruction loss: 3.96839 \tLatent loss: 0.759561\n",
      "710 Train total loss: 4.71095 \tReconstruction loss: 3.95699 \tLatent loss: 0.753957\n",
      "711 Train total loss: 4.70867 \tReconstruction loss: 3.95966 \tLatent loss: 0.749008\n",
      "712 Train total loss: 4.76281 \tReconstruction loss: 4.01412 \tLatent loss: 0.748696\n",
      "713 Train total loss: 4.73677 \tReconstruction loss: 3.99363 \tLatent loss: 0.743137\n",
      "714 Train total loss: 4.77969 \tReconstruction loss: 4.03727 \tLatent loss: 0.742423\n",
      "715 Train total loss: 4.77384 \tReconstruction loss: 4.03489 \tLatent loss: 0.738952\n",
      "716 Train total loss: 4.71934 \tReconstruction loss: 3.9813 \tLatent loss: 0.738039\n",
      "717 Train total loss: 4.74192 \tReconstruction loss: 4.00485 \tLatent loss: 0.737064\n",
      "718 Train total loss: 4.67362 \tReconstruction loss: 3.93772 \tLatent loss: 0.735908\n",
      "719 Train total loss: 4.66821 \tReconstruction loss: 3.93709 \tLatent loss: 0.731121\n",
      "720 Train total loss: 4.72038 \tReconstruction loss: 3.99007 \tLatent loss: 0.730306\n",
      "721 Train total loss: 4.67294 \tReconstruction loss: 3.94671 \tLatent loss: 0.726228\n",
      "722 Train total loss: 4.69529 \tReconstruction loss: 3.97064 \tLatent loss: 0.724649\n",
      "723 Train total loss: 4.75256 \tReconstruction loss: 4.03316 \tLatent loss: 0.719397\n",
      "724 Train total loss: 4.69351 \tReconstruction loss: 3.95763 \tLatent loss: 0.735878\n",
      "725 Train total loss: 4.6718 \tReconstruction loss: 3.95795 \tLatent loss: 0.713852\n",
      "726 Train total loss: 4.66185 \tReconstruction loss: 3.94913 \tLatent loss: 0.712719\n",
      "727 Train total loss: 4.68474 \tReconstruction loss: 3.97497 \tLatent loss: 0.709771\n",
      "728 Train total loss: 4.64323 \tReconstruction loss: 3.93804 \tLatent loss: 0.705182\n",
      "729 Train total loss: 4.63715 \tReconstruction loss: 3.93583 \tLatent loss: 0.701318\n",
      "730 Train total loss: 4.65562 \tReconstruction loss: 3.9605 \tLatent loss: 0.695126\n",
      "731 Train total loss: 4.6404 \tReconstruction loss: 3.95037 \tLatent loss: 0.690036\n",
      "732 Train total loss: 4.63262 \tReconstruction loss: 3.94753 \tLatent loss: 0.685095\n",
      "733 Train total loss: 4.7203 \tReconstruction loss: 4.03776 \tLatent loss: 0.682539\n",
      "734 Train total loss: 4.70233 \tReconstruction loss: 4.01978 \tLatent loss: 0.682551\n",
      "735 Train total loss: 4.63604 \tReconstruction loss: 3.95459 \tLatent loss: 0.681455\n",
      "736 Train total loss: 4.63956 \tReconstruction loss: 3.96077 \tLatent loss: 0.678787\n",
      "737 Train total loss: 4.61124 \tReconstruction loss: 3.92903 \tLatent loss: 0.682211\n",
      "738 Train total loss: 4.65382 \tReconstruction loss: 3.97466 \tLatent loss: 0.679157\n",
      "739 Train total loss: 4.62142 \tReconstruction loss: 3.94947 \tLatent loss: 0.671957\n",
      "740 Train total loss: 4.66102 \tReconstruction loss: 3.98992 \tLatent loss: 0.671094\n",
      "741 Train total loss: 4.62053 \tReconstruction loss: 3.94811 \tLatent loss: 0.672416\n",
      "742 Train total loss: 4.59238 \tReconstruction loss: 3.92202 \tLatent loss: 0.670361\n",
      "743 Train total loss: 4.62148 \tReconstruction loss: 3.95244 \tLatent loss: 0.669043\n",
      "744 Train total loss: 4.60383 \tReconstruction loss: 3.92382 \tLatent loss: 0.680011\n",
      "745 Train total loss: 4.56101 \tReconstruction loss: 3.90251 \tLatent loss: 0.6585\n",
      "746 Train total loss: 4.5946 \tReconstruction loss: 3.93598 \tLatent loss: 0.658617\n",
      "747 Train total loss: 4.60548 \tReconstruction loss: 3.95037 \tLatent loss: 0.655108\n",
      "748 Train total loss: 4.55581 \tReconstruction loss: 3.90471 \tLatent loss: 0.651104\n",
      "749 Train total loss: 4.56348 \tReconstruction loss: 3.91526 \tLatent loss: 0.648219\n",
      "750 Train total loss: 4.58547 \tReconstruction loss: 3.93769 \tLatent loss: 0.647778\n",
      "751 Train total loss: 4.57933 \tReconstruction loss: 3.93736 \tLatent loss: 0.641968\n",
      "752 Train total loss: 4.59123 \tReconstruction loss: 3.94701 \tLatent loss: 0.644213\n",
      "753 Train total loss: 4.61688 \tReconstruction loss: 3.97211 \tLatent loss: 0.644777\n",
      "754 Train total loss: 4.60992 \tReconstruction loss: 3.95888 \tLatent loss: 0.651034\n",
      "755 Train total loss: 4.55039 \tReconstruction loss: 3.90748 \tLatent loss: 0.642904\n",
      "756 Train total loss: 4.58475 \tReconstruction loss: 3.94816 \tLatent loss: 0.636593\n",
      "757 Train total loss: 4.53656 \tReconstruction loss: 3.9095 \tLatent loss: 0.627061\n",
      "758 Train total loss: 4.51947 \tReconstruction loss: 3.89435 \tLatent loss: 0.625118\n",
      "759 Train total loss: 4.63054 \tReconstruction loss: 4.00788 \tLatent loss: 0.622657\n",
      "760 Train total loss: 4.57129 \tReconstruction loss: 3.94391 \tLatent loss: 0.627378\n",
      "761 Train total loss: 4.565 \tReconstruction loss: 3.94286 \tLatent loss: 0.622138\n",
      "762 Train total loss: 4.58569 \tReconstruction loss: 3.96683 \tLatent loss: 0.618859\n",
      "763 Train total loss: 4.57025 \tReconstruction loss: 3.95018 \tLatent loss: 0.62007\n",
      "764 Train total loss: 4.52462 \tReconstruction loss: 3.88968 \tLatent loss: 0.634932\n",
      "765 Train total loss: 4.49899 \tReconstruction loss: 3.8814 \tLatent loss: 0.617589\n",
      "766 Train total loss: 4.497 \tReconstruction loss: 3.88184 \tLatent loss: 0.615156\n",
      "767 Train total loss: 4.53963 \tReconstruction loss: 3.93403 \tLatent loss: 0.605599\n",
      "768 Train total loss: 4.50552 \tReconstruction loss: 3.9005 \tLatent loss: 0.605026\n",
      "769 Train total loss: 4.52423 \tReconstruction loss: 3.92465 \tLatent loss: 0.599582\n",
      "770 Train total loss: 4.50554 \tReconstruction loss: 3.90683 \tLatent loss: 0.598701\n",
      "771 Train total loss: 4.50661 \tReconstruction loss: 3.91202 \tLatent loss: 0.594591\n",
      "772 Train total loss: 4.50631 \tReconstruction loss: 3.91516 \tLatent loss: 0.591152\n",
      "773 Train total loss: 4.50106 \tReconstruction loss: 3.9145 \tLatent loss: 0.58656\n",
      "774 Train total loss: 4.59215 \tReconstruction loss: 4.00686 \tLatent loss: 0.585283\n",
      "775 Train total loss: 4.48914 \tReconstruction loss: 3.89713 \tLatent loss: 0.592006\n",
      "776 Train total loss: 4.48668 \tReconstruction loss: 3.8981 \tLatent loss: 0.588583\n",
      "777 Train total loss: 4.50361 \tReconstruction loss: 3.91858 \tLatent loss: 0.585025\n",
      "778 Train total loss: 4.50274 \tReconstruction loss: 3.91464 \tLatent loss: 0.588094\n",
      "779 Train total loss: 4.48477 \tReconstruction loss: 3.89896 \tLatent loss: 0.585805\n",
      "780 Train total loss: 4.52481 \tReconstruction loss: 3.93879 \tLatent loss: 0.586029\n",
      "781 Train total loss: 4.48103 \tReconstruction loss: 3.89979 \tLatent loss: 0.581243\n",
      "782 Train total loss: 4.62265 \tReconstruction loss: 4.04417 \tLatent loss: 0.578486\n",
      "783 Train total loss: 4.48518 \tReconstruction loss: 3.9005 \tLatent loss: 0.584687\n",
      "784 Train total loss: 4.52095 \tReconstruction loss: 3.91492 \tLatent loss: 0.60603\n",
      "785 Train total loss: 4.45399 \tReconstruction loss: 3.8702 \tLatent loss: 0.583791\n",
      "786 Train total loss: 4.46394 \tReconstruction loss: 3.88045 \tLatent loss: 0.583496\n",
      "787 Train total loss: 4.44732 \tReconstruction loss: 3.87654 \tLatent loss: 0.570781\n",
      "788 Train total loss: 4.483 \tReconstruction loss: 3.92195 \tLatent loss: 0.561051\n",
      "789 Train total loss: 4.49183 \tReconstruction loss: 3.93647 \tLatent loss: 0.555363\n",
      "790 Train total loss: 4.43912 \tReconstruction loss: 3.88129 \tLatent loss: 0.557824\n",
      "791 Train total loss: 4.42138 \tReconstruction loss: 3.86812 \tLatent loss: 0.553259\n",
      "792 Train total loss: 4.46334 \tReconstruction loss: 3.89972 \tLatent loss: 0.563617\n",
      "793 Train total loss: 4.53226 \tReconstruction loss: 3.97433 \tLatent loss: 0.55793\n",
      "794 Train total loss: 4.47621 \tReconstruction loss: 3.92013 \tLatent loss: 0.556078\n",
      "795 Train total loss: 4.42736 \tReconstruction loss: 3.87081 \tLatent loss: 0.556549\n",
      "796 Train total loss: 4.4321 \tReconstruction loss: 3.87989 \tLatent loss: 0.552209\n",
      "797 Train total loss: 4.43164 \tReconstruction loss: 3.88144 \tLatent loss: 0.550204\n",
      "798 Train total loss: 4.43753 \tReconstruction loss: 3.89478 \tLatent loss: 0.542749\n",
      "799 Train total loss: 4.42312 \tReconstruction loss: 3.87456 \tLatent loss: 0.548558\n",
      "800 Train total loss: 4.4977 \tReconstruction loss: 3.95287 \tLatent loss: 0.544832\n",
      "801 Train total loss: 4.41176 \tReconstruction loss: 3.87574 \tLatent loss: 0.536018\n",
      "802 Train total loss: 4.45555 \tReconstruction loss: 3.9201 \tLatent loss: 0.53545\n",
      "803 Train total loss: 4.42689 \tReconstruction loss: 3.87828 \tLatent loss: 0.548616\n",
      "804 Train total loss: 4.41722 \tReconstruction loss: 3.86431 \tLatent loss: 0.552915\n",
      "805 Train total loss: 4.40444 \tReconstruction loss: 3.85743 \tLatent loss: 0.547004\n",
      "806 Train total loss: 4.57436 \tReconstruction loss: 4.02979 \tLatent loss: 0.544572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807 Train total loss: 4.41966 \tReconstruction loss: 3.87974 \tLatent loss: 0.539926\n",
      "808 Train total loss: 4.41342 \tReconstruction loss: 3.87547 \tLatent loss: 0.537946\n",
      "809 Train total loss: 4.40691 \tReconstruction loss: 3.87395 \tLatent loss: 0.532953\n",
      "810 Train total loss: 4.40051 \tReconstruction loss: 3.87252 \tLatent loss: 0.527986\n",
      "811 Train total loss: 4.38303 \tReconstruction loss: 3.84634 \tLatent loss: 0.536694\n",
      "812 Train total loss: 4.42765 \tReconstruction loss: 3.85898 \tLatent loss: 0.568672\n",
      "813 Train total loss: 4.4418 \tReconstruction loss: 3.87266 \tLatent loss: 0.569136\n",
      "814 Train total loss: 4.44123 \tReconstruction loss: 3.89986 \tLatent loss: 0.541368\n",
      "815 Train total loss: 4.40623 \tReconstruction loss: 3.87072 \tLatent loss: 0.535512\n",
      "816 Train total loss: 4.38975 \tReconstruction loss: 3.86719 \tLatent loss: 0.522559\n",
      "817 Train total loss: 4.39663 \tReconstruction loss: 3.88081 \tLatent loss: 0.515819\n",
      "818 Train total loss: 4.40672 \tReconstruction loss: 3.8917 \tLatent loss: 0.515019\n",
      "819 Train total loss: 4.41196 \tReconstruction loss: 3.89227 \tLatent loss: 0.519695\n",
      "820 Train total loss: 4.42315 \tReconstruction loss: 3.88839 \tLatent loss: 0.534762\n",
      "821 Train total loss: 4.40867 \tReconstruction loss: 3.87421 \tLatent loss: 0.534466\n",
      "822 Train total loss: 4.38336 \tReconstruction loss: 3.86211 \tLatent loss: 0.521249\n",
      "823 Train total loss: 4.64276 \tReconstruction loss: 4.13163 \tLatent loss: 0.511133\n",
      "824 Train total loss: 4.36852 \tReconstruction loss: 3.85601 \tLatent loss: 0.512511\n",
      "825 Train total loss: 4.37356 \tReconstruction loss: 3.86215 \tLatent loss: 0.511409\n",
      "826 Train total loss: 4.35295 \tReconstruction loss: 3.84414 \tLatent loss: 0.508806\n",
      "827 Train total loss: 4.37693 \tReconstruction loss: 3.87425 \tLatent loss: 0.502681\n",
      "828 Train total loss: 4.35634 \tReconstruction loss: 3.85703 \tLatent loss: 0.499316\n",
      "829 Train total loss: 4.38311 \tReconstruction loss: 3.87935 \tLatent loss: 0.503768\n",
      "830 Train total loss: 4.36961 \tReconstruction loss: 3.86569 \tLatent loss: 0.503911\n",
      "831 Train total loss: 4.35915 \tReconstruction loss: 3.86337 \tLatent loss: 0.495775\n",
      "832 Train total loss: 4.36834 \tReconstruction loss: 3.85238 \tLatent loss: 0.51596\n",
      "833 Train total loss: 4.42034 \tReconstruction loss: 3.91966 \tLatent loss: 0.500677\n",
      "834 Train total loss: 4.38477 \tReconstruction loss: 3.89148 \tLatent loss: 0.49329\n",
      "835 Train total loss: 4.35742 \tReconstruction loss: 3.85285 \tLatent loss: 0.504562\n",
      "836 Train total loss: 4.40277 \tReconstruction loss: 3.84685 \tLatent loss: 0.555923\n",
      "837 Train total loss: 4.362 \tReconstruction loss: 3.84452 \tLatent loss: 0.517482\n",
      "838 Train total loss: 4.3401 \tReconstruction loss: 3.85363 \tLatent loss: 0.486464\n",
      "839 Train total loss: 4.35138 \tReconstruction loss: 3.8495 \tLatent loss: 0.50188\n",
      "840 Train total loss: 4.39736 \tReconstruction loss: 3.90264 \tLatent loss: 0.494722\n",
      "841 Train total loss: 4.35209 \tReconstruction loss: 3.84723 \tLatent loss: 0.504862\n",
      "842 Train total loss: 4.34558 \tReconstruction loss: 3.85878 \tLatent loss: 0.486799\n",
      "843 Train total loss: 4.35044 \tReconstruction loss: 3.86877 \tLatent loss: 0.481675\n",
      "844 Train total loss: 4.34791 \tReconstruction loss: 3.86859 \tLatent loss: 0.479323\n",
      "845 Train total loss: 4.46241 \tReconstruction loss: 3.99213 \tLatent loss: 0.470282\n",
      "846 Train total loss: 4.33172 \tReconstruction loss: 3.85511 \tLatent loss: 0.476616\n",
      "847 Train total loss: 4.32259 \tReconstruction loss: 3.85572 \tLatent loss: 0.466865\n",
      "848 Train total loss: 4.34174 \tReconstruction loss: 3.86813 \tLatent loss: 0.473611\n",
      "849 Train total loss: 4.32837 \tReconstruction loss: 3.85921 \tLatent loss: 0.469161\n",
      "850 Train total loss: 4.32544 \tReconstruction loss: 3.84554 \tLatent loss: 0.479898\n",
      "851 Train total loss: 4.3609 \tReconstruction loss: 3.89891 \tLatent loss: 0.461987\n",
      "852 Train total loss: 4.48919 \tReconstruction loss: 4.03589 \tLatent loss: 0.453304\n",
      "853 Train total loss: 4.3685 \tReconstruction loss: 3.90287 \tLatent loss: 0.465634\n",
      "854 Train total loss: 4.36942 \tReconstruction loss: 3.88646 \tLatent loss: 0.482961\n",
      "855 Train total loss: 4.30155 \tReconstruction loss: 3.83521 \tLatent loss: 0.466342\n",
      "856 Train total loss: 4.35613 \tReconstruction loss: 3.89499 \tLatent loss: 0.461146\n",
      "857 Train total loss: 4.29667 \tReconstruction loss: 3.83116 \tLatent loss: 0.465508\n",
      "858 Train total loss: 4.36102 \tReconstruction loss: 3.89171 \tLatent loss: 0.469318\n",
      "859 Train total loss: 4.2952 \tReconstruction loss: 3.83302 \tLatent loss: 0.462186\n",
      "860 Train total loss: 4.34126 \tReconstruction loss: 3.85512 \tLatent loss: 0.486143\n",
      "861 Train total loss: 4.41968 \tReconstruction loss: 3.93829 \tLatent loss: 0.481394\n",
      "862 Train total loss: 4.3228 \tReconstruction loss: 3.83844 \tLatent loss: 0.484359\n",
      "863 Train total loss: 4.30754 \tReconstruction loss: 3.85774 \tLatent loss: 0.449801\n",
      "864 Train total loss: 4.30189 \tReconstruction loss: 3.85695 \tLatent loss: 0.444942\n",
      "865 Train total loss: 4.35368 \tReconstruction loss: 3.89195 \tLatent loss: 0.461725\n",
      "866 Train total loss: 4.29055 \tReconstruction loss: 3.81613 \tLatent loss: 0.474415\n",
      "867 Train total loss: 4.30091 \tReconstruction loss: 3.82317 \tLatent loss: 0.47774\n",
      "868 Train total loss: 4.33478 \tReconstruction loss: 3.89446 \tLatent loss: 0.440322\n",
      "869 Train total loss: 4.37561 \tReconstruction loss: 3.92684 \tLatent loss: 0.448769\n",
      "870 Train total loss: 4.2675 \tReconstruction loss: 3.81925 \tLatent loss: 0.448256\n",
      "871 Train total loss: 4.31063 \tReconstruction loss: 3.8058 \tLatent loss: 0.504827\n",
      "872 Train total loss: 4.31146 \tReconstruction loss: 3.8499 \tLatent loss: 0.461565\n",
      "873 Train total loss: 4.40881 \tReconstruction loss: 3.97495 \tLatent loss: 0.433861\n",
      "874 Train total loss: 4.39474 \tReconstruction loss: 3.95029 \tLatent loss: 0.444446\n",
      "875 Train total loss: 4.31518 \tReconstruction loss: 3.87014 \tLatent loss: 0.44504\n",
      "876 Train total loss: 4.29605 \tReconstruction loss: 3.85535 \tLatent loss: 0.440703\n",
      "877 Train total loss: 4.27009 \tReconstruction loss: 3.82598 \tLatent loss: 0.444112\n",
      "878 Train total loss: 4.73319 \tReconstruction loss: 4.31154 \tLatent loss: 0.421656\n",
      "879 Train total loss: 4.29529 \tReconstruction loss: 3.86786 \tLatent loss: 0.427432\n",
      "880 Train total loss: 4.32763 \tReconstruction loss: 3.89007 \tLatent loss: 0.43756\n",
      "881 Train total loss: 4.26192 \tReconstruction loss: 3.84744 \tLatent loss: 0.414486\n",
      "882 Train total loss: 4.26697 \tReconstruction loss: 3.84589 \tLatent loss: 0.421077\n",
      "883 Train total loss: 4.29247 \tReconstruction loss: 3.81571 \tLatent loss: 0.476759\n",
      "884 Train total loss: 4.25506 \tReconstruction loss: 3.80814 \tLatent loss: 0.446915\n",
      "885 Train total loss: 4.24317 \tReconstruction loss: 3.82663 \tLatent loss: 0.416536\n",
      "886 Train total loss: 4.29852 \tReconstruction loss: 3.85496 \tLatent loss: 0.443558\n",
      "887 Train total loss: 4.25615 \tReconstruction loss: 3.82283 \tLatent loss: 0.433314\n",
      "888 Train total loss: 4.25805 \tReconstruction loss: 3.83799 \tLatent loss: 0.42006\n",
      "889 Train total loss: 4.2402 \tReconstruction loss: 3.82904 \tLatent loss: 0.41116\n",
      "890 Train total loss: 4.29837 \tReconstruction loss: 3.8769 \tLatent loss: 0.421468\n",
      "891 Train total loss: 4.34041 \tReconstruction loss: 3.92347 \tLatent loss: 0.416943\n",
      "892 Train total loss: 4.35469 \tReconstruction loss: 3.95211 \tLatent loss: 0.402585\n",
      "893 Train total loss: 4.26451 \tReconstruction loss: 3.85823 \tLatent loss: 0.406289\n",
      "894 Train total loss: 4.27124 \tReconstruction loss: 3.8608 \tLatent loss: 0.410437\n",
      "895 Train total loss: 4.22316 \tReconstruction loss: 3.81744 \tLatent loss: 0.405724\n",
      "896 Train total loss: 4.22468 \tReconstruction loss: 3.82373 \tLatent loss: 0.400953\n",
      "897 Train total loss: 4.22422 \tReconstruction loss: 3.81234 \tLatent loss: 0.41188\n",
      "898 Train total loss: 4.24244 \tReconstruction loss: 3.83866 \tLatent loss: 0.403779\n",
      "899 Train total loss: 4.24224 \tReconstruction loss: 3.82799 \tLatent loss: 0.414256\n",
      "900 Train total loss: 4.28721 \tReconstruction loss: 3.85859 \tLatent loss: 0.428615\n",
      "901 Train total loss: 4.22335 \tReconstruction loss: 3.81268 \tLatent loss: 0.410664\n",
      "902 Train total loss: 4.23265 \tReconstruction loss: 3.84022 \tLatent loss: 0.392432\n",
      "903 Train total loss: 4.25107 \tReconstruction loss: 3.83896 \tLatent loss: 0.412109\n",
      "904 Train total loss: 4.21132 \tReconstruction loss: 3.80307 \tLatent loss: 0.40825\n",
      "905 Train total loss: 4.20911 \tReconstruction loss: 3.80581 \tLatent loss: 0.403301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906 Train total loss: 4.22786 \tReconstruction loss: 3.83571 \tLatent loss: 0.392157\n",
      "907 Train total loss: 4.31193 \tReconstruction loss: 3.8076 \tLatent loss: 0.504328\n",
      "908 Train total loss: 4.28645 \tReconstruction loss: 3.79673 \tLatent loss: 0.489719\n",
      "909 Train total loss: 4.21456 \tReconstruction loss: 3.79971 \tLatent loss: 0.414853\n",
      "910 Train total loss: 4.22292 \tReconstruction loss: 3.8363 \tLatent loss: 0.386622\n",
      "911 Train total loss: 4.21815 \tReconstruction loss: 3.80894 \tLatent loss: 0.40921\n",
      "912 Train total loss: 4.98228 \tReconstruction loss: 4.5672 \tLatent loss: 0.415084\n",
      "913 Train total loss: 4.25429 \tReconstruction loss: 3.85496 \tLatent loss: 0.399325\n",
      "914 Train total loss: 4.25944 \tReconstruction loss: 3.86268 \tLatent loss: 0.396765\n",
      "915 Train total loss: 4.21258 \tReconstruction loss: 3.82337 \tLatent loss: 0.389206\n",
      "916 Train total loss: 4.18151 \tReconstruction loss: 3.80236 \tLatent loss: 0.37915\n",
      "917 Train total loss: 4.21215 \tReconstruction loss: 3.79718 \tLatent loss: 0.414978\n",
      "918 Train total loss: 4.20392 \tReconstruction loss: 3.79333 \tLatent loss: 0.410589\n",
      "919 Train total loss: 4.18531 \tReconstruction loss: 3.80079 \tLatent loss: 0.384518\n",
      "920 Train total loss: 4.21961 \tReconstruction loss: 3.83584 \tLatent loss: 0.383768\n",
      "921 Train total loss: 4.17926 \tReconstruction loss: 3.7871 \tLatent loss: 0.392158\n",
      "922 Train total loss: 4.19756 \tReconstruction loss: 3.80054 \tLatent loss: 0.397017\n",
      "923 Train total loss: 4.20825 \tReconstruction loss: 3.83377 \tLatent loss: 0.374483\n",
      "924 Train total loss: 4.18682 \tReconstruction loss: 3.79472 \tLatent loss: 0.392095\n",
      "925 Train total loss: 4.17664 \tReconstruction loss: 3.80253 \tLatent loss: 0.374113\n",
      "926 Train total loss: 4.16318 \tReconstruction loss: 3.77966 \tLatent loss: 0.383522\n",
      "927 Train total loss: 4.17224 \tReconstruction loss: 3.81023 \tLatent loss: 0.362007\n",
      "928 Train total loss: 4.17443 \tReconstruction loss: 3.79853 \tLatent loss: 0.375902\n",
      "929 Train total loss: 4.1679 \tReconstruction loss: 3.79854 \tLatent loss: 0.369367\n",
      "930 Train total loss: 4.17962 \tReconstruction loss: 3.79778 \tLatent loss: 0.381836\n",
      "931 Train total loss: 4.16003 \tReconstruction loss: 3.7759 \tLatent loss: 0.384129\n",
      "932 Train total loss: 4.17884 \tReconstruction loss: 3.82411 \tLatent loss: 0.354732\n",
      "933 Train total loss: 4.17945 \tReconstruction loss: 3.80335 \tLatent loss: 0.376098\n",
      "934 Train total loss: 4.17741 \tReconstruction loss: 3.82635 \tLatent loss: 0.351062\n",
      "935 Train total loss: 4.50288 \tReconstruction loss: 3.7797 \tLatent loss: 0.723173\n",
      "936 Train total loss: 4.49894 \tReconstruction loss: 3.7757 \tLatent loss: 0.723232\n",
      "937 Train total loss: 4.40452 \tReconstruction loss: 3.76811 \tLatent loss: 0.636409\n",
      "938 Train total loss: 4.30756 \tReconstruction loss: 3.75521 \tLatent loss: 0.552345\n",
      "939 Train total loss: 4.26309 \tReconstruction loss: 3.78035 \tLatent loss: 0.482737\n",
      "940 Train total loss: 4.2857 \tReconstruction loss: 3.84945 \tLatent loss: 0.436248\n",
      "941 Train total loss: 4.17535 \tReconstruction loss: 3.76855 \tLatent loss: 0.406804\n",
      "942 Train total loss: 4.17689 \tReconstruction loss: 3.79058 \tLatent loss: 0.386305\n",
      "943 Train total loss: 4.16934 \tReconstruction loss: 3.78759 \tLatent loss: 0.381752\n",
      "944 Train total loss: 4.12976 \tReconstruction loss: 3.75775 \tLatent loss: 0.372009\n",
      "945 Train total loss: 4.16561 \tReconstruction loss: 3.80793 \tLatent loss: 0.357682\n",
      "946 Train total loss: 4.12553 \tReconstruction loss: 3.7648 \tLatent loss: 0.360729\n",
      "947 Train total loss: 4.30185 \tReconstruction loss: 3.93806 \tLatent loss: 0.363795\n",
      "948 Train total loss: 4.1365 \tReconstruction loss: 3.77494 \tLatent loss: 0.361556\n",
      "949 Train total loss: 4.15678 \tReconstruction loss: 3.76408 \tLatent loss: 0.392698\n",
      "950 Train total loss: 4.13948 \tReconstruction loss: 3.76644 \tLatent loss: 0.373032\n",
      "951 Train total loss: 4.13831 \tReconstruction loss: 3.78328 \tLatent loss: 0.355031\n",
      "952 Train total loss: 4.20535 \tReconstruction loss: 3.78174 \tLatent loss: 0.423604\n",
      "953 Train total loss: 4.20596 \tReconstruction loss: 3.78502 \tLatent loss: 0.420936\n",
      "954 Train total loss: 4.19207 \tReconstruction loss: 3.81564 \tLatent loss: 0.37642\n",
      "955 Train total loss: 4.12669 \tReconstruction loss: 3.77208 \tLatent loss: 0.354614\n",
      "956 Train total loss: 4.12401 \tReconstruction loss: 3.76526 \tLatent loss: 0.35875\n",
      "957 Train total loss: 4.15404 \tReconstruction loss: 3.79827 \tLatent loss: 0.35577\n",
      "958 Train total loss: 4.10822 \tReconstruction loss: 3.7545 \tLatent loss: 0.353717\n",
      "959 Train total loss: 4.12613 \tReconstruction loss: 3.76288 \tLatent loss: 0.363243\n",
      "960 Train total loss: 4.22016 \tReconstruction loss: 3.86944 \tLatent loss: 0.350719\n",
      "961 Train total loss: 4.13328 \tReconstruction loss: 3.75413 \tLatent loss: 0.37915\n",
      "962 Train total loss: 4.14695 \tReconstruction loss: 3.79288 \tLatent loss: 0.354066\n",
      "963 Train total loss: 4.10091 \tReconstruction loss: 3.76296 \tLatent loss: 0.337944\n",
      "964 Train total loss: 4.09449 \tReconstruction loss: 3.75388 \tLatent loss: 0.340608\n",
      "965 Train total loss: 4.10613 \tReconstruction loss: 3.7637 \tLatent loss: 0.34243\n",
      "966 Train total loss: 4.19946 \tReconstruction loss: 3.73893 \tLatent loss: 0.460535\n",
      "967 Train total loss: 4.23491 \tReconstruction loss: 3.74871 \tLatent loss: 0.486195\n",
      "968 Train total loss: 4.18045 \tReconstruction loss: 3.75042 \tLatent loss: 0.430023\n",
      "969 Train total loss: 4.12134 \tReconstruction loss: 3.74239 \tLatent loss: 0.378953\n",
      "970 Train total loss: 4.10879 \tReconstruction loss: 3.75805 \tLatent loss: 0.350746\n",
      "971 Train total loss: 4.09025 \tReconstruction loss: 3.7427 \tLatent loss: 0.347551\n",
      "972 Train total loss: 4.12388 \tReconstruction loss: 3.78183 \tLatent loss: 0.342052\n",
      "973 Train total loss: 4.19596 \tReconstruction loss: 3.86597 \tLatent loss: 0.329984\n",
      "974 Train total loss: 4.1201 \tReconstruction loss: 3.79871 \tLatent loss: 0.321387\n",
      "975 Train total loss: 4.08474 \tReconstruction loss: 3.75083 \tLatent loss: 0.333916\n",
      "976 Train total loss: 4.10617 \tReconstruction loss: 3.76904 \tLatent loss: 0.337135\n",
      "977 Train total loss: 4.08382 \tReconstruction loss: 3.75134 \tLatent loss: 0.332481\n",
      "978 Train total loss: 4.05593 \tReconstruction loss: 3.73155 \tLatent loss: 0.324378\n",
      "979 Train total loss: 4.09808 \tReconstruction loss: 3.77125 \tLatent loss: 0.32683\n",
      "980 Train total loss: 4.13459 \tReconstruction loss: 3.81094 \tLatent loss: 0.323658\n",
      "981 Train total loss: 4.07711 \tReconstruction loss: 3.75821 \tLatent loss: 0.318905\n",
      "982 Train total loss: 4.07413 \tReconstruction loss: 3.74215 \tLatent loss: 0.331974\n",
      "983 Train total loss: 4.07167 \tReconstruction loss: 3.74135 \tLatent loss: 0.33032\n",
      "984 Train total loss: 4.10965 \tReconstruction loss: 3.79161 \tLatent loss: 0.31804\n",
      "985 Train total loss: 4.0534 \tReconstruction loss: 3.74168 \tLatent loss: 0.311721\n",
      "986 Train total loss: 4.0384 \tReconstruction loss: 3.73071 \tLatent loss: 0.30769\n",
      "987 Train total loss: 4.09469 \tReconstruction loss: 3.75918 \tLatent loss: 0.335515\n",
      "988 Train total loss: 4.09847 \tReconstruction loss: 3.76839 \tLatent loss: 0.33008\n",
      "989 Train total loss: 4.09781 \tReconstruction loss: 3.73033 \tLatent loss: 0.367487\n",
      "990 Train total loss: 4.08425 \tReconstruction loss: 3.74154 \tLatent loss: 0.342706\n",
      "991 Train total loss: 4.04044 \tReconstruction loss: 3.72278 \tLatent loss: 0.317665\n",
      "992 Train total loss: 4.09002 \tReconstruction loss: 3.75134 \tLatent loss: 0.338681\n",
      "993 Train total loss: 4.14366 \tReconstruction loss: 3.80036 \tLatent loss: 0.343303\n",
      "994 Train total loss: 4.18144 \tReconstruction loss: 3.84937 \tLatent loss: 0.332071\n",
      "995 Train total loss: 4.06085 \tReconstruction loss: 3.74155 \tLatent loss: 0.319305\n",
      "996 Train total loss: 4.05035 \tReconstruction loss: 3.73054 \tLatent loss: 0.319814\n",
      "997 Train total loss: 4.04387 \tReconstruction loss: 3.72114 \tLatent loss: 0.32273\n",
      "998 Train total loss: 4.03814 \tReconstruction loss: 3.70801 \tLatent loss: 0.330122\n",
      "999 Train total loss: 4.04545 \tReconstruction loss: 3.71781 \tLatent loss: 0.327642\n",
      "1000 Train total loss: 4.088 \tReconstruction loss: 3.76528 \tLatent loss: 0.322719\n",
      "1001 Train total loss: 4.69214 \tReconstruction loss: 4.34921 \tLatent loss: 0.342936\n",
      "1002 Train total loss: 4.06983 \tReconstruction loss: 3.72007 \tLatent loss: 0.34976\n",
      "1003 Train total loss: 4.04822 \tReconstruction loss: 3.73209 \tLatent loss: 0.316129\n",
      "1004 Train total loss: 4.08398 \tReconstruction loss: 3.7064 \tLatent loss: 0.377572\n",
      "1005 Train total loss: 4.07178 \tReconstruction loss: 3.70455 \tLatent loss: 0.367231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006 Train total loss: 4.0123 \tReconstruction loss: 3.68862 \tLatent loss: 0.323681\n",
      "1007 Train total loss: 4.04196 \tReconstruction loss: 3.72765 \tLatent loss: 0.314311\n",
      "1008 Train total loss: 4.02684 \tReconstruction loss: 3.72596 \tLatent loss: 0.300872\n",
      "1009 Train total loss: 4.03805 \tReconstruction loss: 3.72379 \tLatent loss: 0.314257\n",
      "1010 Train total loss: 4.04251 \tReconstruction loss: 3.70719 \tLatent loss: 0.335315\n",
      "1011 Train total loss: 4.02559 \tReconstruction loss: 3.72442 \tLatent loss: 0.301167\n",
      "1012 Train total loss: 4.08794 \tReconstruction loss: 3.71757 \tLatent loss: 0.370369\n",
      "1013 Train total loss: 4.09054 \tReconstruction loss: 3.74097 \tLatent loss: 0.349566\n",
      "1014 Train total loss: 4.1116 \tReconstruction loss: 3.80715 \tLatent loss: 0.304443\n",
      "1015 Train total loss: 4.04029 \tReconstruction loss: 3.70116 \tLatent loss: 0.339129\n",
      "1016 Train total loss: 4.04745 \tReconstruction loss: 3.71507 \tLatent loss: 0.332382\n",
      "1017 Train total loss: 3.99791 \tReconstruction loss: 3.70191 \tLatent loss: 0.295999\n",
      "1018 Train total loss: 4.0006 \tReconstruction loss: 3.70444 \tLatent loss: 0.296153\n",
      "1019 Train total loss: 4.00968 \tReconstruction loss: 3.69707 \tLatent loss: 0.312606\n",
      "1020 Train total loss: 4.06221 \tReconstruction loss: 3.734 \tLatent loss: 0.328215\n",
      "1021 Train total loss: 4.00183 \tReconstruction loss: 3.70091 \tLatent loss: 0.300922\n",
      "1022 Train total loss: 3.98607 \tReconstruction loss: 3.70799 \tLatent loss: 0.278076\n",
      "1023 Train total loss: 3.98926 \tReconstruction loss: 3.71571 \tLatent loss: 0.273545\n",
      "1024 Train total loss: 3.97383 \tReconstruction loss: 3.68868 \tLatent loss: 0.28515\n",
      "1025 Train total loss: 3.9795 \tReconstruction loss: 3.69223 \tLatent loss: 0.287273\n",
      "1026 Train total loss: 4.05746 \tReconstruction loss: 3.75483 \tLatent loss: 0.302636\n",
      "1027 Train total loss: 3.99245 \tReconstruction loss: 3.70849 \tLatent loss: 0.283962\n",
      "1028 Train total loss: 4.08202 \tReconstruction loss: 3.68732 \tLatent loss: 0.394707\n",
      "1029 Train total loss: 4.04188 \tReconstruction loss: 3.69072 \tLatent loss: 0.35116\n",
      "1030 Train total loss: 3.99619 \tReconstruction loss: 3.68729 \tLatent loss: 0.308898\n",
      "1031 Train total loss: 3.98512 \tReconstruction loss: 3.6937 \tLatent loss: 0.291419\n",
      "1032 Train total loss: 4.0337 \tReconstruction loss: 3.70372 \tLatent loss: 0.329982\n",
      "1033 Train total loss: 4.01943 \tReconstruction loss: 3.71799 \tLatent loss: 0.301438\n",
      "1034 Train total loss: 4.02377 \tReconstruction loss: 3.74492 \tLatent loss: 0.278842\n",
      "1035 Train total loss: 4.20696 \tReconstruction loss: 3.92004 \tLatent loss: 0.286912\n",
      "1036 Train total loss: 4.0115 \tReconstruction loss: 3.71886 \tLatent loss: 0.292642\n",
      "1037 Train total loss: 3.9911 \tReconstruction loss: 3.70851 \tLatent loss: 0.282596\n",
      "1038 Train total loss: 3.99645 \tReconstruction loss: 3.73354 \tLatent loss: 0.262906\n",
      "1039 Train total loss: 3.96492 \tReconstruction loss: 3.68552 \tLatent loss: 0.279396\n",
      "1040 Train total loss: 4.00046 \tReconstruction loss: 3.7162 \tLatent loss: 0.284261\n",
      "1041 Train total loss: 3.94944 \tReconstruction loss: 3.67325 \tLatent loss: 0.276193\n",
      "1042 Train total loss: 3.94099 \tReconstruction loss: 3.6771 \tLatent loss: 0.263889\n",
      "1043 Train total loss: 4.32134 \tReconstruction loss: 4.02961 \tLatent loss: 0.291729\n",
      "1044 Train total loss: 3.9503 \tReconstruction loss: 3.67503 \tLatent loss: 0.275273\n",
      "1045 Train total loss: 3.9356 \tReconstruction loss: 3.67682 \tLatent loss: 0.258783\n",
      "1046 Train total loss: 3.91281 \tReconstruction loss: 3.65391 \tLatent loss: 0.2589\n",
      "1047 Train total loss: 3.93565 \tReconstruction loss: 3.66979 \tLatent loss: 0.265856\n",
      "1048 Train total loss: 4.00076 \tReconstruction loss: 3.66988 \tLatent loss: 0.330878\n",
      "1049 Train total loss: 3.99334 \tReconstruction loss: 3.66928 \tLatent loss: 0.324064\n",
      "1050 Train total loss: 3.94208 \tReconstruction loss: 3.66251 \tLatent loss: 0.27957\n",
      "1051 Train total loss: 3.94975 \tReconstruction loss: 3.64473 \tLatent loss: 0.305023\n",
      "1052 Train total loss: 3.98339 \tReconstruction loss: 3.67336 \tLatent loss: 0.310024\n",
      "1053 Train total loss: 4.00731 \tReconstruction loss: 3.69011 \tLatent loss: 0.317198\n",
      "1054 Train total loss: 4.01717 \tReconstruction loss: 3.72991 \tLatent loss: 0.287262\n",
      "1055 Train total loss: 3.94756 \tReconstruction loss: 3.66865 \tLatent loss: 0.278911\n",
      "1056 Train total loss: 3.97378 \tReconstruction loss: 3.65171 \tLatent loss: 0.322072\n",
      "1057 Train total loss: 3.93809 \tReconstruction loss: 3.65354 \tLatent loss: 0.28455\n",
      "1058 Train total loss: 3.90308 \tReconstruction loss: 3.63928 \tLatent loss: 0.263801\n",
      "1059 Train total loss: 3.93921 \tReconstruction loss: 3.65377 \tLatent loss: 0.285436\n",
      "1060 Train total loss: 3.96625 \tReconstruction loss: 3.69683 \tLatent loss: 0.26942\n",
      "1061 Train total loss: 3.936 \tReconstruction loss: 3.68776 \tLatent loss: 0.248233\n",
      "1062 Train total loss: 3.98222 \tReconstruction loss: 3.64725 \tLatent loss: 0.334975\n",
      "1063 Train total loss: 4.21504 \tReconstruction loss: 3.64836 \tLatent loss: 0.566682\n",
      "1064 Train total loss: 4.16102 \tReconstruction loss: 3.63722 \tLatent loss: 0.523804\n",
      "1065 Train total loss: 4.08462 \tReconstruction loss: 3.63068 \tLatent loss: 0.453942\n",
      "1066 Train total loss: 4.00506 \tReconstruction loss: 3.60997 \tLatent loss: 0.395088\n",
      "1067 Train total loss: 3.98459 \tReconstruction loss: 3.63874 \tLatent loss: 0.345846\n",
      "1068 Train total loss: 3.94481 \tReconstruction loss: 3.63641 \tLatent loss: 0.308403\n",
      "1069 Train total loss: 3.94292 \tReconstruction loss: 3.6605 \tLatent loss: 0.282411\n",
      "1070 Train total loss: 3.91951 \tReconstruction loss: 3.6522 \tLatent loss: 0.267305\n",
      "1071 Train total loss: 3.8798 \tReconstruction loss: 3.62218 \tLatent loss: 0.257625\n",
      "1072 Train total loss: 3.93623 \tReconstruction loss: 3.64974 \tLatent loss: 0.286492\n",
      "1073 Train total loss: 3.96109 \tReconstruction loss: 3.67202 \tLatent loss: 0.28907\n",
      "1074 Train total loss: 3.96338 \tReconstruction loss: 3.69665 \tLatent loss: 0.266726\n",
      "1075 Train total loss: 3.8854 \tReconstruction loss: 3.62734 \tLatent loss: 0.258058\n",
      "1076 Train total loss: 3.90108 \tReconstruction loss: 3.63882 \tLatent loss: 0.262263\n",
      "1077 Train total loss: 3.88996 \tReconstruction loss: 3.63375 \tLatent loss: 0.256213\n",
      "1078 Train total loss: 3.85635 \tReconstruction loss: 3.61138 \tLatent loss: 0.244969\n",
      "1079 Train total loss: 3.8846 \tReconstruction loss: 3.6411 \tLatent loss: 0.243502\n",
      "1080 Train total loss: 3.94817 \tReconstruction loss: 3.70615 \tLatent loss: 0.242026\n",
      "1081 Train total loss: 3.88945 \tReconstruction loss: 3.63155 \tLatent loss: 0.257908\n",
      "1082 Train total loss: 3.90373 \tReconstruction loss: 3.62315 \tLatent loss: 0.280585\n",
      "1083 Train total loss: 3.89938 \tReconstruction loss: 3.6353 \tLatent loss: 0.264074\n",
      "1084 Train total loss: 3.87962 \tReconstruction loss: 3.62489 \tLatent loss: 0.254735\n",
      "1085 Train total loss: 3.8506 \tReconstruction loss: 3.61186 \tLatent loss: 0.238745\n",
      "1086 Train total loss: 3.8473 \tReconstruction loss: 3.58798 \tLatent loss: 0.259321\n",
      "1087 Train total loss: 3.88866 \tReconstruction loss: 3.63641 \tLatent loss: 0.252247\n",
      "1088 Train total loss: 3.85251 \tReconstruction loss: 3.61905 \tLatent loss: 0.233457\n",
      "1089 Train total loss: 3.87052 \tReconstruction loss: 3.63555 \tLatent loss: 0.234973\n",
      "1090 Train total loss: 3.85053 \tReconstruction loss: 3.61463 \tLatent loss: 0.235902\n",
      "1091 Train total loss: 3.83031 \tReconstruction loss: 3.59712 \tLatent loss: 0.233199\n",
      "1092 Train total loss: 3.85179 \tReconstruction loss: 3.62522 \tLatent loss: 0.226563\n",
      "1093 Train total loss: 4.00453 \tReconstruction loss: 3.78469 \tLatent loss: 0.21984\n",
      "1094 Train total loss: 3.89734 \tReconstruction loss: 3.6611 \tLatent loss: 0.236239\n",
      "1095 Train total loss: 3.83665 \tReconstruction loss: 3.60454 \tLatent loss: 0.23211\n",
      "1096 Train total loss: 3.85336 \tReconstruction loss: 3.61541 \tLatent loss: 0.237948\n",
      "1097 Train total loss: 3.82476 \tReconstruction loss: 3.59551 \tLatent loss: 0.229249\n",
      "1098 Train total loss: 3.93278 \tReconstruction loss: 3.71467 \tLatent loss: 0.218113\n",
      "1099 Train total loss: 3.81919 \tReconstruction loss: 3.60626 \tLatent loss: 0.212929\n",
      "1100 Train total loss: 3.91286 \tReconstruction loss: 3.64146 \tLatent loss: 0.271393\n",
      "1101 Train total loss: 3.84262 \tReconstruction loss: 3.58966 \tLatent loss: 0.252959\n",
      "1102 Train total loss: 3.8444 \tReconstruction loss: 3.60906 \tLatent loss: 0.235344\n",
      "1103 Train total loss: 3.88906 \tReconstruction loss: 3.59031 \tLatent loss: 0.298752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104 Train total loss: 3.8556 \tReconstruction loss: 3.58205 \tLatent loss: 0.273553\n",
      "1105 Train total loss: 3.85828 \tReconstruction loss: 3.6183 \tLatent loss: 0.239983\n",
      "1106 Train total loss: 3.78287 \tReconstruction loss: 3.55244 \tLatent loss: 0.230433\n",
      "1107 Train total loss: 3.80584 \tReconstruction loss: 3.58776 \tLatent loss: 0.218078\n",
      "1108 Train total loss: 3.81075 \tReconstruction loss: 3.58381 \tLatent loss: 0.226935\n",
      "1109 Train total loss: 3.83862 \tReconstruction loss: 3.571 \tLatent loss: 0.267623\n",
      "1110 Train total loss: 3.8434 \tReconstruction loss: 3.58709 \tLatent loss: 0.256303\n",
      "1111 Train total loss: 3.78942 \tReconstruction loss: 3.56082 \tLatent loss: 0.228598\n",
      "1112 Train total loss: 3.81348 \tReconstruction loss: 3.60115 \tLatent loss: 0.212332\n",
      "1113 Train total loss: 3.85311 \tReconstruction loss: 3.64995 \tLatent loss: 0.203161\n",
      "1114 Train total loss: 3.87016 \tReconstruction loss: 3.63171 \tLatent loss: 0.238444\n",
      "1115 Train total loss: 3.79719 \tReconstruction loss: 3.56884 \tLatent loss: 0.228352\n",
      "1116 Train total loss: 3.93601 \tReconstruction loss: 3.71775 \tLatent loss: 0.218253\n",
      "1117 Train total loss: 3.765 \tReconstruction loss: 3.55751 \tLatent loss: 0.207492\n",
      "1118 Train total loss: 3.79445 \tReconstruction loss: 3.58756 \tLatent loss: 0.206889\n",
      "1119 Train total loss: 3.77744 \tReconstruction loss: 3.56319 \tLatent loss: 0.214251\n",
      "1120 Train total loss: 3.83198 \tReconstruction loss: 3.62605 \tLatent loss: 0.20593\n",
      "1121 Train total loss: 3.76169 \tReconstruction loss: 3.55794 \tLatent loss: 0.203751\n",
      "1122 Train total loss: 3.76244 \tReconstruction loss: 3.55781 \tLatent loss: 0.204621\n",
      "1123 Train total loss: 3.76058 \tReconstruction loss: 3.5618 \tLatent loss: 0.19878\n",
      "1124 Train total loss: 3.77491 \tReconstruction loss: 3.55794 \tLatent loss: 0.216979\n",
      "1125 Train total loss: 3.76763 \tReconstruction loss: 3.5486 \tLatent loss: 0.219027\n",
      "1126 Train total loss: 3.7448 \tReconstruction loss: 3.53694 \tLatent loss: 0.207856\n",
      "1127 Train total loss: 3.74765 \tReconstruction loss: 3.55359 \tLatent loss: 0.194061\n",
      "1128 Train total loss: 3.79835 \tReconstruction loss: 3.57495 \tLatent loss: 0.223403\n",
      "1129 Train total loss: 3.77299 \tReconstruction loss: 3.55367 \tLatent loss: 0.219313\n",
      "1130 Train total loss: 3.75586 \tReconstruction loss: 3.55271 \tLatent loss: 0.203146\n",
      "1131 Train total loss: 3.7378 \tReconstruction loss: 3.52478 \tLatent loss: 0.213024\n",
      "1132 Train total loss: 3.75662 \tReconstruction loss: 3.55547 \tLatent loss: 0.201148\n",
      "1133 Train total loss: 3.77999 \tReconstruction loss: 3.59013 \tLatent loss: 0.18986\n",
      "1134 Train total loss: 3.81346 \tReconstruction loss: 3.61842 \tLatent loss: 0.195039\n",
      "1135 Train total loss: 3.73941 \tReconstruction loss: 3.52217 \tLatent loss: 0.217241\n",
      "1136 Train total loss: 3.79836 \tReconstruction loss: 3.59907 \tLatent loss: 0.199292\n",
      "1137 Train total loss: 3.71848 \tReconstruction loss: 3.52792 \tLatent loss: 0.19056\n",
      "1138 Train total loss: 3.7448 \tReconstruction loss: 3.50688 \tLatent loss: 0.237918\n",
      "1139 Train total loss: 4.04106 \tReconstruction loss: 3.52851 \tLatent loss: 0.512554\n",
      "1140 Train total loss: 4.08573 \tReconstruction loss: 3.58061 \tLatent loss: 0.505125\n",
      "1141 Train total loss: 3.97535 \tReconstruction loss: 3.51636 \tLatent loss: 0.45899\n",
      "1142 Train total loss: 3.93456 \tReconstruction loss: 3.51588 \tLatent loss: 0.418678\n",
      "1143 Train total loss: 3.90492 \tReconstruction loss: 3.52032 \tLatent loss: 0.384606\n",
      "1144 Train total loss: 3.88041 \tReconstruction loss: 3.52336 \tLatent loss: 0.35706\n",
      "1145 Train total loss: 3.83353 \tReconstruction loss: 3.50064 \tLatent loss: 0.332895\n",
      "1146 Train total loss: 3.7876 \tReconstruction loss: 3.47566 \tLatent loss: 0.311934\n",
      "1147 Train total loss: 3.81047 \tReconstruction loss: 3.5172 \tLatent loss: 0.293263\n",
      "1148 Train total loss: 3.78339 \tReconstruction loss: 3.50674 \tLatent loss: 0.276657\n",
      "1149 Train total loss: 3.76051 \tReconstruction loss: 3.49853 \tLatent loss: 0.261978\n",
      "1150 Train total loss: 3.76263 \tReconstruction loss: 3.51366 \tLatent loss: 0.248967\n",
      "1151 Train total loss: 3.72761 \tReconstruction loss: 3.48973 \tLatent loss: 0.237883\n",
      "1152 Train total loss: 3.74955 \tReconstruction loss: 3.52238 \tLatent loss: 0.227171\n",
      "1153 Train total loss: 3.77357 \tReconstruction loss: 3.54859 \tLatent loss: 0.224985\n",
      "1154 Train total loss: 3.79724 \tReconstruction loss: 3.57773 \tLatent loss: 0.219508\n",
      "1155 Train total loss: 3.69784 \tReconstruction loss: 3.48539 \tLatent loss: 0.212449\n",
      "1156 Train total loss: 3.70626 \tReconstruction loss: 3.49695 \tLatent loss: 0.209312\n",
      "1157 Train total loss: 3.69626 \tReconstruction loss: 3.49038 \tLatent loss: 0.205874\n",
      "1158 Train total loss: 3.67212 \tReconstruction loss: 3.47166 \tLatent loss: 0.200453\n",
      "1159 Train total loss: 3.79396 \tReconstruction loss: 3.49148 \tLatent loss: 0.30248\n",
      "1160 Train total loss: 3.86442 \tReconstruction loss: 3.55281 \tLatent loss: 0.311613\n",
      "1161 Train total loss: 3.7723 \tReconstruction loss: 3.48059 \tLatent loss: 0.291713\n",
      "1162 Train total loss: 3.7527 \tReconstruction loss: 3.48044 \tLatent loss: 0.272261\n",
      "1163 Train total loss: 3.74256 \tReconstruction loss: 3.48742 \tLatent loss: 0.255142\n",
      "1164 Train total loss: 3.73703 \tReconstruction loss: 3.49616 \tLatent loss: 0.240865\n",
      "1165 Train total loss: 3.70079 \tReconstruction loss: 3.47207 \tLatent loss: 0.228725\n",
      "1166 Train total loss: 3.66232 \tReconstruction loss: 3.44312 \tLatent loss: 0.2192\n",
      "1167 Train total loss: 3.7009 \tReconstruction loss: 3.49066 \tLatent loss: 0.210246\n",
      "1168 Train total loss: 3.68764 \tReconstruction loss: 3.48476 \tLatent loss: 0.20288\n",
      "1169 Train total loss: 3.66622 \tReconstruction loss: 3.46928 \tLatent loss: 0.196936\n",
      "1170 Train total loss: 3.6775 \tReconstruction loss: 3.487 \tLatent loss: 0.1905\n",
      "1171 Train total loss: 3.68898 \tReconstruction loss: 3.46015 \tLatent loss: 0.228831\n",
      "1172 Train total loss: 3.72888 \tReconstruction loss: 3.48615 \tLatent loss: 0.242734\n",
      "1173 Train total loss: 3.75124 \tReconstruction loss: 3.52188 \tLatent loss: 0.229363\n",
      "1174 Train total loss: 3.75702 \tReconstruction loss: 3.53805 \tLatent loss: 0.218964\n",
      "1175 Train total loss: 3.65477 \tReconstruction loss: 3.44593 \tLatent loss: 0.20884\n",
      "1176 Train total loss: 3.65468 \tReconstruction loss: 3.4533 \tLatent loss: 0.201374\n",
      "1177 Train total loss: 3.64911 \tReconstruction loss: 3.44951 \tLatent loss: 0.199596\n",
      "1178 Train total loss: 3.62944 \tReconstruction loss: 3.4364 \tLatent loss: 0.193038\n",
      "1179 Train total loss: 3.66636 \tReconstruction loss: 3.47467 \tLatent loss: 0.191687\n",
      "1180 Train total loss: 3.7152 \tReconstruction loss: 3.52415 \tLatent loss: 0.191055\n",
      "1181 Train total loss: 3.634 \tReconstruction loss: 3.44825 \tLatent loss: 0.185751\n",
      "1182 Train total loss: 3.63895 \tReconstruction loss: 3.45741 \tLatent loss: 0.181541\n",
      "1183 Train total loss: 3.67433 \tReconstruction loss: 3.49111 \tLatent loss: 0.18322\n",
      "1184 Train total loss: 3.6759 \tReconstruction loss: 3.47002 \tLatent loss: 0.205874\n",
      "1185 Train total loss: 3.64771 \tReconstruction loss: 3.43392 \tLatent loss: 0.213789\n",
      "1186 Train total loss: 3.60999 \tReconstruction loss: 3.40702 \tLatent loss: 0.202966\n",
      "1187 Train total loss: 3.64921 \tReconstruction loss: 3.45733 \tLatent loss: 0.191878\n",
      "1188 Train total loss: 3.62007 \tReconstruction loss: 3.43728 \tLatent loss: 0.182789\n",
      "1189 Train total loss: 3.62716 \tReconstruction loss: 3.44812 \tLatent loss: 0.179047\n",
      "1190 Train total loss: 3.63841 \tReconstruction loss: 3.45393 \tLatent loss: 0.184478\n",
      "1191 Train total loss: 3.61646 \tReconstruction loss: 3.43456 \tLatent loss: 0.181898\n",
      "1192 Train total loss: 3.64255 \tReconstruction loss: 3.45113 \tLatent loss: 0.191415\n",
      "1193 Train total loss: 3.71757 \tReconstruction loss: 3.47624 \tLatent loss: 0.241331\n",
      "1194 Train total loss: 3.73797 \tReconstruction loss: 3.50672 \tLatent loss: 0.231246\n",
      "1195 Train total loss: 3.62031 \tReconstruction loss: 3.40441 \tLatent loss: 0.215899\n",
      "1196 Train total loss: 3.60901 \tReconstruction loss: 3.40701 \tLatent loss: 0.202\n",
      "1197 Train total loss: 3.59924 \tReconstruction loss: 3.40826 \tLatent loss: 0.190985\n",
      "1198 Train total loss: 3.57123 \tReconstruction loss: 3.38928 \tLatent loss: 0.181947\n",
      "1199 Train total loss: 3.59506 \tReconstruction loss: 3.42091 \tLatent loss: 0.174156\n",
      "1200 Train total loss: 3.69079 \tReconstruction loss: 3.51574 \tLatent loss: 0.175048\n",
      "1201 Train total loss: 3.58614 \tReconstruction loss: 3.40998 \tLatent loss: 0.176163\n",
      "1202 Train total loss: 4.48685 \tReconstruction loss: 4.3086 \tLatent loss: 0.178243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203 Train total loss: 3.60214 \tReconstruction loss: 3.42808 \tLatent loss: 0.174065\n",
      "1204 Train total loss: 3.61077 \tReconstruction loss: 3.44196 \tLatent loss: 0.16881\n",
      "1205 Train total loss: 3.62876 \tReconstruction loss: 3.46242 \tLatent loss: 0.166335\n",
      "1206 Train total loss: 3.54466 \tReconstruction loss: 3.37744 \tLatent loss: 0.167222\n",
      "1207 Train total loss: 3.58839 \tReconstruction loss: 3.4224 \tLatent loss: 0.165996\n",
      "1208 Train total loss: 3.5797 \tReconstruction loss: 3.41781 \tLatent loss: 0.161889\n",
      "1209 Train total loss: 3.56667 \tReconstruction loss: 3.39958 \tLatent loss: 0.167086\n",
      "1210 Train total loss: 3.62575 \tReconstruction loss: 3.4298 \tLatent loss: 0.195951\n",
      "1211 Train total loss: 3.58953 \tReconstruction loss: 3.40129 \tLatent loss: 0.188236\n",
      "1212 Train total loss: 3.60081 \tReconstruction loss: 3.42323 \tLatent loss: 0.177586\n",
      "1213 Train total loss: 3.63277 \tReconstruction loss: 3.45884 \tLatent loss: 0.173926\n",
      "1214 Train total loss: 3.66115 \tReconstruction loss: 3.49185 \tLatent loss: 0.169307\n",
      "1215 Train total loss: 3.63896 \tReconstruction loss: 3.47459 \tLatent loss: 0.164373\n",
      "1216 Train total loss: 3.53278 \tReconstruction loss: 3.37313 \tLatent loss: 0.159654\n",
      "1217 Train total loss: 3.53897 \tReconstruction loss: 3.38041 \tLatent loss: 0.15856\n",
      "1218 Train total loss: 3.51636 \tReconstruction loss: 3.35621 \tLatent loss: 0.160153\n",
      "1219 Train total loss: 3.5512 \tReconstruction loss: 3.39408 \tLatent loss: 0.157117\n",
      "1220 Train total loss: 3.62969 \tReconstruction loss: 3.47332 \tLatent loss: 0.156365\n",
      "1221 Train total loss: 3.54543 \tReconstruction loss: 3.38779 \tLatent loss: 0.157632\n",
      "1222 Train total loss: 3.53186 \tReconstruction loss: 3.37327 \tLatent loss: 0.158584\n",
      "1223 Train total loss: 3.54468 \tReconstruction loss: 3.38678 \tLatent loss: 0.157899\n",
      "1224 Train total loss: 3.57215 \tReconstruction loss: 3.41204 \tLatent loss: 0.160109\n",
      "1225 Train total loss: 3.5307 \tReconstruction loss: 3.36265 \tLatent loss: 0.168053\n",
      "1226 Train total loss: 3.50046 \tReconstruction loss: 3.33404 \tLatent loss: 0.16642\n",
      "1227 Train total loss: 3.55342 \tReconstruction loss: 3.38744 \tLatent loss: 0.165977\n",
      "1228 Train total loss: 3.52856 \tReconstruction loss: 3.36827 \tLatent loss: 0.160291\n",
      "1229 Train total loss: 3.5263 \tReconstruction loss: 3.37044 \tLatent loss: 0.15586\n",
      "1230 Train total loss: 3.5487 \tReconstruction loss: 3.39781 \tLatent loss: 0.150891\n",
      "1231 Train total loss: 3.51324 \tReconstruction loss: 3.36047 \tLatent loss: 0.152767\n",
      "1232 Train total loss: 3.53867 \tReconstruction loss: 3.38745 \tLatent loss: 0.151215\n",
      "1233 Train total loss: 3.70307 \tReconstruction loss: 3.41181 \tLatent loss: 0.291256\n",
      "1234 Train total loss: 3.73723 \tReconstruction loss: 3.43929 \tLatent loss: 0.297938\n",
      "1235 Train total loss: 3.59614 \tReconstruction loss: 3.32705 \tLatent loss: 0.26909\n",
      "1236 Train total loss: 3.57295 \tReconstruction loss: 3.32843 \tLatent loss: 0.244516\n",
      "1237 Train total loss: 3.55342 \tReconstruction loss: 3.32837 \tLatent loss: 0.225051\n",
      "1238 Train total loss: 3.51866 \tReconstruction loss: 3.3094 \tLatent loss: 0.209264\n",
      "1239 Train total loss: 3.53407 \tReconstruction loss: 3.33811 \tLatent loss: 0.195964\n",
      "1240 Train total loss: 3.61111 \tReconstruction loss: 3.42561 \tLatent loss: 0.185492\n",
      "1241 Train total loss: 3.513 \tReconstruction loss: 3.33761 \tLatent loss: 0.175385\n",
      "1242 Train total loss: 3.50118 \tReconstruction loss: 3.33418 \tLatent loss: 0.167008\n",
      "1243 Train total loss: 3.50874 \tReconstruction loss: 3.34816 \tLatent loss: 0.160579\n",
      "1244 Train total loss: 3.54684 \tReconstruction loss: 3.38562 \tLatent loss: 0.161221\n",
      "1245 Train total loss: 3.51266 \tReconstruction loss: 3.32753 \tLatent loss: 0.185126\n",
      "1246 Train total loss: 3.47226 \tReconstruction loss: 3.29434 \tLatent loss: 0.177917\n",
      "1247 Train total loss: 3.52196 \tReconstruction loss: 3.35269 \tLatent loss: 0.16927\n",
      "1248 Train total loss: 3.48864 \tReconstruction loss: 3.32543 \tLatent loss: 0.163208\n",
      "1249 Train total loss: 3.48538 \tReconstruction loss: 3.32243 \tLatent loss: 0.162946\n",
      "1250 Train total loss: 3.51514 \tReconstruction loss: 3.35707 \tLatent loss: 0.158075\n",
      "1251 Train total loss: 3.47746 \tReconstruction loss: 3.32421 \tLatent loss: 0.153255\n",
      "1252 Train total loss: 3.51268 \tReconstruction loss: 3.36239 \tLatent loss: 0.150296\n",
      "1253 Train total loss: 3.56172 \tReconstruction loss: 3.38535 \tLatent loss: 0.17637\n",
      "1254 Train total loss: 3.5955 \tReconstruction loss: 3.41123 \tLatent loss: 0.184266\n",
      "1255 Train total loss: 3.46466 \tReconstruction loss: 3.292 \tLatent loss: 0.172657\n",
      "1256 Train total loss: 3.45418 \tReconstruction loss: 3.29243 \tLatent loss: 0.161751\n",
      "1257 Train total loss: 3.45085 \tReconstruction loss: 3.29447 \tLatent loss: 0.156378\n",
      "1258 Train total loss: 3.45442 \tReconstruction loss: 3.2767 \tLatent loss: 0.177723\n",
      "1259 Train total loss: 3.51779 \tReconstruction loss: 3.30287 \tLatent loss: 0.21492\n",
      "1260 Train total loss: 3.58864 \tReconstruction loss: 3.38485 \tLatent loss: 0.203788\n",
      "1261 Train total loss: 3.48306 \tReconstruction loss: 3.29512 \tLatent loss: 0.187939\n",
      "1262 Train total loss: 3.47336 \tReconstruction loss: 3.29842 \tLatent loss: 0.174941\n",
      "1263 Train total loss: 3.4811 \tReconstruction loss: 3.31668 \tLatent loss: 0.164414\n",
      "1264 Train total loss: 3.52394 \tReconstruction loss: 3.36779 \tLatent loss: 0.156153\n",
      "1265 Train total loss: 3.81127 \tReconstruction loss: 3.65934 \tLatent loss: 0.151932\n",
      "1266 Train total loss: 3.42305 \tReconstruction loss: 3.2608 \tLatent loss: 0.162255\n",
      "1267 Train total loss: 3.47732 \tReconstruction loss: 3.31873 \tLatent loss: 0.158588\n",
      "1268 Train total loss: 3.45077 \tReconstruction loss: 3.29648 \tLatent loss: 0.15429\n",
      "1269 Train total loss: 3.44091 \tReconstruction loss: 3.29214 \tLatent loss: 0.148768\n",
      "1270 Train total loss: 3.47003 \tReconstruction loss: 3.3267 \tLatent loss: 0.143329\n",
      "1271 Train total loss: 3.43923 \tReconstruction loss: 3.29203 \tLatent loss: 0.147196\n",
      "1272 Train total loss: 3.46004 \tReconstruction loss: 3.31386 \tLatent loss: 0.146181\n",
      "1273 Train total loss: 3.50293 \tReconstruction loss: 3.35999 \tLatent loss: 0.142939\n",
      "1274 Train total loss: 3.53211 \tReconstruction loss: 3.38868 \tLatent loss: 0.14343\n",
      "1275 Train total loss: 3.40042 \tReconstruction loss: 3.25919 \tLatent loss: 0.141228\n",
      "1276 Train total loss: 3.45924 \tReconstruction loss: 3.25534 \tLatent loss: 0.203906\n",
      "1277 Train total loss: 3.45955 \tReconstruction loss: 3.25565 \tLatent loss: 0.203902\n",
      "1278 Train total loss: 3.41994 \tReconstruction loss: 3.23426 \tLatent loss: 0.185684\n",
      "1279 Train total loss: 3.43449 \tReconstruction loss: 3.26378 \tLatent loss: 0.170703\n",
      "1280 Train total loss: 3.52501 \tReconstruction loss: 3.36487 \tLatent loss: 0.160137\n",
      "1281 Train total loss: 3.41686 \tReconstruction loss: 3.26618 \tLatent loss: 0.150676\n",
      "1282 Train total loss: 3.41236 \tReconstruction loss: 3.2699 \tLatent loss: 0.142466\n",
      "1283 Train total loss: 3.42152 \tReconstruction loss: 3.28189 \tLatent loss: 0.13963\n",
      "1284 Train total loss: 3.48247 \tReconstruction loss: 3.3439 \tLatent loss: 0.138566\n",
      "1285 Train total loss: 3.39848 \tReconstruction loss: 3.26081 \tLatent loss: 0.137668\n",
      "1286 Train total loss: 3.35881 \tReconstruction loss: 3.22202 \tLatent loss: 0.136795\n",
      "1287 Train total loss: 3.4351 \tReconstruction loss: 3.29969 \tLatent loss: 0.135403\n",
      "1288 Train total loss: 3.59051 \tReconstruction loss: 3.25825 \tLatent loss: 0.33227\n",
      "1289 Train total loss: 3.57929 \tReconstruction loss: 3.25173 \tLatent loss: 0.327554\n",
      "1290 Train total loss: 3.58072 \tReconstruction loss: 3.28893 \tLatent loss: 0.291794\n",
      "1291 Train total loss: 3.52177 \tReconstruction loss: 3.25555 \tLatent loss: 0.266221\n",
      "1292 Train total loss: 3.51573 \tReconstruction loss: 3.27027 \tLatent loss: 0.245462\n",
      "1293 Train total loss: 3.55534 \tReconstruction loss: 3.32518 \tLatent loss: 0.230162\n",
      "1294 Train total loss: 3.5735 \tReconstruction loss: 3.35603 \tLatent loss: 0.217472\n",
      "1295 Train total loss: 3.42065 \tReconstruction loss: 3.21426 \tLatent loss: 0.206388\n",
      "1296 Train total loss: 3.40771 \tReconstruction loss: 3.21119 \tLatent loss: 0.196521\n",
      "1297 Train total loss: 3.40465 \tReconstruction loss: 3.21713 \tLatent loss: 0.187519\n",
      "1298 Train total loss: 3.37596 \tReconstruction loss: 3.19619 \tLatent loss: 0.179768\n",
      "1299 Train total loss: 3.39994 \tReconstruction loss: 3.2277 \tLatent loss: 0.172239\n",
      "1300 Train total loss: 3.49434 \tReconstruction loss: 3.328 \tLatent loss: 0.166344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1301 Train total loss: 3.38051 \tReconstruction loss: 3.22158 \tLatent loss: 0.158923\n",
      "1302 Train total loss: 3.37957 \tReconstruction loss: 3.22607 \tLatent loss: 0.153505\n",
      "1303 Train total loss: 3.40648 \tReconstruction loss: 3.25745 \tLatent loss: 0.149031\n",
      "1304 Train total loss: 3.45421 \tReconstruction loss: 3.30877 \tLatent loss: 0.145442\n",
      "1305 Train total loss: 3.36854 \tReconstruction loss: 3.22718 \tLatent loss: 0.141355\n",
      "1306 Train total loss: 3.33027 \tReconstruction loss: 3.1884 \tLatent loss: 0.141864\n",
      "1307 Train total loss: 3.39446 \tReconstruction loss: 3.25576 \tLatent loss: 0.138705\n",
      "1308 Train total loss: 3.35903 \tReconstruction loss: 3.22179 \tLatent loss: 0.137241\n",
      "1309 Train total loss: 3.36005 \tReconstruction loss: 3.22463 \tLatent loss: 0.135419\n",
      "1310 Train total loss: 3.39877 \tReconstruction loss: 3.26355 \tLatent loss: 0.135218\n",
      "1311 Train total loss: 3.3665 \tReconstruction loss: 3.23201 \tLatent loss: 0.134488\n",
      "1312 Train total loss: 3.376 \tReconstruction loss: 3.24446 \tLatent loss: 0.131543\n",
      "1313 Train total loss: 3.44925 \tReconstruction loss: 3.29775 \tLatent loss: 0.151501\n",
      "1314 Train total loss: 3.48513 \tReconstruction loss: 3.32672 \tLatent loss: 0.158414\n",
      "1315 Train total loss: 3.33571 \tReconstruction loss: 3.18453 \tLatent loss: 0.15118\n",
      "1316 Train total loss: 3.32486 \tReconstruction loss: 3.17959 \tLatent loss: 0.145264\n",
      "1317 Train total loss: 3.32658 \tReconstruction loss: 3.18616 \tLatent loss: 0.140418\n",
      "1318 Train total loss: 3.3026 \tReconstruction loss: 3.16544 \tLatent loss: 0.137165\n",
      "1319 Train total loss: 3.32937 \tReconstruction loss: 3.19386 \tLatent loss: 0.135513\n",
      "1320 Train total loss: 3.43388 \tReconstruction loss: 3.30052 \tLatent loss: 0.133358\n",
      "1321 Train total loss: 3.32283 \tReconstruction loss: 3.19182 \tLatent loss: 0.131011\n",
      "1322 Train total loss: 3.32787 \tReconstruction loss: 3.19683 \tLatent loss: 0.131043\n",
      "1323 Train total loss: 3.34766 \tReconstruction loss: 3.21814 \tLatent loss: 0.12952\n",
      "1324 Train total loss: 3.42205 \tReconstruction loss: 3.29178 \tLatent loss: 0.130275\n",
      "1325 Train total loss: 3.32412 \tReconstruction loss: 3.19526 \tLatent loss: 0.12886\n",
      "1326 Train total loss: 3.27899 \tReconstruction loss: 3.1529 \tLatent loss: 0.126087\n",
      "1327 Train total loss: 3.35668 \tReconstruction loss: 3.22595 \tLatent loss: 0.130736\n",
      "1328 Train total loss: 3.36623 \tReconstruction loss: 3.18976 \tLatent loss: 0.17647\n",
      "1329 Train total loss: 3.36486 \tReconstruction loss: 3.18685 \tLatent loss: 0.178009\n",
      "1330 Train total loss: 3.39795 \tReconstruction loss: 3.232 \tLatent loss: 0.165952\n",
      "1331 Train total loss: 3.35455 \tReconstruction loss: 3.19786 \tLatent loss: 0.156688\n",
      "1332 Train total loss: 3.35379 \tReconstruction loss: 3.20674 \tLatent loss: 0.14705\n",
      "1333 Train total loss: 3.41224 \tReconstruction loss: 3.2724 \tLatent loss: 0.139843\n",
      "1334 Train total loss: 3.45697 \tReconstruction loss: 3.32309 \tLatent loss: 0.133876\n",
      "1335 Train total loss: 3.28998 \tReconstruction loss: 3.16037 \tLatent loss: 0.129609\n",
      "1336 Train total loss: 3.27269 \tReconstruction loss: 3.14637 \tLatent loss: 0.126314\n",
      "1337 Train total loss: 3.2865 \tReconstruction loss: 3.16299 \tLatent loss: 0.123511\n",
      "1338 Train total loss: 3.28758 \tReconstruction loss: 3.13357 \tLatent loss: 0.154009\n",
      "1339 Train total loss: 3.32179 \tReconstruction loss: 3.15953 \tLatent loss: 0.162256\n",
      "1340 Train total loss: 3.42196 \tReconstruction loss: 3.26938 \tLatent loss: 0.15258\n",
      "1341 Train total loss: 3.29637 \tReconstruction loss: 3.15387 \tLatent loss: 0.142498\n",
      "1342 Train total loss: 3.30552 \tReconstruction loss: 3.17011 \tLatent loss: 0.135408\n",
      "1343 Train total loss: 3.31625 \tReconstruction loss: 3.18645 \tLatent loss: 0.1298\n",
      "1344 Train total loss: 3.39352 \tReconstruction loss: 3.26831 \tLatent loss: 0.125208\n",
      "1345 Train total loss: 3.28719 \tReconstruction loss: 3.164 \tLatent loss: 0.123184\n",
      "1346 Train total loss: 3.24609 \tReconstruction loss: 3.12511 \tLatent loss: 0.120982\n",
      "1347 Train total loss: 3.35671 \tReconstruction loss: 3.19478 \tLatent loss: 0.161937\n",
      "1348 Train total loss: 3.36668 \tReconstruction loss: 3.15646 \tLatent loss: 0.21022\n",
      "1349 Train total loss: 3.34946 \tReconstruction loss: 3.15345 \tLatent loss: 0.19601\n",
      "1350 Train total loss: 3.38769 \tReconstruction loss: 3.20804 \tLatent loss: 0.179651\n",
      "1351 Train total loss: 3.33739 \tReconstruction loss: 3.1693 \tLatent loss: 0.168096\n",
      "1352 Train total loss: 3.3331 \tReconstruction loss: 3.17612 \tLatent loss: 0.156986\n",
      "1353 Train total loss: 3.39544 \tReconstruction loss: 3.24588 \tLatent loss: 0.149554\n",
      "1354 Train total loss: 3.42259 \tReconstruction loss: 3.27898 \tLatent loss: 0.143614\n",
      "1355 Train total loss: 3.25654 \tReconstruction loss: 3.11779 \tLatent loss: 0.138747\n",
      "1356 Train total loss: 3.24304 \tReconstruction loss: 3.10934 \tLatent loss: 0.133698\n",
      "1357 Train total loss: 3.24878 \tReconstruction loss: 3.11889 \tLatent loss: 0.129883\n",
      "1358 Train total loss: 3.22376 \tReconstruction loss: 3.09837 \tLatent loss: 0.125388\n",
      "1359 Train total loss: 3.25015 \tReconstruction loss: 3.12763 \tLatent loss: 0.122527\n",
      "1360 Train total loss: 3.36006 \tReconstruction loss: 3.24032 \tLatent loss: 0.119747\n",
      "1361 Train total loss: 3.28507 \tReconstruction loss: 3.16836 \tLatent loss: 0.116705\n",
      "1362 Train total loss: 3.25399 \tReconstruction loss: 3.1392 \tLatent loss: 0.114782\n",
      "1363 Train total loss: 3.27149 \tReconstruction loss: 3.15921 \tLatent loss: 0.112275\n",
      "1364 Train total loss: 3.35755 \tReconstruction loss: 3.24611 \tLatent loss: 0.111442\n",
      "1365 Train total loss: 3.36252 \tReconstruction loss: 3.13847 \tLatent loss: 0.224053\n",
      "1366 Train total loss: 3.38887 \tReconstruction loss: 3.09372 \tLatent loss: 0.295152\n",
      "1367 Train total loss: 3.44219 \tReconstruction loss: 3.16587 \tLatent loss: 0.276321\n",
      "1368 Train total loss: 3.37666 \tReconstruction loss: 3.12199 \tLatent loss: 0.254662\n",
      "1369 Train total loss: 3.3608 \tReconstruction loss: 3.12243 \tLatent loss: 0.238365\n",
      "1370 Train total loss: 3.40951 \tReconstruction loss: 3.1837 \tLatent loss: 0.225809\n",
      "1371 Train total loss: 3.35871 \tReconstruction loss: 3.14219 \tLatent loss: 0.216522\n",
      "1372 Train total loss: 3.3523 \tReconstruction loss: 3.14532 \tLatent loss: 0.206981\n",
      "1373 Train total loss: 3.42005 \tReconstruction loss: 3.22089 \tLatent loss: 0.199157\n",
      "1374 Train total loss: 3.44682 \tReconstruction loss: 3.25459 \tLatent loss: 0.192227\n",
      "1375 Train total loss: 3.2666 \tReconstruction loss: 3.08075 \tLatent loss: 0.18585\n",
      "1376 Train total loss: 3.25484 \tReconstruction loss: 3.0749 \tLatent loss: 0.179942\n",
      "1377 Train total loss: 3.25903 \tReconstruction loss: 3.08463 \tLatent loss: 0.174394\n",
      "1378 Train total loss: 3.23085 \tReconstruction loss: 3.06169 \tLatent loss: 0.169164\n",
      "1379 Train total loss: 3.25509 \tReconstruction loss: 3.09088 \tLatent loss: 0.164206\n",
      "1380 Train total loss: 3.37112 \tReconstruction loss: 3.21069 \tLatent loss: 0.160432\n",
      "1381 Train total loss: 3.2433 \tReconstruction loss: 3.08833 \tLatent loss: 0.154969\n",
      "1382 Train total loss: 3.24712 \tReconstruction loss: 3.09637 \tLatent loss: 0.150751\n",
      "1383 Train total loss: 3.26955 \tReconstruction loss: 3.12293 \tLatent loss: 0.146622\n",
      "1384 Train total loss: 3.37054 \tReconstruction loss: 3.22755 \tLatent loss: 0.142987\n",
      "1385 Train total loss: 3.23756 \tReconstruction loss: 3.09815 \tLatent loss: 0.139412\n",
      "1386 Train total loss: 3.18842 \tReconstruction loss: 3.05245 \tLatent loss: 0.135974\n",
      "1387 Train total loss: 3.27789 \tReconstruction loss: 3.14086 \tLatent loss: 0.137027\n",
      "1388 Train total loss: 3.23151 \tReconstruction loss: 3.09647 \tLatent loss: 0.135041\n",
      "1389 Train total loss: 3.23044 \tReconstruction loss: 3.09829 \tLatent loss: 0.132155\n",
      "1390 Train total loss: 3.29187 \tReconstruction loss: 3.16286 \tLatent loss: 0.129016\n",
      "1391 Train total loss: 3.247 \tReconstruction loss: 3.11953 \tLatent loss: 0.12747\n",
      "1392 Train total loss: 3.24832 \tReconstruction loss: 3.12479 \tLatent loss: 0.123537\n",
      "1393 Train total loss: 3.319 \tReconstruction loss: 3.19755 \tLatent loss: 0.121447\n",
      "1394 Train total loss: 3.35254 \tReconstruction loss: 3.23155 \tLatent loss: 0.12099\n",
      "1395 Train total loss: 3.1747 \tReconstruction loss: 3.05574 \tLatent loss: 0.118957\n",
      "1396 Train total loss: 3.16681 \tReconstruction loss: 3.04956 \tLatent loss: 0.117249\n",
      "1397 Train total loss: 3.17473 \tReconstruction loss: 3.05797 \tLatent loss: 0.116755\n",
      "1398 Train total loss: 3.15089 \tReconstruction loss: 3.03558 \tLatent loss: 0.115313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1399 Train total loss: 3.36276 \tReconstruction loss: 3.24925 \tLatent loss: 0.113515\n",
      "1400 Train total loss: 3.30437 \tReconstruction loss: 3.18967 \tLatent loss: 0.114694\n",
      "1401 Train total loss: 3.1924 \tReconstruction loss: 3.06355 \tLatent loss: 0.128852\n",
      "1402 Train total loss: 3.19563 \tReconstruction loss: 3.07095 \tLatent loss: 0.124678\n",
      "1403 Train total loss: 3.21839 \tReconstruction loss: 3.09705 \tLatent loss: 0.121344\n",
      "1404 Train total loss: 3.32893 \tReconstruction loss: 3.21006 \tLatent loss: 0.118869\n",
      "1405 Train total loss: 3.19048 \tReconstruction loss: 3.07378 \tLatent loss: 0.116706\n",
      "1406 Train total loss: 3.14094 \tReconstruction loss: 3.02658 \tLatent loss: 0.114363\n",
      "1407 Train total loss: 3.24846 \tReconstruction loss: 3.13604 \tLatent loss: 0.112412\n",
      "1408 Train total loss: 3.18912 \tReconstruction loss: 3.07825 \tLatent loss: 0.110867\n",
      "1409 Train total loss: 3.18807 \tReconstruction loss: 3.07706 \tLatent loss: 0.111004\n",
      "1410 Train total loss: 3.25134 \tReconstruction loss: 3.14215 \tLatent loss: 0.109192\n",
      "1411 Train total loss: 3.20958 \tReconstruction loss: 3.10004 \tLatent loss: 0.10954\n",
      "1412 Train total loss: 3.21503 \tReconstruction loss: 3.10289 \tLatent loss: 0.112141\n",
      "1413 Train total loss: 3.28724 \tReconstruction loss: 3.17687 \tLatent loss: 0.110368\n",
      "1414 Train total loss: 3.32683 \tReconstruction loss: 3.21776 \tLatent loss: 0.10907\n",
      "1415 Train total loss: 3.1421 \tReconstruction loss: 3.03338 \tLatent loss: 0.108724\n",
      "1416 Train total loss: 3.12779 \tReconstruction loss: 3.0205 \tLatent loss: 0.107285\n",
      "1417 Train total loss: 3.1398 \tReconstruction loss: 3.03411 \tLatent loss: 0.105681\n",
      "1418 Train total loss: 3.11568 \tReconstruction loss: 3.01099 \tLatent loss: 0.104694\n",
      "1419 Train total loss: 3.14435 \tReconstruction loss: 3.03686 \tLatent loss: 0.107494\n",
      "1420 Train total loss: 3.28079 \tReconstruction loss: 3.17441 \tLatent loss: 0.106374\n",
      "1421 Train total loss: 3.13647 \tReconstruction loss: 3.03295 \tLatent loss: 0.103522\n",
      "1422 Train total loss: 3.17224 \tReconstruction loss: 3.06883 \tLatent loss: 0.103415\n",
      "1423 Train total loss: 3.17979 \tReconstruction loss: 3.0771 \tLatent loss: 0.102686\n",
      "1424 Train total loss: 3.29536 \tReconstruction loss: 3.1933 \tLatent loss: 0.102064\n",
      "1425 Train total loss: 3.15472 \tReconstruction loss: 3.05392 \tLatent loss: 0.1008\n",
      "1426 Train total loss: 3.11119 \tReconstruction loss: 3.00645 \tLatent loss: 0.10474\n",
      "1427 Train total loss: 3.20524 \tReconstruction loss: 3.09911 \tLatent loss: 0.106124\n",
      "1428 Train total loss: 3.14956 \tReconstruction loss: 3.04397 \tLatent loss: 0.105586\n",
      "1429 Train total loss: 3.16549 \tReconstruction loss: 3.06127 \tLatent loss: 0.104222\n",
      "1430 Train total loss: 3.22885 \tReconstruction loss: 3.12695 \tLatent loss: 0.101903\n",
      "1431 Train total loss: 3.1838 \tReconstruction loss: 3.08217 \tLatent loss: 0.101626\n",
      "1432 Train total loss: 3.17224 \tReconstruction loss: 3.07264 \tLatent loss: 0.0995972\n",
      "1433 Train total loss: 3.24964 \tReconstruction loss: 3.14787 \tLatent loss: 0.101763\n",
      "1434 Train total loss: 3.28302 \tReconstruction loss: 3.18085 \tLatent loss: 0.102166\n",
      "1435 Train total loss: 3.10202 \tReconstruction loss: 3.00187 \tLatent loss: 0.100145\n",
      "1436 Train total loss: 3.09097 \tReconstruction loss: 2.99041 \tLatent loss: 0.100552\n",
      "1437 Train total loss: 3.22446 \tReconstruction loss: 3.00876 \tLatent loss: 0.215702\n",
      "1438 Train total loss: 3.24984 \tReconstruction loss: 2.98373 \tLatent loss: 0.266115\n",
      "1439 Train total loss: 3.24109 \tReconstruction loss: 3.00388 \tLatent loss: 0.237208\n",
      "1440 Train total loss: 3.3529 \tReconstruction loss: 3.13942 \tLatent loss: 0.213484\n",
      "1441 Train total loss: 3.19731 \tReconstruction loss: 3.00092 \tLatent loss: 0.196392\n",
      "1442 Train total loss: 3.19978 \tReconstruction loss: 3.01482 \tLatent loss: 0.184958\n",
      "1443 Train total loss: 3.22116 \tReconstruction loss: 3.04529 \tLatent loss: 0.175869\n",
      "1444 Train total loss: 3.35352 \tReconstruction loss: 3.1848 \tLatent loss: 0.16872\n",
      "1445 Train total loss: 3.19597 \tReconstruction loss: 3.0337 \tLatent loss: 0.162272\n",
      "1446 Train total loss: 3.13062 \tReconstruction loss: 2.97416 \tLatent loss: 0.156465\n",
      "1447 Train total loss: 3.22545 \tReconstruction loss: 3.0743 \tLatent loss: 0.151151\n",
      "1448 Train total loss: 3.16206 \tReconstruction loss: 3.01583 \tLatent loss: 0.146226\n",
      "1449 Train total loss: 3.16429 \tReconstruction loss: 3.02269 \tLatent loss: 0.141599\n",
      "1450 Train total loss: 3.24381 \tReconstruction loss: 3.10653 \tLatent loss: 0.137281\n",
      "1451 Train total loss: 3.19784 \tReconstruction loss: 3.06325 \tLatent loss: 0.134585\n",
      "1452 Train total loss: 3.18159 \tReconstruction loss: 3.05228 \tLatent loss: 0.129306\n",
      "1453 Train total loss: 3.25788 \tReconstruction loss: 3.13214 \tLatent loss: 0.125746\n",
      "1454 Train total loss: 3.29333 \tReconstruction loss: 3.17091 \tLatent loss: 0.122418\n",
      "1455 Train total loss: 3.09284 \tReconstruction loss: 2.97361 \tLatent loss: 0.119233\n",
      "1456 Train total loss: 3.08856 \tReconstruction loss: 2.97241 \tLatent loss: 0.116146\n",
      "1457 Train total loss: 3.09051 \tReconstruction loss: 2.97712 \tLatent loss: 0.113396\n",
      "1458 Train total loss: 3.06726 \tReconstruction loss: 2.95542 \tLatent loss: 0.111832\n",
      "1459 Train total loss: 3.09585 \tReconstruction loss: 2.98641 \tLatent loss: 0.109442\n",
      "1460 Train total loss: 3.24287 \tReconstruction loss: 3.135 \tLatent loss: 0.107868\n",
      "1461 Train total loss: 3.09078 \tReconstruction loss: 2.9857 \tLatent loss: 0.105081\n",
      "1462 Train total loss: 3.10163 \tReconstruction loss: 2.99777 \tLatent loss: 0.10386\n",
      "1463 Train total loss: 3.1364 \tReconstruction loss: 3.03363 \tLatent loss: 0.10277\n",
      "1464 Train total loss: 3.30195 \tReconstruction loss: 3.15814 \tLatent loss: 0.143812\n",
      "1465 Train total loss: 3.21026 \tReconstruction loss: 3.01841 \tLatent loss: 0.191851\n",
      "1466 Train total loss: 3.13784 \tReconstruction loss: 2.95929 \tLatent loss: 0.178545\n",
      "1467 Train total loss: 3.22277 \tReconstruction loss: 3.05663 \tLatent loss: 0.166136\n",
      "1468 Train total loss: 3.15471 \tReconstruction loss: 2.99675 \tLatent loss: 0.15796\n",
      "1469 Train total loss: 3.15658 \tReconstruction loss: 3.0046 \tLatent loss: 0.151978\n",
      "1470 Train total loss: 3.23761 \tReconstruction loss: 3.09073 \tLatent loss: 0.146878\n",
      "1471 Train total loss: 3.24834 \tReconstruction loss: 3.10686 \tLatent loss: 0.14148\n",
      "1472 Train total loss: 3.16791 \tReconstruction loss: 3.04442 \tLatent loss: 0.123493\n",
      "1473 Train total loss: 3.26041 \tReconstruction loss: 3.12069 \tLatent loss: 0.13972\n",
      "1474 Train total loss: 3.26204 \tReconstruction loss: 3.1535 \tLatent loss: 0.108541\n",
      "1475 Train total loss: 3.12024 \tReconstruction loss: 2.96101 \tLatent loss: 0.159234\n",
      "1476 Train total loss: 3.08373 \tReconstruction loss: 2.94793 \tLatent loss: 0.135798\n",
      "1477 Train total loss: 3.08245 \tReconstruction loss: 2.96198 \tLatent loss: 0.120475\n",
      "1478 Train total loss: 3.05045 \tReconstruction loss: 2.93542 \tLatent loss: 0.115035\n",
      "1479 Train total loss: 3.08119 \tReconstruction loss: 2.96949 \tLatent loss: 0.1117\n",
      "1480 Train total loss: 3.22424 \tReconstruction loss: 3.11453 \tLatent loss: 0.109709\n",
      "1481 Train total loss: 3.0719 \tReconstruction loss: 2.96473 \tLatent loss: 0.107168\n",
      "1482 Train total loss: 3.08969 \tReconstruction loss: 2.98469 \tLatent loss: 0.105002\n",
      "1483 Train total loss: 3.11463 \tReconstruction loss: 3.01114 \tLatent loss: 0.10349\n",
      "1484 Train total loss: 3.25811 \tReconstruction loss: 3.15578 \tLatent loss: 0.102336\n",
      "1485 Train total loss: 3.09903 \tReconstruction loss: 2.99798 \tLatent loss: 0.101055\n",
      "1486 Train total loss: 3.03902 \tReconstruction loss: 2.93912 \tLatent loss: 0.0998997\n",
      "1487 Train total loss: 3.14625 \tReconstruction loss: 3.04787 \tLatent loss: 0.0983821\n",
      "1488 Train total loss: 3.08109 \tReconstruction loss: 2.98365 \tLatent loss: 0.0974399\n",
      "1489 Train total loss: 3.08692 \tReconstruction loss: 2.99071 \tLatent loss: 0.0962161\n",
      "1490 Train total loss: 3.17184 \tReconstruction loss: 3.07652 \tLatent loss: 0.0953249\n",
      "1491 Train total loss: 3.12872 \tReconstruction loss: 3.03196 \tLatent loss: 0.0967623\n",
      "1492 Train total loss: 3.15065 \tReconstruction loss: 3.05396 \tLatent loss: 0.0966863\n",
      "1493 Train total loss: 3.19537 \tReconstruction loss: 3.09861 \tLatent loss: 0.0967562\n",
      "1494 Train total loss: 3.23617 \tReconstruction loss: 3.1409 \tLatent loss: 0.0952756\n",
      "1495 Train total loss: 3.03342 \tReconstruction loss: 2.93805 \tLatent loss: 0.0953745\n",
      "1496 Train total loss: 3.0313 \tReconstruction loss: 2.93662 \tLatent loss: 0.094682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497 Train total loss: 3.04125 \tReconstruction loss: 2.9479 \tLatent loss: 0.0933558\n",
      "1498 Train total loss: 3.01085 \tReconstruction loss: 2.91853 \tLatent loss: 0.0923248\n",
      "1499 Train total loss: 3.03888 \tReconstruction loss: 2.94628 \tLatent loss: 0.0926056\n",
      "1500 Train total loss: 3.21861 \tReconstruction loss: 3.12558 \tLatent loss: 0.0930237\n",
      "1501 Train total loss: 3.04402 \tReconstruction loss: 2.94777 \tLatent loss: 0.0962496\n",
      "1502 Train total loss: 3.05603 \tReconstruction loss: 2.95951 \tLatent loss: 0.0965259\n",
      "1503 Train total loss: 3.09514 \tReconstruction loss: 3.00111 \tLatent loss: 0.0940305\n",
      "1504 Train total loss: 3.2319 \tReconstruction loss: 3.13837 \tLatent loss: 0.0935358\n",
      "1505 Train total loss: 3.07639 \tReconstruction loss: 2.9831 \tLatent loss: 0.0932929\n",
      "1506 Train total loss: 3.01834 \tReconstruction loss: 2.92567 \tLatent loss: 0.0926772\n",
      "1507 Train total loss: 3.12965 \tReconstruction loss: 3.02584 \tLatent loss: 0.103809\n",
      "1508 Train total loss: 3.07809 \tReconstruction loss: 2.9667 \tLatent loss: 0.111394\n",
      "1509 Train total loss: 3.07755 \tReconstruction loss: 2.97243 \tLatent loss: 0.105118\n",
      "1510 Train total loss: 3.17081 \tReconstruction loss: 3.06991 \tLatent loss: 0.100904\n",
      "1511 Train total loss: 3.11898 \tReconstruction loss: 3.02097 \tLatent loss: 0.0980156\n",
      "1512 Train total loss: 3.10494 \tReconstruction loss: 3.00915 \tLatent loss: 0.0957839\n",
      "1513 Train total loss: 3.18139 \tReconstruction loss: 3.08775 \tLatent loss: 0.0936367\n",
      "1514 Train total loss: 3.21165 \tReconstruction loss: 3.11908 \tLatent loss: 0.0925703\n",
      "1515 Train total loss: 3.01078 \tReconstruction loss: 2.91925 \tLatent loss: 0.0915309\n",
      "1516 Train total loss: 2.99698 \tReconstruction loss: 2.90612 \tLatent loss: 0.0908593\n",
      "1517 Train total loss: 3.01198 \tReconstruction loss: 2.92259 \tLatent loss: 0.0893908\n",
      "1518 Train total loss: 2.98778 \tReconstruction loss: 2.89843 \tLatent loss: 0.0893534\n",
      "1519 Train total loss: 3.0171 \tReconstruction loss: 2.92881 \tLatent loss: 0.088288\n",
      "1520 Train total loss: 3.18666 \tReconstruction loss: 3.09861 \tLatent loss: 0.0880515\n",
      "1521 Train total loss: 3.01749 \tReconstruction loss: 2.92938 \tLatent loss: 0.0881148\n",
      "1522 Train total loss: 3.03226 \tReconstruction loss: 2.94446 \tLatent loss: 0.0878021\n",
      "1523 Train total loss: 3.0636 \tReconstruction loss: 2.97739 \tLatent loss: 0.0862075\n",
      "1524 Train total loss: 3.21378 \tReconstruction loss: 3.1292 \tLatent loss: 0.0845835\n",
      "1525 Train total loss: 3.0664 \tReconstruction loss: 2.97544 \tLatent loss: 0.0909556\n",
      "1526 Train total loss: 2.99801 \tReconstruction loss: 2.90914 \tLatent loss: 0.0888785\n",
      "1527 Train total loss: 3.10393 \tReconstruction loss: 3.01456 \tLatent loss: 0.0893673\n",
      "1528 Train total loss: 3.04162 \tReconstruction loss: 2.95105 \tLatent loss: 0.0905755\n",
      "1529 Train total loss: 3.04827 \tReconstruction loss: 2.95898 \tLatent loss: 0.0892887\n",
      "1530 Train total loss: 3.13933 \tReconstruction loss: 3.05108 \tLatent loss: 0.0882449\n",
      "1531 Train total loss: 3.09693 \tReconstruction loss: 3.00846 \tLatent loss: 0.0884641\n",
      "1532 Train total loss: 3.12154 \tReconstruction loss: 3.03465 \tLatent loss: 0.0868818\n",
      "1533 Train total loss: 3.15963 \tReconstruction loss: 3.07388 \tLatent loss: 0.0857478\n",
      "1534 Train total loss: 3.19687 \tReconstruction loss: 3.11219 \tLatent loss: 0.0846744\n",
      "1535 Train total loss: 2.99123 \tReconstruction loss: 2.90714 \tLatent loss: 0.084084\n",
      "1536 Train total loss: 2.9739 \tReconstruction loss: 2.89006 \tLatent loss: 0.0838428\n",
      "1537 Train total loss: 2.98916 \tReconstruction loss: 2.90672 \tLatent loss: 0.0824409\n",
      "1538 Train total loss: 2.9642 \tReconstruction loss: 2.88124 \tLatent loss: 0.082953\n",
      "1539 Train total loss: 2.99743 \tReconstruction loss: 2.91074 \tLatent loss: 0.0866919\n",
      "1540 Train total loss: 3.16557 \tReconstruction loss: 3.08098 \tLatent loss: 0.0845876\n",
      "1541 Train total loss: 2.99096 \tReconstruction loss: 2.9087 \tLatent loss: 0.0822555\n",
      "1542 Train total loss: 3.01204 \tReconstruction loss: 2.93096 \tLatent loss: 0.0810848\n",
      "1543 Train total loss: 3.04299 \tReconstruction loss: 2.96365 \tLatent loss: 0.0793424\n",
      "1544 Train total loss: 3.2054 \tReconstruction loss: 3.11778 \tLatent loss: 0.0876247\n",
      "1545 Train total loss: 3.04543 \tReconstruction loss: 2.96099 \tLatent loss: 0.0844442\n",
      "1546 Train total loss: 2.97702 \tReconstruction loss: 2.89496 \tLatent loss: 0.0820632\n",
      "1547 Train total loss: 3.09013 \tReconstruction loss: 3.01016 \tLatent loss: 0.0799707\n",
      "1548 Train total loss: 3.01821 \tReconstruction loss: 2.93998 \tLatent loss: 0.0782319\n",
      "1549 Train total loss: 3.02969 \tReconstruction loss: 2.95193 \tLatent loss: 0.0777655\n",
      "1550 Train total loss: 3.12344 \tReconstruction loss: 3.04598 \tLatent loss: 0.0774615\n",
      "1551 Train total loss: 3.07829 \tReconstruction loss: 3.00017 \tLatent loss: 0.0781147\n",
      "1552 Train total loss: 3.06716 \tReconstruction loss: 2.98662 \tLatent loss: 0.0805437\n",
      "1553 Train total loss: 3.13936 \tReconstruction loss: 3.0602 \tLatent loss: 0.0791566\n",
      "1554 Train total loss: 3.17488 \tReconstruction loss: 3.09679 \tLatent loss: 0.0780835\n",
      "1555 Train total loss: 2.96788 \tReconstruction loss: 2.88905 \tLatent loss: 0.0788272\n",
      "1556 Train total loss: 2.95404 \tReconstruction loss: 2.87395 \tLatent loss: 0.0800891\n",
      "1557 Train total loss: 2.97441 \tReconstruction loss: 2.89295 \tLatent loss: 0.0814638\n",
      "1558 Train total loss: 2.9469 \tReconstruction loss: 2.86706 \tLatent loss: 0.0798443\n",
      "1559 Train total loss: 2.99504 \tReconstruction loss: 2.91698 \tLatent loss: 0.0780668\n",
      "1560 Train total loss: 3.15842 \tReconstruction loss: 3.08214 \tLatent loss: 0.0762798\n",
      "1561 Train total loss: 2.9727 \tReconstruction loss: 2.89526 \tLatent loss: 0.0774412\n",
      "1562 Train total loss: 2.99298 \tReconstruction loss: 2.91703 \tLatent loss: 0.0759524\n",
      "1563 Train total loss: 3.03816 \tReconstruction loss: 2.96381 \tLatent loss: 0.074352\n",
      "1564 Train total loss: 3.18483 \tReconstruction loss: 3.1108 \tLatent loss: 0.0740306\n",
      "1565 Train total loss: 3.02366 \tReconstruction loss: 2.95016 \tLatent loss: 0.0735028\n",
      "1566 Train total loss: 2.96233 \tReconstruction loss: 2.88961 \tLatent loss: 0.0727196\n",
      "1567 Train total loss: 3.07281 \tReconstruction loss: 2.99323 \tLatent loss: 0.0795807\n",
      "1568 Train total loss: 3.0062 \tReconstruction loss: 2.92387 \tLatent loss: 0.0823313\n",
      "1569 Train total loss: 3.01222 \tReconstruction loss: 2.9343 \tLatent loss: 0.0779238\n",
      "1570 Train total loss: 3.11155 \tReconstruction loss: 3.03565 \tLatent loss: 0.075899\n",
      "1571 Train total loss: 3.06602 \tReconstruction loss: 2.98912 \tLatent loss: 0.0769008\n",
      "1572 Train total loss: 3.05073 \tReconstruction loss: 2.97551 \tLatent loss: 0.0752204\n",
      "1573 Train total loss: 3.12537 \tReconstruction loss: 3.05195 \tLatent loss: 0.0734194\n",
      "1574 Train total loss: 3.15826 \tReconstruction loss: 3.08252 \tLatent loss: 0.0757381\n",
      "1575 Train total loss: 2.94726 \tReconstruction loss: 2.87343 \tLatent loss: 0.0738294\n",
      "1576 Train total loss: 2.93463 \tReconstruction loss: 2.86321 \tLatent loss: 0.0714201\n",
      "1577 Train total loss: 2.9456 \tReconstruction loss: 2.87463 \tLatent loss: 0.0709688\n",
      "1578 Train total loss: 2.92462 \tReconstruction loss: 2.85456 \tLatent loss: 0.0700548\n",
      "1579 Train total loss: 3.00272 \tReconstruction loss: 2.92778 \tLatent loss: 0.0749351\n",
      "1580 Train total loss: 3.13952 \tReconstruction loss: 3.06711 \tLatent loss: 0.0724105\n",
      "1581 Train total loss: 2.95475 \tReconstruction loss: 2.88476 \tLatent loss: 0.0699914\n",
      "1582 Train total loss: 2.97273 \tReconstruction loss: 2.90297 \tLatent loss: 0.0697564\n",
      "1583 Train total loss: 3.01084 \tReconstruction loss: 2.94105 \tLatent loss: 0.0697879\n",
      "1584 Train total loss: 3.23996 \tReconstruction loss: 3.17138 \tLatent loss: 0.0685761\n",
      "1585 Train total loss: 3.02247 \tReconstruction loss: 2.95543 \tLatent loss: 0.0670458\n",
      "1586 Train total loss: 2.94258 \tReconstruction loss: 2.87451 \tLatent loss: 0.068062\n",
      "1587 Train total loss: 3.05269 \tReconstruction loss: 2.98526 \tLatent loss: 0.0674377\n",
      "1588 Train total loss: 2.98679 \tReconstruction loss: 2.91911 \tLatent loss: 0.0676717\n",
      "1589 Train total loss: 2.99622 \tReconstruction loss: 2.92588 \tLatent loss: 0.0703404\n",
      "1590 Train total loss: 3.0918 \tReconstruction loss: 3.02348 \tLatent loss: 0.0683111\n",
      "1591 Train total loss: 3.05206 \tReconstruction loss: 2.98413 \tLatent loss: 0.0679287\n",
      "1592 Train total loss: 3.03517 \tReconstruction loss: 2.96846 \tLatent loss: 0.0667046\n",
      "1593 Train total loss: 3.10251 \tReconstruction loss: 3.03639 \tLatent loss: 0.066119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1594 Train total loss: 3.1414 \tReconstruction loss: 3.07359 \tLatent loss: 0.0678165\n",
      "1595 Train total loss: 2.94 \tReconstruction loss: 2.86888 \tLatent loss: 0.0711273\n",
      "1596 Train total loss: 2.91852 \tReconstruction loss: 2.84736 \tLatent loss: 0.0711667\n",
      "1597 Train total loss: 2.93137 \tReconstruction loss: 2.8632 \tLatent loss: 0.0681696\n",
      "1598 Train total loss: 2.90763 \tReconstruction loss: 2.84161 \tLatent loss: 0.0660188\n",
      "1599 Train total loss: 2.93371 \tReconstruction loss: 2.8669 \tLatent loss: 0.0668046\n",
      "1600 Train total loss: 3.12291 \tReconstruction loss: 3.05286 \tLatent loss: 0.0700508\n",
      "1601 Train total loss: 2.93414 \tReconstruction loss: 2.86702 \tLatent loss: 0.0671245\n",
      "1602 Train total loss: 2.95655 \tReconstruction loss: 2.89174 \tLatent loss: 0.0648064\n",
      "1603 Train total loss: 2.98481 \tReconstruction loss: 2.92076 \tLatent loss: 0.0640489\n",
      "1604 Train total loss: 3.16714 \tReconstruction loss: 3.10453 \tLatent loss: 0.0626104\n",
      "1605 Train total loss: 2.99575 \tReconstruction loss: 2.92979 \tLatent loss: 0.0659617\n",
      "1606 Train total loss: 2.92699 \tReconstruction loss: 2.86202 \tLatent loss: 0.0649714\n",
      "1607 Train total loss: 3.04254 \tReconstruction loss: 2.97717 \tLatent loss: 0.0653676\n",
      "1608 Train total loss: 2.97568 \tReconstruction loss: 2.90833 \tLatent loss: 0.0673461\n",
      "1609 Train total loss: 2.97771 \tReconstruction loss: 2.91199 \tLatent loss: 0.0657225\n",
      "1610 Train total loss: 3.08159 \tReconstruction loss: 3.01746 \tLatent loss: 0.0641375\n",
      "1611 Train total loss: 3.04053 \tReconstruction loss: 2.97628 \tLatent loss: 0.0642465\n",
      "1612 Train total loss: 3.02898 \tReconstruction loss: 2.96395 \tLatent loss: 0.0650291\n",
      "1613 Train total loss: 3.08802 \tReconstruction loss: 3.02391 \tLatent loss: 0.064116\n",
      "1614 Train total loss: 3.16408 \tReconstruction loss: 3.10069 \tLatent loss: 0.0633864\n",
      "1615 Train total loss: 2.91126 \tReconstruction loss: 2.84922 \tLatent loss: 0.0620365\n",
      "1616 Train total loss: 2.90359 \tReconstruction loss: 2.84324 \tLatent loss: 0.0603552\n",
      "1617 Train total loss: 2.9097 \tReconstruction loss: 2.85006 \tLatent loss: 0.0596432\n",
      "1618 Train total loss: 2.88801 \tReconstruction loss: 2.8279 \tLatent loss: 0.0601101\n",
      "1619 Train total loss: 2.91753 \tReconstruction loss: 2.85672 \tLatent loss: 0.0608095\n",
      "1620 Train total loss: 3.11541 \tReconstruction loss: 3.05494 \tLatent loss: 0.0604638\n",
      "1621 Train total loss: 2.92219 \tReconstruction loss: 2.86044 \tLatent loss: 0.0617521\n",
      "1622 Train total loss: 2.94649 \tReconstruction loss: 2.88219 \tLatent loss: 0.064294\n",
      "1623 Train total loss: 2.97624 \tReconstruction loss: 2.91179 \tLatent loss: 0.0644459\n",
      "1624 Train total loss: 3.1621 \tReconstruction loss: 3.09898 \tLatent loss: 0.0631183\n",
      "1625 Train total loss: 2.98196 \tReconstruction loss: 2.92068 \tLatent loss: 0.0612863\n",
      "1626 Train total loss: 2.91073 \tReconstruction loss: 2.84891 \tLatent loss: 0.0618195\n",
      "1627 Train total loss: 3.02567 \tReconstruction loss: 2.96575 \tLatent loss: 0.0599283\n",
      "1628 Train total loss: 2.95267 \tReconstruction loss: 2.894 \tLatent loss: 0.0586674\n",
      "1629 Train total loss: 2.96036 \tReconstruction loss: 2.90166 \tLatent loss: 0.0586995\n",
      "1630 Train total loss: 3.06839 \tReconstruction loss: 3.0089 \tLatent loss: 0.059487\n",
      "1631 Train total loss: 3.02634 \tReconstruction loss: 2.96811 \tLatent loss: 0.0582333\n",
      "1632 Train total loss: 3.01309 \tReconstruction loss: 2.95616 \tLatent loss: 0.056926\n",
      "1633 Train total loss: 3.07536 \tReconstruction loss: 3.01699 \tLatent loss: 0.0583682\n",
      "1634 Train total loss: 3.11711 \tReconstruction loss: 3.05924 \tLatent loss: 0.0578692\n",
      "1635 Train total loss: 2.91009 \tReconstruction loss: 2.83796 \tLatent loss: 0.0721292\n",
      "1636 Train total loss: 2.8863 \tReconstruction loss: 2.82175 \tLatent loss: 0.0645476\n",
      "1637 Train total loss: 2.90003 \tReconstruction loss: 2.83861 \tLatent loss: 0.0614192\n",
      "1638 Train total loss: 2.8724 \tReconstruction loss: 2.81408 \tLatent loss: 0.058317\n",
      "1639 Train total loss: 2.90072 \tReconstruction loss: 2.84444 \tLatent loss: 0.0562781\n",
      "1640 Train total loss: 3.11072 \tReconstruction loss: 3.05352 \tLatent loss: 0.0572002\n",
      "1641 Train total loss: 2.90592 \tReconstruction loss: 2.84764 \tLatent loss: 0.0582803\n",
      "1642 Train total loss: 2.92899 \tReconstruction loss: 2.87123 \tLatent loss: 0.0577626\n",
      "1643 Train total loss: 2.95768 \tReconstruction loss: 2.90075 \tLatent loss: 0.0569267\n",
      "1644 Train total loss: 3.15099 \tReconstruction loss: 3.09444 \tLatent loss: 0.0565508\n",
      "1645 Train total loss: 2.97287 \tReconstruction loss: 2.91698 \tLatent loss: 0.055886\n",
      "1646 Train total loss: 2.92068 \tReconstruction loss: 2.8424 \tLatent loss: 0.0782777\n",
      "1647 Train total loss: 3.01934 \tReconstruction loss: 2.95167 \tLatent loss: 0.0676688\n",
      "1648 Train total loss: 2.94593 \tReconstruction loss: 2.88231 \tLatent loss: 0.0636219\n",
      "1649 Train total loss: 2.94889 \tReconstruction loss: 2.88874 \tLatent loss: 0.0601543\n",
      "1650 Train total loss: 3.05965 \tReconstruction loss: 3.00257 \tLatent loss: 0.0570807\n",
      "1651 Train total loss: 3.01811 \tReconstruction loss: 2.962 \tLatent loss: 0.056105\n",
      "1652 Train total loss: 3.00027 \tReconstruction loss: 2.94597 \tLatent loss: 0.0543064\n",
      "1653 Train total loss: 3.074 \tReconstruction loss: 3.01894 \tLatent loss: 0.0550594\n",
      "1654 Train total loss: 3.12146 \tReconstruction loss: 3.03854 \tLatent loss: 0.0829224\n",
      "1655 Train total loss: 2.93204 \tReconstruction loss: 2.82902 \tLatent loss: 0.103016\n",
      "1656 Train total loss: 2.88704 \tReconstruction loss: 2.80778 \tLatent loss: 0.0792595\n",
      "1657 Train total loss: 2.89913 \tReconstruction loss: 2.82713 \tLatent loss: 0.0720021\n",
      "1658 Train total loss: 2.87271 \tReconstruction loss: 2.80521 \tLatent loss: 0.0674922\n",
      "1659 Train total loss: 2.92318 \tReconstruction loss: 2.85946 \tLatent loss: 0.063726\n",
      "1660 Train total loss: 3.11008 \tReconstruction loss: 3.04886 \tLatent loss: 0.061217\n",
      "1661 Train total loss: 2.89835 \tReconstruction loss: 2.83531 \tLatent loss: 0.0630456\n",
      "1662 Train total loss: 2.92342 \tReconstruction loss: 2.86184 \tLatent loss: 0.0615812\n",
      "1663 Train total loss: 2.95048 \tReconstruction loss: 2.89201 \tLatent loss: 0.0584766\n",
      "1664 Train total loss: 3.16344 \tReconstruction loss: 3.09112 \tLatent loss: 0.0723255\n",
      "1665 Train total loss: 2.9802 \tReconstruction loss: 2.91143 \tLatent loss: 0.0687715\n",
      "1666 Train total loss: 2.89481 \tReconstruction loss: 2.83073 \tLatent loss: 0.0640846\n",
      "1667 Train total loss: 3.00684 \tReconstruction loss: 2.94576 \tLatent loss: 0.0610762\n",
      "1668 Train total loss: 2.94365 \tReconstruction loss: 2.885 \tLatent loss: 0.0586461\n",
      "1669 Train total loss: 2.93789 \tReconstruction loss: 2.88111 \tLatent loss: 0.0567855\n",
      "1670 Train total loss: 3.05374 \tReconstruction loss: 2.99792 \tLatent loss: 0.05582\n",
      "1671 Train total loss: 3.01101 \tReconstruction loss: 2.95539 \tLatent loss: 0.0556165\n",
      "1672 Train total loss: 2.99708 \tReconstruction loss: 2.94344 \tLatent loss: 0.0536361\n",
      "1673 Train total loss: 3.05638 \tReconstruction loss: 3.00377 \tLatent loss: 0.0526088\n",
      "1674 Train total loss: 3.09561 \tReconstruction loss: 3.04294 \tLatent loss: 0.0526717\n",
      "1675 Train total loss: 2.8785 \tReconstruction loss: 2.82083 \tLatent loss: 0.057667\n",
      "1676 Train total loss: 2.85535 \tReconstruction loss: 2.80051 \tLatent loss: 0.054849\n",
      "1677 Train total loss: 2.87489 \tReconstruction loss: 2.82143 \tLatent loss: 0.0534682\n",
      "1678 Train total loss: 2.85243 \tReconstruction loss: 2.79929 \tLatent loss: 0.053145\n",
      "1679 Train total loss: 2.8784 \tReconstruction loss: 2.82676 \tLatent loss: 0.0516422\n",
      "1680 Train total loss: 3.09559 \tReconstruction loss: 3.04513 \tLatent loss: 0.0504664\n",
      "1681 Train total loss: 2.88627 \tReconstruction loss: 2.83659 \tLatent loss: 0.0496797\n",
      "1682 Train total loss: 2.91402 \tReconstruction loss: 2.85803 \tLatent loss: 0.0559867\n",
      "1683 Train total loss: 2.95632 \tReconstruction loss: 2.88672 \tLatent loss: 0.0695952\n",
      "1684 Train total loss: 3.16643 \tReconstruction loss: 3.0916 \tLatent loss: 0.0748365\n",
      "1685 Train total loss: 2.97652 \tReconstruction loss: 2.91309 \tLatent loss: 0.0634326\n",
      "1686 Train total loss: 2.88853 \tReconstruction loss: 2.82981 \tLatent loss: 0.0587134\n",
      "1687 Train total loss: 2.99638 \tReconstruction loss: 2.94092 \tLatent loss: 0.0554586\n",
      "1688 Train total loss: 2.92512 \tReconstruction loss: 2.87175 \tLatent loss: 0.0533645\n",
      "1689 Train total loss: 2.9292 \tReconstruction loss: 2.87748 \tLatent loss: 0.0517185\n",
      "1690 Train total loss: 3.04472 \tReconstruction loss: 2.99273 \tLatent loss: 0.0519834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1691 Train total loss: 3.00804 \tReconstruction loss: 2.95668 \tLatent loss: 0.0513539\n",
      "1692 Train total loss: 2.99098 \tReconstruction loss: 2.94051 \tLatent loss: 0.0504734\n",
      "1693 Train total loss: 3.04502 \tReconstruction loss: 2.99576 \tLatent loss: 0.049252\n",
      "1694 Train total loss: 3.09083 \tReconstruction loss: 3.03628 \tLatent loss: 0.0545475\n",
      "1695 Train total loss: 2.87014 \tReconstruction loss: 2.81678 \tLatent loss: 0.0533563\n",
      "1696 Train total loss: 2.84474 \tReconstruction loss: 2.79385 \tLatent loss: 0.0508993\n",
      "1697 Train total loss: 2.86551 \tReconstruction loss: 2.81656 \tLatent loss: 0.0489575\n",
      "1698 Train total loss: 2.8456 \tReconstruction loss: 2.7965 \tLatent loss: 0.0491001\n",
      "1699 Train total loss: 2.87299 \tReconstruction loss: 2.82401 \tLatent loss: 0.0489818\n",
      "1700 Train total loss: 3.10348 \tReconstruction loss: 3.03633 \tLatent loss: 0.0671498\n",
      "1701 Train total loss: 2.87889 \tReconstruction loss: 2.82285 \tLatent loss: 0.0560381\n",
      "1702 Train total loss: 2.90456 \tReconstruction loss: 2.85232 \tLatent loss: 0.0522369\n",
      "1703 Train total loss: 2.93184 \tReconstruction loss: 2.88081 \tLatent loss: 0.0510377\n",
      "1704 Train total loss: 3.14047 \tReconstruction loss: 3.09088 \tLatent loss: 0.0495896\n",
      "1705 Train total loss: 2.97879 \tReconstruction loss: 2.93045 \tLatent loss: 0.0483395\n",
      "1706 Train total loss: 2.8713 \tReconstruction loss: 2.824 \tLatent loss: 0.047295\n",
      "1707 Train total loss: 2.98671 \tReconstruction loss: 2.93943 \tLatent loss: 0.0472774\n",
      "1708 Train total loss: 3.01562 \tReconstruction loss: 2.86992 \tLatent loss: 0.145705\n",
      "1709 Train total loss: 3.0021 \tReconstruction loss: 2.87504 \tLatent loss: 0.127058\n",
      "1710 Train total loss: 3.08697 \tReconstruction loss: 2.99316 \tLatent loss: 0.0938017\n",
      "1711 Train total loss: 3.03443 \tReconstruction loss: 2.95396 \tLatent loss: 0.0804635\n",
      "1712 Train total loss: 3.01063 \tReconstruction loss: 2.93668 \tLatent loss: 0.0739548\n",
      "1713 Train total loss: 3.05752 \tReconstruction loss: 2.98796 \tLatent loss: 0.0695597\n",
      "1714 Train total loss: 3.09742 \tReconstruction loss: 3.03137 \tLatent loss: 0.0660495\n",
      "1715 Train total loss: 2.87006 \tReconstruction loss: 2.80706 \tLatent loss: 0.0629959\n",
      "1716 Train total loss: 2.84931 \tReconstruction loss: 2.78906 \tLatent loss: 0.060248\n",
      "1717 Train total loss: 2.86854 \tReconstruction loss: 2.8106 \tLatent loss: 0.0579404\n",
      "1718 Train total loss: 2.84358 \tReconstruction loss: 2.78781 \tLatent loss: 0.0557698\n",
      "1719 Train total loss: 2.87279 \tReconstruction loss: 2.81861 \tLatent loss: 0.0541717\n",
      "1720 Train total loss: 3.09885 \tReconstruction loss: 3.04603 \tLatent loss: 0.0528193\n",
      "1721 Train total loss: 2.87304 \tReconstruction loss: 2.82192 \tLatent loss: 0.0511176\n",
      "1722 Train total loss: 2.90026 \tReconstruction loss: 2.85041 \tLatent loss: 0.0498527\n",
      "1723 Train total loss: 2.93555 \tReconstruction loss: 2.88681 \tLatent loss: 0.0487393\n",
      "1724 Train total loss: 3.13827 \tReconstruction loss: 3.08969 \tLatent loss: 0.0485835\n",
      "1725 Train total loss: 2.96004 \tReconstruction loss: 2.91102 \tLatent loss: 0.0490221\n",
      "1726 Train total loss: 2.87032 \tReconstruction loss: 2.82115 \tLatent loss: 0.0491707\n",
      "1727 Train total loss: 2.99812 \tReconstruction loss: 2.94928 \tLatent loss: 0.0488423\n",
      "1728 Train total loss: 2.91435 \tReconstruction loss: 2.86604 \tLatent loss: 0.0483152\n",
      "1729 Train total loss: 2.92826 \tReconstruction loss: 2.87113 \tLatent loss: 0.0571299\n",
      "1730 Train total loss: 3.11292 \tReconstruction loss: 2.98711 \tLatent loss: 0.12581\n",
      "1731 Train total loss: 3.0499 \tReconstruction loss: 2.95015 \tLatent loss: 0.0997552\n",
      "1732 Train total loss: 3.01539 \tReconstruction loss: 2.93252 \tLatent loss: 0.0828725\n",
      "1733 Train total loss: 3.05966 \tReconstruction loss: 2.98486 \tLatent loss: 0.0748012\n",
      "1734 Train total loss: 3.097 \tReconstruction loss: 3.02677 \tLatent loss: 0.0702345\n",
      "1735 Train total loss: 2.87058 \tReconstruction loss: 2.80354 \tLatent loss: 0.0670409\n",
      "1736 Train total loss: 2.84966 \tReconstruction loss: 2.78522 \tLatent loss: 0.0644379\n",
      "1737 Train total loss: 2.86891 \tReconstruction loss: 2.80683 \tLatent loss: 0.062085\n",
      "1738 Train total loss: 2.84455 \tReconstruction loss: 2.78457 \tLatent loss: 0.0599723\n",
      "1739 Train total loss: 2.87235 \tReconstruction loss: 2.81424 \tLatent loss: 0.0581134\n",
      "1740 Train total loss: 3.09699 \tReconstruction loss: 3.04037 \tLatent loss: 0.0566197\n",
      "1741 Train total loss: 2.86967 \tReconstruction loss: 2.815 \tLatent loss: 0.0546697\n",
      "1742 Train total loss: 2.899 \tReconstruction loss: 2.84581 \tLatent loss: 0.0531894\n",
      "1743 Train total loss: 2.93552 \tReconstruction loss: 2.88333 \tLatent loss: 0.0521956\n",
      "1744 Train total loss: 3.14066 \tReconstruction loss: 3.08963 \tLatent loss: 0.0510281\n",
      "1745 Train total loss: 2.96138 \tReconstruction loss: 2.91146 \tLatent loss: 0.0499229\n",
      "1746 Train total loss: 2.86643 \tReconstruction loss: 2.81745 \tLatent loss: 0.0489803\n",
      "1747 Train total loss: 2.9976 \tReconstruction loss: 2.94969 \tLatent loss: 0.047915\n",
      "1748 Train total loss: 2.91089 \tReconstruction loss: 2.86391 \tLatent loss: 0.0469803\n",
      "1749 Train total loss: 2.91699 \tReconstruction loss: 2.87044 \tLatent loss: 0.0465428\n",
      "1750 Train total loss: 3.03407 \tReconstruction loss: 2.98589 \tLatent loss: 0.0481798\n",
      "1751 Train total loss: 2.99637 \tReconstruction loss: 2.9485 \tLatent loss: 0.0478714\n",
      "1752 Train total loss: 2.98457 \tReconstruction loss: 2.93749 \tLatent loss: 0.0470885\n",
      "1753 Train total loss: 3.03199 \tReconstruction loss: 2.98518 \tLatent loss: 0.0468056\n",
      "1754 Train total loss: 3.07837 \tReconstruction loss: 3.03173 \tLatent loss: 0.0466444\n",
      "1755 Train total loss: 2.85189 \tReconstruction loss: 2.80587 \tLatent loss: 0.046023\n",
      "1756 Train total loss: 2.8322 \tReconstruction loss: 2.78542 \tLatent loss: 0.0467881\n",
      "1757 Train total loss: 2.85521 \tReconstruction loss: 2.80926 \tLatent loss: 0.0459523\n",
      "1758 Train total loss: 2.82702 \tReconstruction loss: 2.78209 \tLatent loss: 0.0449366\n",
      "1759 Train total loss: 2.86025 \tReconstruction loss: 2.81596 \tLatent loss: 0.0442899\n",
      "1760 Train total loss: 3.08743 \tReconstruction loss: 3.04311 \tLatent loss: 0.0443176\n",
      "1761 Train total loss: 2.93683 \tReconstruction loss: 2.8198 \tLatent loss: 0.117035\n",
      "1762 Train total loss: 2.97776 \tReconstruction loss: 2.84746 \tLatent loss: 0.130302\n",
      "1763 Train total loss: 2.97095 \tReconstruction loss: 2.87194 \tLatent loss: 0.099012\n",
      "1764 Train total loss: 3.17675 \tReconstruction loss: 3.09287 \tLatent loss: 0.083882\n",
      "1765 Train total loss: 2.99316 \tReconstruction loss: 2.91671 \tLatent loss: 0.0764433\n",
      "1766 Train total loss: 2.88636 \tReconstruction loss: 2.81444 \tLatent loss: 0.0719225\n",
      "1767 Train total loss: 2.99829 \tReconstruction loss: 2.92961 \tLatent loss: 0.0686797\n",
      "1768 Train total loss: 2.92358 \tReconstruction loss: 2.85752 \tLatent loss: 0.0660617\n",
      "1769 Train total loss: 2.92704 \tReconstruction loss: 2.86322 \tLatent loss: 0.0638187\n",
      "1770 Train total loss: 3.04857 \tReconstruction loss: 2.98673 \tLatent loss: 0.0618406\n",
      "1771 Train total loss: 3.00733 \tReconstruction loss: 2.94732 \tLatent loss: 0.0600114\n",
      "1772 Train total loss: 2.98969 \tReconstruction loss: 2.93142 \tLatent loss: 0.0582643\n",
      "1773 Train total loss: 3.0358 \tReconstruction loss: 2.97908 \tLatent loss: 0.0567229\n",
      "1774 Train total loss: 3.07914 \tReconstruction loss: 3.02397 \tLatent loss: 0.0551758\n",
      "1775 Train total loss: 2.85182 \tReconstruction loss: 2.79803 \tLatent loss: 0.0537935\n",
      "1776 Train total loss: 2.83166 \tReconstruction loss: 2.77909 \tLatent loss: 0.0525754\n",
      "1777 Train total loss: 2.85411 \tReconstruction loss: 2.8024 \tLatent loss: 0.0517112\n",
      "1778 Train total loss: 2.82953 \tReconstruction loss: 2.7789 \tLatent loss: 0.0506296\n",
      "1779 Train total loss: 2.87452 \tReconstruction loss: 2.82501 \tLatent loss: 0.0495095\n",
      "1780 Train total loss: 3.09083 \tReconstruction loss: 3.04205 \tLatent loss: 0.0487821\n",
      "1781 Train total loss: 2.85916 \tReconstruction loss: 2.81127 \tLatent loss: 0.0478973\n",
      "1782 Train total loss: 2.89013 \tReconstruction loss: 2.84252 \tLatent loss: 0.0476032\n",
      "1783 Train total loss: 2.91713 \tReconstruction loss: 2.86988 \tLatent loss: 0.0472491\n",
      "1784 Train total loss: 3.1396 \tReconstruction loss: 3.09315 \tLatent loss: 0.0464561\n",
      "1785 Train total loss: 2.9599 \tReconstruction loss: 2.91432 \tLatent loss: 0.0455792\n",
      "1786 Train total loss: 2.85757 \tReconstruction loss: 2.8126 \tLatent loss: 0.0449753\n",
      "1787 Train total loss: 2.97257 \tReconstruction loss: 2.92802 \tLatent loss: 0.0445522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788 Train total loss: 2.90383 \tReconstruction loss: 2.85952 \tLatent loss: 0.0443039\n",
      "1789 Train total loss: 2.906 \tReconstruction loss: 2.8624 \tLatent loss: 0.0435926\n",
      "1790 Train total loss: 3.02896 \tReconstruction loss: 2.98578 \tLatent loss: 0.043179\n",
      "1791 Train total loss: 3.00657 \tReconstruction loss: 2.96218 \tLatent loss: 0.0443824\n",
      "1792 Train total loss: 2.98095 \tReconstruction loss: 2.93316 \tLatent loss: 0.0477911\n",
      "1793 Train total loss: 3.02734 \tReconstruction loss: 2.98063 \tLatent loss: 0.0467113\n",
      "1794 Train total loss: 3.0798 \tReconstruction loss: 3.03449 \tLatent loss: 0.0453111\n",
      "1795 Train total loss: 2.84373 \tReconstruction loss: 2.79924 \tLatent loss: 0.0444973\n",
      "1796 Train total loss: 2.82366 \tReconstruction loss: 2.77931 \tLatent loss: 0.0443418\n",
      "1797 Train total loss: 2.84733 \tReconstruction loss: 2.80384 \tLatent loss: 0.0434857\n",
      "1798 Train total loss: 2.84316 \tReconstruction loss: 2.78298 \tLatent loss: 0.0601815\n",
      "1799 Train total loss: 2.86195 \tReconstruction loss: 2.80945 \tLatent loss: 0.0524996\n",
      "1800 Train total loss: 3.08654 \tReconstruction loss: 3.03623 \tLatent loss: 0.0503068\n",
      "1801 Train total loss: 2.85684 \tReconstruction loss: 2.80802 \tLatent loss: 0.0488189\n",
      "1802 Train total loss: 2.88858 \tReconstruction loss: 2.84073 \tLatent loss: 0.0478489\n",
      "1803 Train total loss: 2.91598 \tReconstruction loss: 2.86812 \tLatent loss: 0.0478683\n",
      "1804 Train total loss: 3.13423 \tReconstruction loss: 3.0868 \tLatent loss: 0.0474271\n",
      "1805 Train total loss: 2.96057 \tReconstruction loss: 2.91456 \tLatent loss: 0.0460036\n",
      "1806 Train total loss: 2.85654 \tReconstruction loss: 2.81183 \tLatent loss: 0.0447118\n",
      "1807 Train total loss: 2.97092 \tReconstruction loss: 2.9273 \tLatent loss: 0.0436236\n",
      "1808 Train total loss: 2.9015 \tReconstruction loss: 2.85576 \tLatent loss: 0.045736\n",
      "1809 Train total loss: 2.9142 \tReconstruction loss: 2.86953 \tLatent loss: 0.0446666\n",
      "1810 Train total loss: 3.03218 \tReconstruction loss: 2.98872 \tLatent loss: 0.0434594\n",
      "1811 Train total loss: 2.98797 \tReconstruction loss: 2.94506 \tLatent loss: 0.0429096\n",
      "1812 Train total loss: 2.97075 \tReconstruction loss: 2.92883 \tLatent loss: 0.0419249\n",
      "1813 Train total loss: 3.0333 \tReconstruction loss: 2.99035 \tLatent loss: 0.0429445\n",
      "1814 Train total loss: 3.06275 \tReconstruction loss: 3.02009 \tLatent loss: 0.0426627\n",
      "1815 Train total loss: 2.85031 \tReconstruction loss: 2.79938 \tLatent loss: 0.0509266\n",
      "1816 Train total loss: 2.82773 \tReconstruction loss: 2.78134 \tLatent loss: 0.0463853\n",
      "1817 Train total loss: 2.86144 \tReconstruction loss: 2.80253 \tLatent loss: 0.0589114\n",
      "1818 Train total loss: 2.84207 \tReconstruction loss: 2.78028 \tLatent loss: 0.0617866\n",
      "1819 Train total loss: 2.85916 \tReconstruction loss: 2.8064 \tLatent loss: 0.0527579\n",
      "1820 Train total loss: 3.0884 \tReconstruction loss: 3.03874 \tLatent loss: 0.0496576\n",
      "1821 Train total loss: 2.85415 \tReconstruction loss: 2.80684 \tLatent loss: 0.047311\n",
      "1822 Train total loss: 2.8848 \tReconstruction loss: 2.83903 \tLatent loss: 0.0457744\n",
      "1823 Train total loss: 2.91206 \tReconstruction loss: 2.86746 \tLatent loss: 0.044602\n",
      "1824 Train total loss: 3.13468 \tReconstruction loss: 3.09124 \tLatent loss: 0.0434348\n",
      "1825 Train total loss: 2.95871 \tReconstruction loss: 2.91453 \tLatent loss: 0.0441856\n",
      "1826 Train total loss: 2.85817 \tReconstruction loss: 2.81299 \tLatent loss: 0.0451785\n",
      "1827 Train total loss: 2.98219 \tReconstruction loss: 2.93923 \tLatent loss: 0.0429595\n",
      "1828 Train total loss: 2.90474 \tReconstruction loss: 2.8632 \tLatent loss: 0.0415442\n",
      "1829 Train total loss: 2.98813 \tReconstruction loss: 2.8646 \tLatent loss: 0.123533\n",
      "1830 Train total loss: 3.08482 \tReconstruction loss: 2.98578 \tLatent loss: 0.0990465\n",
      "1831 Train total loss: 3.02508 \tReconstruction loss: 2.9478 \tLatent loss: 0.0772736\n",
      "1832 Train total loss: 2.99522 \tReconstruction loss: 2.92772 \tLatent loss: 0.0675021\n",
      "1833 Train total loss: 3.03811 \tReconstruction loss: 2.97547 \tLatent loss: 0.0626454\n",
      "1834 Train total loss: 3.07973 \tReconstruction loss: 3.02017 \tLatent loss: 0.0595554\n",
      "1835 Train total loss: 2.85376 \tReconstruction loss: 2.79661 \tLatent loss: 0.057148\n",
      "1836 Train total loss: 2.82954 \tReconstruction loss: 2.77447 \tLatent loss: 0.0550686\n",
      "1837 Train total loss: 2.85163 \tReconstruction loss: 2.79844 \tLatent loss: 0.0531922\n",
      "1838 Train total loss: 2.82831 \tReconstruction loss: 2.77684 \tLatent loss: 0.0514778\n",
      "1839 Train total loss: 2.85674 \tReconstruction loss: 2.80686 \tLatent loss: 0.049872\n",
      "1840 Train total loss: 3.08843 \tReconstruction loss: 3.03978 \tLatent loss: 0.0486477\n",
      "1841 Train total loss: 2.8554 \tReconstruction loss: 2.80845 \tLatent loss: 0.0469559\n",
      "1842 Train total loss: 2.88393 \tReconstruction loss: 2.83822 \tLatent loss: 0.0457092\n",
      "1843 Train total loss: 2.92182 \tReconstruction loss: 2.87733 \tLatent loss: 0.0444897\n",
      "1844 Train total loss: 3.13175 \tReconstruction loss: 3.08788 \tLatent loss: 0.0438629\n",
      "1845 Train total loss: 2.96859 \tReconstruction loss: 2.92535 \tLatent loss: 0.0432346\n",
      "1846 Train total loss: 2.8559 \tReconstruction loss: 2.81353 \tLatent loss: 0.042373\n",
      "1847 Train total loss: 2.97728 \tReconstruction loss: 2.93567 \tLatent loss: 0.0416045\n",
      "1848 Train total loss: 2.89838 \tReconstruction loss: 2.85717 \tLatent loss: 0.0412115\n",
      "1849 Train total loss: 2.90505 \tReconstruction loss: 2.86407 \tLatent loss: 0.0409805\n",
      "1850 Train total loss: 3.02579 \tReconstruction loss: 2.98488 \tLatent loss: 0.040906\n",
      "1851 Train total loss: 2.98711 \tReconstruction loss: 2.94681 \tLatent loss: 0.0403013\n",
      "1852 Train total loss: 2.96882 \tReconstruction loss: 2.92905 \tLatent loss: 0.0397743\n",
      "1853 Train total loss: 3.01702 \tReconstruction loss: 2.9768 \tLatent loss: 0.0402278\n",
      "1854 Train total loss: 3.06399 \tReconstruction loss: 3.0237 \tLatent loss: 0.0402865\n",
      "1855 Train total loss: 2.84462 \tReconstruction loss: 2.80528 \tLatent loss: 0.0393381\n",
      "1856 Train total loss: 2.82056 \tReconstruction loss: 2.78176 \tLatent loss: 0.0387996\n",
      "1857 Train total loss: 2.86234 \tReconstruction loss: 2.80294 \tLatent loss: 0.0594025\n",
      "1858 Train total loss: 2.83541 \tReconstruction loss: 2.77987 \tLatent loss: 0.0555322\n",
      "1859 Train total loss: 2.85646 \tReconstruction loss: 2.80727 \tLatent loss: 0.0491858\n",
      "1860 Train total loss: 3.08327 \tReconstruction loss: 3.03662 \tLatent loss: 0.0466506\n",
      "1861 Train total loss: 2.8535 \tReconstruction loss: 2.80869 \tLatent loss: 0.0448008\n",
      "1862 Train total loss: 2.88322 \tReconstruction loss: 2.83943 \tLatent loss: 0.0437812\n",
      "1863 Train total loss: 2.91516 \tReconstruction loss: 2.87035 \tLatent loss: 0.044809\n",
      "1864 Train total loss: 3.1341 \tReconstruction loss: 3.09112 \tLatent loss: 0.0429824\n",
      "1865 Train total loss: 2.95339 \tReconstruction loss: 2.9118 \tLatent loss: 0.041586\n",
      "1866 Train total loss: 2.85165 \tReconstruction loss: 2.81116 \tLatent loss: 0.0404923\n",
      "1867 Train total loss: 2.96487 \tReconstruction loss: 2.92523 \tLatent loss: 0.0396321\n",
      "1868 Train total loss: 2.89623 \tReconstruction loss: 2.85691 \tLatent loss: 0.0393166\n",
      "1869 Train total loss: 2.90326 \tReconstruction loss: 2.86404 \tLatent loss: 0.0392231\n",
      "1870 Train total loss: 3.02396 \tReconstruction loss: 2.98459 \tLatent loss: 0.03937\n",
      "1871 Train total loss: 2.98489 \tReconstruction loss: 2.94651 \tLatent loss: 0.0383763\n",
      "1872 Train total loss: 2.96715 \tReconstruction loss: 2.92914 \tLatent loss: 0.0380116\n",
      "1873 Train total loss: 3.02665 \tReconstruction loss: 2.98819 \tLatent loss: 0.0384602\n",
      "1874 Train total loss: 3.05839 \tReconstruction loss: 3.01909 \tLatent loss: 0.0392917\n",
      "1875 Train total loss: 2.83885 \tReconstruction loss: 2.80031 \tLatent loss: 0.0385474\n",
      "1876 Train total loss: 2.81627 \tReconstruction loss: 2.77864 \tLatent loss: 0.0376348\n",
      "1877 Train total loss: 2.84297 \tReconstruction loss: 2.80125 \tLatent loss: 0.0417188\n",
      "1878 Train total loss: 2.81825 \tReconstruction loss: 2.77811 \tLatent loss: 0.0401398\n",
      "1879 Train total loss: 2.84677 \tReconstruction loss: 2.80719 \tLatent loss: 0.0395869\n",
      "1880 Train total loss: 3.07723 \tReconstruction loss: 3.03905 \tLatent loss: 0.0381847\n",
      "1881 Train total loss: 2.84597 \tReconstruction loss: 2.80784 \tLatent loss: 0.0381347\n",
      "1882 Train total loss: 2.87738 \tReconstruction loss: 2.83981 \tLatent loss: 0.0375681\n",
      "1883 Train total loss: 2.90545 \tReconstruction loss: 2.86812 \tLatent loss: 0.0373356\n",
      "1884 Train total loss: 3.12648 \tReconstruction loss: 3.0901 \tLatent loss: 0.0363853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1885 Train total loss: 3.02925 \tReconstruction loss: 2.99226 \tLatent loss: 0.0369873\n",
      "1886 Train total loss: 2.85143 \tReconstruction loss: 2.81352 \tLatent loss: 0.0379077\n",
      "1887 Train total loss: 2.96764 \tReconstruction loss: 2.92897 \tLatent loss: 0.0386667\n",
      "1888 Train total loss: 2.89163 \tReconstruction loss: 2.85509 \tLatent loss: 0.036536\n",
      "1889 Train total loss: 2.89741 \tReconstruction loss: 2.86043 \tLatent loss: 0.0369857\n",
      "1890 Train total loss: 3.03098 \tReconstruction loss: 2.99511 \tLatent loss: 0.035878\n",
      "1891 Train total loss: 2.98167 \tReconstruction loss: 2.94654 \tLatent loss: 0.0351284\n",
      "1892 Train total loss: 2.96422 \tReconstruction loss: 2.92827 \tLatent loss: 0.0359571\n",
      "1893 Train total loss: 3.01074 \tReconstruction loss: 2.97504 \tLatent loss: 0.0357003\n",
      "1894 Train total loss: 3.07949 \tReconstruction loss: 3.04462 \tLatent loss: 0.0348723\n",
      "1895 Train total loss: 2.8314 \tReconstruction loss: 2.79676 \tLatent loss: 0.0346449\n",
      "1896 Train total loss: 2.81078 \tReconstruction loss: 2.7767 \tLatent loss: 0.0340779\n",
      "1897 Train total loss: 2.83381 \tReconstruction loss: 2.80027 \tLatent loss: 0.0335392\n",
      "1898 Train total loss: 2.83567 \tReconstruction loss: 2.78216 \tLatent loss: 0.0535041\n",
      "1899 Train total loss: 2.96764 \tReconstruction loss: 2.81301 \tLatent loss: 0.154634\n",
      "1900 Train total loss: 3.15 \tReconstruction loss: 3.03107 \tLatent loss: 0.11893\n",
      "1901 Train total loss: 2.89679 \tReconstruction loss: 2.80597 \tLatent loss: 0.0908137\n",
      "1902 Train total loss: 2.91439 \tReconstruction loss: 2.83818 \tLatent loss: 0.0762032\n",
      "1903 Train total loss: 2.93374 \tReconstruction loss: 2.86599 \tLatent loss: 0.0677504\n",
      "1904 Train total loss: 3.1575 \tReconstruction loss: 3.09522 \tLatent loss: 0.0622784\n",
      "1905 Train total loss: 2.97477 \tReconstruction loss: 2.9164 \tLatent loss: 0.058374\n",
      "1906 Train total loss: 2.86623 \tReconstruction loss: 2.81087 \tLatent loss: 0.0553527\n",
      "1907 Train total loss: 2.97849 \tReconstruction loss: 2.92563 \tLatent loss: 0.052865\n",
      "1908 Train total loss: 2.90456 \tReconstruction loss: 2.8536 \tLatent loss: 0.0509635\n",
      "1909 Train total loss: 2.90847 \tReconstruction loss: 2.85937 \tLatent loss: 0.049093\n",
      "1910 Train total loss: 3.03395 \tReconstruction loss: 2.98657 \tLatent loss: 0.0473748\n",
      "1911 Train total loss: 2.9927 \tReconstruction loss: 2.94678 \tLatent loss: 0.0459203\n",
      "1912 Train total loss: 2.97307 \tReconstruction loss: 2.92856 \tLatent loss: 0.0445101\n",
      "1913 Train total loss: 3.02487 \tReconstruction loss: 2.98157 \tLatent loss: 0.0432972\n",
      "1914 Train total loss: 3.06227 \tReconstruction loss: 3.02024 \tLatent loss: 0.0420289\n",
      "1915 Train total loss: 2.83567 \tReconstruction loss: 2.7948 \tLatent loss: 0.0408683\n",
      "1916 Train total loss: 2.81506 \tReconstruction loss: 2.7751 \tLatent loss: 0.0399571\n",
      "1917 Train total loss: 2.83936 \tReconstruction loss: 2.80022 \tLatent loss: 0.039139\n",
      "1918 Train total loss: 2.81557 \tReconstruction loss: 2.77692 \tLatent loss: 0.0386507\n",
      "1919 Train total loss: 2.84668 \tReconstruction loss: 2.80704 \tLatent loss: 0.0396451\n",
      "1920 Train total loss: 3.07813 \tReconstruction loss: 3.03905 \tLatent loss: 0.0390892\n",
      "1921 Train total loss: 2.84832 \tReconstruction loss: 2.80933 \tLatent loss: 0.0389967\n",
      "1922 Train total loss: 2.87837 \tReconstruction loss: 2.84011 \tLatent loss: 0.0382633\n",
      "1923 Train total loss: 2.9083 \tReconstruction loss: 2.86965 \tLatent loss: 0.0386478\n",
      "1924 Train total loss: 3.12518 \tReconstruction loss: 3.08744 \tLatent loss: 0.0377365\n",
      "1925 Train total loss: 2.95001 \tReconstruction loss: 2.91324 \tLatent loss: 0.0367707\n",
      "1926 Train total loss: 2.84695 \tReconstruction loss: 2.81094 \tLatent loss: 0.036006\n",
      "1927 Train total loss: 2.96207 \tReconstruction loss: 2.92669 \tLatent loss: 0.0353811\n",
      "1928 Train total loss: 2.89125 \tReconstruction loss: 2.85666 \tLatent loss: 0.0345879\n",
      "1929 Train total loss: 2.89621 \tReconstruction loss: 2.8618 \tLatent loss: 0.0344098\n",
      "1930 Train total loss: 3.02058 \tReconstruction loss: 2.9859 \tLatent loss: 0.0346726\n",
      "1931 Train total loss: 2.98104 \tReconstruction loss: 2.94549 \tLatent loss: 0.0355493\n",
      "1932 Train total loss: 2.96333 \tReconstruction loss: 2.92901 \tLatent loss: 0.0343224\n",
      "1933 Train total loss: 3.01003 \tReconstruction loss: 2.97663 \tLatent loss: 0.0334016\n",
      "1934 Train total loss: 3.05809 \tReconstruction loss: 3.02192 \tLatent loss: 0.0361689\n",
      "1935 Train total loss: 2.83318 \tReconstruction loss: 2.79831 \tLatent loss: 0.0348757\n",
      "1936 Train total loss: 2.81834 \tReconstruction loss: 2.78435 \tLatent loss: 0.0339864\n",
      "1937 Train total loss: 2.83214 \tReconstruction loss: 2.79892 \tLatent loss: 0.0332212\n",
      "1938 Train total loss: 2.80861 \tReconstruction loss: 2.77589 \tLatent loss: 0.0327253\n",
      "1939 Train total loss: 2.839 \tReconstruction loss: 2.80617 \tLatent loss: 0.032829\n",
      "1940 Train total loss: 3.06984 \tReconstruction loss: 3.03509 \tLatent loss: 0.0347451\n",
      "1941 Train total loss: 2.84465 \tReconstruction loss: 2.80904 \tLatent loss: 0.0356131\n",
      "1942 Train total loss: 2.87569 \tReconstruction loss: 2.84136 \tLatent loss: 0.0343332\n",
      "1943 Train total loss: 2.90245 \tReconstruction loss: 2.86698 \tLatent loss: 0.0354655\n",
      "1944 Train total loss: 3.12531 \tReconstruction loss: 3.09137 \tLatent loss: 0.0339422\n",
      "1945 Train total loss: 2.94662 \tReconstruction loss: 2.91367 \tLatent loss: 0.0329538\n",
      "1946 Train total loss: 2.84443 \tReconstruction loss: 2.8122 \tLatent loss: 0.0322266\n",
      "1947 Train total loss: 2.96329 \tReconstruction loss: 2.93052 \tLatent loss: 0.032771\n",
      "1948 Train total loss: 2.89185 \tReconstruction loss: 2.85772 \tLatent loss: 0.0341332\n",
      "1949 Train total loss: 2.89476 \tReconstruction loss: 2.86093 \tLatent loss: 0.0338259\n",
      "1950 Train total loss: 3.01903 \tReconstruction loss: 2.98606 \tLatent loss: 0.0329752\n",
      "1951 Train total loss: 2.97947 \tReconstruction loss: 2.94663 \tLatent loss: 0.0328416\n",
      "1952 Train total loss: 2.96227 \tReconstruction loss: 2.93034 \tLatent loss: 0.0319287\n",
      "1953 Train total loss: 3.00924 \tReconstruction loss: 2.97584 \tLatent loss: 0.0333994\n",
      "1954 Train total loss: 3.05283 \tReconstruction loss: 3.01995 \tLatent loss: 0.0328711\n",
      "1955 Train total loss: 2.82914 \tReconstruction loss: 2.79705 \tLatent loss: 0.0320938\n",
      "1956 Train total loss: 2.80847 \tReconstruction loss: 2.77581 \tLatent loss: 0.0326633\n",
      "1957 Train total loss: 2.83328 \tReconstruction loss: 2.80147 \tLatent loss: 0.0318173\n",
      "1958 Train total loss: 2.80793 \tReconstruction loss: 2.77661 \tLatent loss: 0.0313213\n",
      "1959 Train total loss: 2.84108 \tReconstruction loss: 2.81003 \tLatent loss: 0.0310473\n",
      "1960 Train total loss: 3.07088 \tReconstruction loss: 3.03993 \tLatent loss: 0.0309449\n",
      "1961 Train total loss: 2.83924 \tReconstruction loss: 2.80884 \tLatent loss: 0.0303992\n",
      "1962 Train total loss: 2.87678 \tReconstruction loss: 2.84637 \tLatent loss: 0.0304052\n",
      "1963 Train total loss: 2.90607 \tReconstruction loss: 2.87421 \tLatent loss: 0.0318567\n",
      "1964 Train total loss: 3.12709 \tReconstruction loss: 3.09661 \tLatent loss: 0.0304808\n",
      "1965 Train total loss: 2.93888 \tReconstruction loss: 2.9094 \tLatent loss: 0.0294804\n",
      "1966 Train total loss: 2.84227 \tReconstruction loss: 2.81257 \tLatent loss: 0.0297059\n",
      "1967 Train total loss: 2.95823 \tReconstruction loss: 2.92873 \tLatent loss: 0.0295028\n",
      "1968 Train total loss: 2.88684 \tReconstruction loss: 2.85805 \tLatent loss: 0.0287968\n",
      "1969 Train total loss: 2.90045 \tReconstruction loss: 2.86179 \tLatent loss: 0.0386666\n",
      "1970 Train total loss: 3.02829 \tReconstruction loss: 2.98403 \tLatent loss: 0.0442608\n",
      "1971 Train total loss: 2.98714 \tReconstruction loss: 2.95158 \tLatent loss: 0.0355554\n",
      "1972 Train total loss: 2.96126 \tReconstruction loss: 2.92749 \tLatent loss: 0.0337653\n",
      "1973 Train total loss: 3.0112 \tReconstruction loss: 2.97892 \tLatent loss: 0.0322803\n",
      "1974 Train total loss: 3.05118 \tReconstruction loss: 3.01942 \tLatent loss: 0.031759\n",
      "1975 Train total loss: 2.84107 \tReconstruction loss: 2.80932 \tLatent loss: 0.0317499\n",
      "1976 Train total loss: 2.80547 \tReconstruction loss: 2.77502 \tLatent loss: 0.0304501\n",
      "1977 Train total loss: 2.83058 \tReconstruction loss: 2.8003 \tLatent loss: 0.0302768\n",
      "1978 Train total loss: 2.82985 \tReconstruction loss: 2.78045 \tLatent loss: 0.0493985\n",
      "1979 Train total loss: 2.85443 \tReconstruction loss: 2.80785 \tLatent loss: 0.0465836\n",
      "1980 Train total loss: 3.08145 \tReconstruction loss: 3.04214 \tLatent loss: 0.0393025\n",
      "1981 Train total loss: 2.84248 \tReconstruction loss: 2.80669 \tLatent loss: 0.0357892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1982 Train total loss: 2.87201 \tReconstruction loss: 2.83835 \tLatent loss: 0.0336598\n",
      "1983 Train total loss: 2.90105 \tReconstruction loss: 2.869 \tLatent loss: 0.0320425\n",
      "1984 Train total loss: 3.12149 \tReconstruction loss: 3.09074 \tLatent loss: 0.0307481\n",
      "1985 Train total loss: 2.94249 \tReconstruction loss: 2.91296 \tLatent loss: 0.0295325\n",
      "1986 Train total loss: 2.84708 \tReconstruction loss: 2.81847 \tLatent loss: 0.0286115\n",
      "1987 Train total loss: 2.95605 \tReconstruction loss: 2.92677 \tLatent loss: 0.0292766\n",
      "1988 Train total loss: 2.88587 \tReconstruction loss: 2.85671 \tLatent loss: 0.029156\n",
      "1989 Train total loss: 2.89779 \tReconstruction loss: 2.86529 \tLatent loss: 0.0324992\n",
      "1990 Train total loss: 3.01636 \tReconstruction loss: 2.98619 \tLatent loss: 0.0301731\n",
      "1991 Train total loss: 2.97802 \tReconstruction loss: 2.94924 \tLatent loss: 0.0287754\n",
      "1992 Train total loss: 2.96472 \tReconstruction loss: 2.92812 \tLatent loss: 0.0366019\n",
      "1993 Train total loss: 3.02207 \tReconstruction loss: 2.98954 \tLatent loss: 0.0325379\n",
      "1994 Train total loss: 3.05219 \tReconstruction loss: 3.02159 \tLatent loss: 0.0306063\n",
      "1995 Train total loss: 2.82457 \tReconstruction loss: 2.79541 \tLatent loss: 0.0291647\n",
      "1996 Train total loss: 2.80573 \tReconstruction loss: 2.77659 \tLatent loss: 0.0291385\n",
      "1997 Train total loss: 2.82915 \tReconstruction loss: 2.80121 \tLatent loss: 0.0279448\n",
      "1998 Train total loss: 3.0862 \tReconstruction loss: 3.05888 \tLatent loss: 0.0273222\n",
      "1999 Train total loss: 2.83501 \tReconstruction loss: 2.80754 \tLatent loss: 0.0274758\n",
      "2000 Train total loss: 3.06574 \tReconstruction loss: 3.03904 \tLatent loss: 0.026704\n",
      "2001 Train total loss: 2.83568 \tReconstruction loss: 2.80964 \tLatent loss: 0.02604\n",
      "2002 Train total loss: 2.86673 \tReconstruction loss: 2.84098 \tLatent loss: 0.0257453\n",
      "2003 Train total loss: 2.8964 \tReconstruction loss: 2.86807 \tLatent loss: 0.0283227\n",
      "2004 Train total loss: 3.11448 \tReconstruction loss: 3.08784 \tLatent loss: 0.0266386\n",
      "2005 Train total loss: 3.01236 \tReconstruction loss: 2.9118 \tLatent loss: 0.100558\n",
      "2006 Train total loss: 3.0782 \tReconstruction loss: 2.82468 \tLatent loss: 0.253519\n",
      "2007 Train total loss: 3.14949 \tReconstruction loss: 2.92534 \tLatent loss: 0.224142\n",
      "2008 Train total loss: 3.03828 \tReconstruction loss: 2.85527 \tLatent loss: 0.183013\n",
      "2009 Train total loss: 3.01256 \tReconstruction loss: 2.86014 \tLatent loss: 0.152419\n",
      "2010 Train total loss: 3.11762 \tReconstruction loss: 2.98745 \tLatent loss: 0.130168\n",
      "2011 Train total loss: 3.06168 \tReconstruction loss: 2.94812 \tLatent loss: 0.113562\n",
      "2012 Train total loss: 3.02861 \tReconstruction loss: 2.92777 \tLatent loss: 0.100844\n",
      "2013 Train total loss: 3.06544 \tReconstruction loss: 2.97453 \tLatent loss: 0.0909039\n",
      "2014 Train total loss: 3.10212 \tReconstruction loss: 3.01911 \tLatent loss: 0.0830103\n",
      "2015 Train total loss: 2.87032 \tReconstruction loss: 2.79367 \tLatent loss: 0.0766494\n",
      "2016 Train total loss: 2.84424 \tReconstruction loss: 2.77278 \tLatent loss: 0.0714611\n",
      "2017 Train total loss: 2.86418 \tReconstruction loss: 2.797 \tLatent loss: 0.0671754\n",
      "2018 Train total loss: 2.83816 \tReconstruction loss: 2.77456 \tLatent loss: 0.0635965\n",
      "2019 Train total loss: 2.86492 \tReconstruction loss: 2.80433 \tLatent loss: 0.060585\n",
      "2020 Train total loss: 3.09682 \tReconstruction loss: 3.03851 \tLatent loss: 0.0583069\n",
      "2021 Train total loss: 2.86121 \tReconstruction loss: 2.80544 \tLatent loss: 0.0557727\n",
      "2022 Train total loss: 2.89081 \tReconstruction loss: 2.83699 \tLatent loss: 0.0538185\n",
      "2023 Train total loss: 2.9181 \tReconstruction loss: 2.866 \tLatent loss: 0.0520972\n",
      "2024 Train total loss: 3.14325 \tReconstruction loss: 3.09268 \tLatent loss: 0.0505646\n",
      "2025 Train total loss: 2.96454 \tReconstruction loss: 2.91535 \tLatent loss: 0.0491948\n",
      "2026 Train total loss: 2.85754 \tReconstruction loss: 2.80959 \tLatent loss: 0.0479484\n",
      "2027 Train total loss: 2.97296 \tReconstruction loss: 2.92615 \tLatent loss: 0.0468071\n",
      "2028 Train total loss: 2.89976 \tReconstruction loss: 2.85401 \tLatent loss: 0.0457481\n",
      "2029 Train total loss: 2.90347 \tReconstruction loss: 2.8587 \tLatent loss: 0.0447721\n",
      "2030 Train total loss: 3.03057 \tReconstruction loss: 2.98671 \tLatent loss: 0.0438529\n",
      "2031 Train total loss: 2.98932 \tReconstruction loss: 2.94633 \tLatent loss: 0.0429911\n",
      "2032 Train total loss: 2.97068 \tReconstruction loss: 2.92851 \tLatent loss: 0.0421685\n",
      "2033 Train total loss: 3.01694 \tReconstruction loss: 2.97554 \tLatent loss: 0.0413941\n",
      "2034 Train total loss: 3.06146 \tReconstruction loss: 3.02081 \tLatent loss: 0.0406483\n",
      "2035 Train total loss: 2.83379 \tReconstruction loss: 2.79383 \tLatent loss: 0.0399552\n",
      "2036 Train total loss: 2.81452 \tReconstruction loss: 2.77474 \tLatent loss: 0.0397831\n",
      "2037 Train total loss: 2.83851 \tReconstruction loss: 2.7993 \tLatent loss: 0.0392045\n",
      "2038 Train total loss: 2.81315 \tReconstruction loss: 2.77463 \tLatent loss: 0.0385257\n",
      "2039 Train total loss: 2.84321 \tReconstruction loss: 2.80527 \tLatent loss: 0.0379433\n",
      "2040 Train total loss: 3.07669 \tReconstruction loss: 3.03907 \tLatent loss: 0.0376187\n",
      "2041 Train total loss: 2.84275 \tReconstruction loss: 2.80598 \tLatent loss: 0.0367724\n",
      "2042 Train total loss: 2.87362 \tReconstruction loss: 2.83737 \tLatent loss: 0.0362498\n",
      "2043 Train total loss: 2.90305 \tReconstruction loss: 2.86733 \tLatent loss: 0.0357207\n",
      "2044 Train total loss: 3.12467 \tReconstruction loss: 3.08949 \tLatent loss: 0.0351863\n",
      "2045 Train total loss: 2.94634 \tReconstruction loss: 2.9116 \tLatent loss: 0.0347392\n",
      "2046 Train total loss: 2.84423 \tReconstruction loss: 2.80985 \tLatent loss: 0.0343728\n",
      "2047 Train total loss: 2.95955 \tReconstruction loss: 2.92564 \tLatent loss: 0.0339179\n",
      "2048 Train total loss: 2.8895 \tReconstruction loss: 2.85607 \tLatent loss: 0.0334241\n",
      "2049 Train total loss: 2.89204 \tReconstruction loss: 2.8591 \tLatent loss: 0.0329476\n",
      "2050 Train total loss: 3.01843 \tReconstruction loss: 2.98593 \tLatent loss: 0.0325017\n",
      "2051 Train total loss: 2.97985 \tReconstruction loss: 2.94771 \tLatent loss: 0.032139\n",
      "2052 Train total loss: 2.96092 \tReconstruction loss: 2.9292 \tLatent loss: 0.0317177\n",
      "2053 Train total loss: 3.00888 \tReconstruction loss: 2.97752 \tLatent loss: 0.031359\n",
      "2054 Train total loss: 3.05718 \tReconstruction loss: 3.02065 \tLatent loss: 0.0365307\n",
      "2055 Train total loss: 2.8292 \tReconstruction loss: 2.79561 \tLatent loss: 0.0335891\n",
      "2056 Train total loss: 2.80729 \tReconstruction loss: 2.77477 \tLatent loss: 0.0325229\n",
      "2057 Train total loss: 2.82978 \tReconstruction loss: 2.79793 \tLatent loss: 0.031854\n",
      "2058 Train total loss: 2.80655 \tReconstruction loss: 2.77525 \tLatent loss: 0.0312983\n",
      "2059 Train total loss: 2.8367 \tReconstruction loss: 2.8059 \tLatent loss: 0.0307964\n",
      "2060 Train total loss: 3.07351 \tReconstruction loss: 3.04288 \tLatent loss: 0.0306261\n",
      "2061 Train total loss: 2.838 \tReconstruction loss: 2.80773 \tLatent loss: 0.0302633\n",
      "2062 Train total loss: 2.86959 \tReconstruction loss: 2.8392 \tLatent loss: 0.0303957\n",
      "2063 Train total loss: 2.89712 \tReconstruction loss: 2.86711 \tLatent loss: 0.0300096\n",
      "2064 Train total loss: 3.11844 \tReconstruction loss: 3.0889 \tLatent loss: 0.0295429\n",
      "2065 Train total loss: 2.94369 \tReconstruction loss: 2.91457 \tLatent loss: 0.0291179\n",
      "2066 Train total loss: 2.83925 \tReconstruction loss: 2.81049 \tLatent loss: 0.0287591\n",
      "2067 Train total loss: 2.95361 \tReconstruction loss: 2.92522 \tLatent loss: 0.0283891\n",
      "2068 Train total loss: 2.88351 \tReconstruction loss: 2.85549 \tLatent loss: 0.0280132\n",
      "2069 Train total loss: 2.88794 \tReconstruction loss: 2.86 \tLatent loss: 0.0279445\n",
      "2070 Train total loss: 3.01422 \tReconstruction loss: 2.9866 \tLatent loss: 0.0276229\n",
      "2071 Train total loss: 2.97503 \tReconstruction loss: 2.9475 \tLatent loss: 0.0275295\n",
      "2072 Train total loss: 2.95602 \tReconstruction loss: 2.92827 \tLatent loss: 0.0277531\n",
      "2073 Train total loss: 3.0073 \tReconstruction loss: 2.97844 \tLatent loss: 0.0288615\n",
      "2074 Train total loss: 3.05058 \tReconstruction loss: 3.02247 \tLatent loss: 0.0281067\n",
      "2075 Train total loss: 2.82456 \tReconstruction loss: 2.79695 \tLatent loss: 0.0276076\n",
      "2076 Train total loss: 2.80299 \tReconstruction loss: 2.77575 \tLatent loss: 0.027236\n",
      "2077 Train total loss: 2.82643 \tReconstruction loss: 2.79951 \tLatent loss: 0.0269197\n",
      "2078 Train total loss: 2.80466 \tReconstruction loss: 2.77745 \tLatent loss: 0.0272037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2079 Train total loss: 2.83642 \tReconstruction loss: 2.80967 \tLatent loss: 0.0267452\n",
      "2080 Train total loss: 3.06586 \tReconstruction loss: 3.03927 \tLatent loss: 0.0265931\n",
      "2081 Train total loss: 2.84031 \tReconstruction loss: 2.8143 \tLatent loss: 0.0260082\n",
      "2082 Train total loss: 2.8703 \tReconstruction loss: 2.84438 \tLatent loss: 0.0259166\n",
      "2083 Train total loss: 2.89853 \tReconstruction loss: 2.87283 \tLatent loss: 0.0257037\n",
      "2084 Train total loss: 3.1146 \tReconstruction loss: 3.08918 \tLatent loss: 0.0254195\n",
      "2085 Train total loss: 2.93883 \tReconstruction loss: 2.91288 \tLatent loss: 0.0259511\n",
      "2086 Train total loss: 2.84185 \tReconstruction loss: 2.81336 \tLatent loss: 0.0284872\n",
      "2087 Train total loss: 2.95421 \tReconstruction loss: 2.92587 \tLatent loss: 0.0283356\n",
      "2088 Train total loss: 2.88262 \tReconstruction loss: 2.85574 \tLatent loss: 0.026881\n",
      "2089 Train total loss: 2.8876 \tReconstruction loss: 2.86137 \tLatent loss: 0.0262337\n",
      "2090 Train total loss: 3.01199 \tReconstruction loss: 2.98622 \tLatent loss: 0.0257722\n",
      "2091 Train total loss: 2.97143 \tReconstruction loss: 2.94609 \tLatent loss: 0.0253427\n",
      "2092 Train total loss: 2.9714 \tReconstruction loss: 2.94647 \tLatent loss: 0.0249271\n",
      "2093 Train total loss: 3.01695 \tReconstruction loss: 2.96903 \tLatent loss: 0.0479248\n",
      "2094 Train total loss: 3.20608 \tReconstruction loss: 3.00563 \tLatent loss: 0.200453\n",
      "2095 Train total loss: 2.99814 \tReconstruction loss: 2.80321 \tLatent loss: 0.194927\n",
      "2096 Train total loss: 2.94483 \tReconstruction loss: 2.7766 \tLatent loss: 0.16823\n",
      "2097 Train total loss: 2.9445 \tReconstruction loss: 2.79828 \tLatent loss: 0.14622\n",
      "2098 Train total loss: 2.90501 \tReconstruction loss: 2.7757 \tLatent loss: 0.129313\n",
      "2099 Train total loss: 2.92033 \tReconstruction loss: 2.80422 \tLatent loss: 0.116109\n",
      "2100 Train total loss: 3.14311 \tReconstruction loss: 3.03692 \tLatent loss: 0.106194\n",
      "2101 Train total loss: 2.90187 \tReconstruction loss: 2.80479 \tLatent loss: 0.0970776\n",
      "2102 Train total loss: 2.92724 \tReconstruction loss: 2.83714 \tLatent loss: 0.0901004\n",
      "2103 Train total loss: 2.9502 \tReconstruction loss: 2.86588 \tLatent loss: 0.0843207\n",
      "2104 Train total loss: 3.17337 \tReconstruction loss: 3.09389 \tLatent loss: 0.0794855\n",
      "2105 Train total loss: 2.99088 \tReconstruction loss: 2.91547 \tLatent loss: 0.075405\n",
      "2106 Train total loss: 2.88195 \tReconstruction loss: 2.81002 \tLatent loss: 0.0719332\n",
      "2107 Train total loss: 2.99435 \tReconstruction loss: 2.9254 \tLatent loss: 0.0689565\n",
      "2108 Train total loss: 2.91998 \tReconstruction loss: 2.85359 \tLatent loss: 0.0663856\n",
      "2109 Train total loss: 2.92296 \tReconstruction loss: 2.85881 \tLatent loss: 0.0641494\n",
      "2110 Train total loss: 3.04913 \tReconstruction loss: 2.98694 \tLatent loss: 0.0621876\n",
      "2111 Train total loss: 3.00749 \tReconstruction loss: 2.94703 \tLatent loss: 0.060457\n",
      "2112 Train total loss: 2.98739 \tReconstruction loss: 2.92847 \tLatent loss: 0.0589178\n",
      "2113 Train total loss: 3.03246 \tReconstruction loss: 2.97492 \tLatent loss: 0.0575386\n",
      "2114 Train total loss: 3.07699 \tReconstruction loss: 3.0207 \tLatent loss: 0.0562958\n",
      "2115 Train total loss: 2.84849 \tReconstruction loss: 2.79332 \tLatent loss: 0.0551663\n",
      "2116 Train total loss: 2.82719 \tReconstruction loss: 2.77305 \tLatent loss: 0.0541343\n",
      "2117 Train total loss: 2.85002 \tReconstruction loss: 2.79684 \tLatent loss: 0.0531833\n",
      "2118 Train total loss: 2.82654 \tReconstruction loss: 2.77424 \tLatent loss: 0.0523041\n",
      "2119 Train total loss: 2.85603 \tReconstruction loss: 2.80454 \tLatent loss: 0.0514842\n",
      "2120 Train total loss: 3.09005 \tReconstruction loss: 3.03905 \tLatent loss: 0.0510051\n",
      "2121 Train total loss: 2.85577 \tReconstruction loss: 2.80578 \tLatent loss: 0.0499969\n",
      "2122 Train total loss: 2.8863 \tReconstruction loss: 2.83698 \tLatent loss: 0.0493205\n",
      "2123 Train total loss: 2.91446 \tReconstruction loss: 2.86579 \tLatent loss: 0.0486726\n",
      "2124 Train total loss: 3.13936 \tReconstruction loss: 3.09131 \tLatent loss: 0.0480523\n",
      "2125 Train total loss: 2.96202 \tReconstruction loss: 2.91456 \tLatent loss: 0.0474586\n",
      "2126 Train total loss: 2.85634 \tReconstruction loss: 2.80946 \tLatent loss: 0.0468827\n",
      "2127 Train total loss: 2.97188 \tReconstruction loss: 2.92555 \tLatent loss: 0.0463291\n",
      "2128 Train total loss: 2.90012 \tReconstruction loss: 2.85433 \tLatent loss: 0.0457913\n",
      "2129 Train total loss: 2.90401 \tReconstruction loss: 2.85874 \tLatent loss: 0.0452776\n",
      "2130 Train total loss: 3.0311 \tReconstruction loss: 2.98632 \tLatent loss: 0.0447749\n",
      "2131 Train total loss: 2.99121 \tReconstruction loss: 2.94692 \tLatent loss: 0.044285\n",
      "2132 Train total loss: 2.97214 \tReconstruction loss: 2.92834 \tLatent loss: 0.043798\n",
      "2133 Train total loss: 3.02083 \tReconstruction loss: 2.9775 \tLatent loss: 0.0433266\n",
      "2134 Train total loss: 3.065 \tReconstruction loss: 3.02214 \tLatent loss: 0.0428632\n",
      "2135 Train total loss: 2.83632 \tReconstruction loss: 2.79391 \tLatent loss: 0.042406\n",
      "2136 Train total loss: 2.81583 \tReconstruction loss: 2.77387 \tLatent loss: 0.0419585\n",
      "2137 Train total loss: 2.83845 \tReconstruction loss: 2.79693 \tLatent loss: 0.0415177\n",
      "2138 Train total loss: 2.81493 \tReconstruction loss: 2.77384 \tLatent loss: 0.041083\n",
      "2139 Train total loss: 2.84575 \tReconstruction loss: 2.8051 \tLatent loss: 0.0406513\n",
      "2140 Train total loss: 3.08044 \tReconstruction loss: 3.03994 \tLatent loss: 0.0404946\n",
      "2141 Train total loss: 2.84601 \tReconstruction loss: 2.80618 \tLatent loss: 0.0398283\n",
      "2142 Train total loss: 2.87643 \tReconstruction loss: 2.837 \tLatent loss: 0.039426\n",
      "2143 Train total loss: 2.90506 \tReconstruction loss: 2.86604 \tLatent loss: 0.0390194\n",
      "2144 Train total loss: 3.12872 \tReconstruction loss: 3.0901 \tLatent loss: 0.0386196\n",
      "2145 Train total loss: 2.95191 \tReconstruction loss: 2.91368 \tLatent loss: 0.0382292\n",
      "2146 Train total loss: 2.84743 \tReconstruction loss: 2.80959 \tLatent loss: 0.0378364\n",
      "2147 Train total loss: 2.96269 \tReconstruction loss: 2.92522 \tLatent loss: 0.0374646\n",
      "2148 Train total loss: 2.89212 \tReconstruction loss: 2.85502 \tLatent loss: 0.0370975\n",
      "2149 Train total loss: 2.8955 \tReconstruction loss: 2.85879 \tLatent loss: 0.0367146\n",
      "2150 Train total loss: 3.02346 \tReconstruction loss: 2.98712 \tLatent loss: 0.036343\n",
      "2151 Train total loss: 2.98183 \tReconstruction loss: 2.94586 \tLatent loss: 0.0359705\n",
      "2152 Train total loss: 2.96584 \tReconstruction loss: 2.93024 \tLatent loss: 0.0356054\n",
      "2153 Train total loss: 3.01097 \tReconstruction loss: 2.97574 \tLatent loss: 0.035237\n",
      "2154 Train total loss: 3.05709 \tReconstruction loss: 3.02222 \tLatent loss: 0.0348716\n",
      "2155 Train total loss: 2.8284 \tReconstruction loss: 2.79388 \tLatent loss: 0.0345217\n",
      "2156 Train total loss: 2.8083 \tReconstruction loss: 2.77412 \tLatent loss: 0.0341747\n",
      "2157 Train total loss: 2.83138 \tReconstruction loss: 2.79753 \tLatent loss: 0.0338432\n",
      "2158 Train total loss: 2.80792 \tReconstruction loss: 2.77441 \tLatent loss: 0.0335152\n",
      "2159 Train total loss: 2.8389 \tReconstruction loss: 2.80573 \tLatent loss: 0.0331729\n",
      "2160 Train total loss: 3.07351 \tReconstruction loss: 3.0404 \tLatent loss: 0.0331121\n",
      "2161 Train total loss: 2.83884 \tReconstruction loss: 2.80633 \tLatent loss: 0.0325057\n",
      "2162 Train total loss: 2.86953 \tReconstruction loss: 2.83735 \tLatent loss: 0.0321793\n",
      "2163 Train total loss: 2.8982 \tReconstruction loss: 2.86633 \tLatent loss: 0.0318663\n",
      "2164 Train total loss: 3.12027 \tReconstruction loss: 3.08871 \tLatent loss: 0.0315537\n",
      "2165 Train total loss: 2.94504 \tReconstruction loss: 2.91379 \tLatent loss: 0.0312462\n",
      "2166 Train total loss: 2.8399 \tReconstruction loss: 2.80896 \tLatent loss: 0.0309437\n",
      "2167 Train total loss: 2.95559 \tReconstruction loss: 2.92494 \tLatent loss: 0.0306498\n",
      "2168 Train total loss: 2.88672 \tReconstruction loss: 2.85637 \tLatent loss: 0.0303495\n",
      "2169 Train total loss: 2.88911 \tReconstruction loss: 2.85905 \tLatent loss: 0.0300603\n",
      "2170 Train total loss: 3.01526 \tReconstruction loss: 2.98527 \tLatent loss: 0.0299942\n",
      "2171 Train total loss: 2.97559 \tReconstruction loss: 2.94574 \tLatent loss: 0.0298454\n",
      "2172 Train total loss: 2.9605 \tReconstruction loss: 2.93099 \tLatent loss: 0.0295094\n",
      "2173 Train total loss: 3.00605 \tReconstruction loss: 2.97608 \tLatent loss: 0.0299775\n",
      "2174 Train total loss: 3.05175 \tReconstruction loss: 3.0224 \tLatent loss: 0.0293469\n",
      "2175 Train total loss: 2.82474 \tReconstruction loss: 2.79576 \tLatent loss: 0.0289782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2176 Train total loss: 2.80318 \tReconstruction loss: 2.77455 \tLatent loss: 0.0286352\n",
      "2177 Train total loss: 2.82682 \tReconstruction loss: 2.7985 \tLatent loss: 0.0283197\n",
      "2178 Train total loss: 2.83609 \tReconstruction loss: 2.80806 \tLatent loss: 0.0280309\n",
      "2179 Train total loss: 2.8348 \tReconstruction loss: 2.80705 \tLatent loss: 0.027748\n",
      "2180 Train total loss: 3.06645 \tReconstruction loss: 3.03867 \tLatent loss: 0.0277785\n",
      "2181 Train total loss: 2.83506 \tReconstruction loss: 2.80785 \tLatent loss: 0.0272179\n",
      "2182 Train total loss: 2.86457 \tReconstruction loss: 2.83758 \tLatent loss: 0.0269818\n",
      "2183 Train total loss: 2.89488 \tReconstruction loss: 2.86817 \tLatent loss: 0.0267095\n",
      "2184 Train total loss: 3.11605 \tReconstruction loss: 3.08958 \tLatent loss: 0.0264686\n",
      "2185 Train total loss: 2.94072 \tReconstruction loss: 2.91453 \tLatent loss: 0.0261983\n",
      "2186 Train total loss: 2.83562 \tReconstruction loss: 2.80964 \tLatent loss: 0.0259798\n",
      "2187 Train total loss: 2.95372 \tReconstruction loss: 2.92778 \tLatent loss: 0.0259401\n",
      "2188 Train total loss: 2.88284 \tReconstruction loss: 2.85722 \tLatent loss: 0.0256111\n",
      "2189 Train total loss: 2.88794 \tReconstruction loss: 2.86262 \tLatent loss: 0.0253196\n",
      "2190 Train total loss: 3.01077 \tReconstruction loss: 2.98575 \tLatent loss: 0.0250251\n",
      "2191 Train total loss: 2.97162 \tReconstruction loss: 2.94679 \tLatent loss: 0.0248358\n",
      "2192 Train total loss: 2.9541 \tReconstruction loss: 2.92948 \tLatent loss: 0.0246237\n",
      "2193 Train total loss: 2.99989 \tReconstruction loss: 2.97545 \tLatent loss: 0.0244443\n",
      "2194 Train total loss: 3.04389 \tReconstruction loss: 3.01963 \tLatent loss: 0.0242689\n",
      "2195 Train total loss: 2.82059 \tReconstruction loss: 2.79646 \tLatent loss: 0.0241287\n",
      "2196 Train total loss: 2.79939 \tReconstruction loss: 2.77552 \tLatent loss: 0.0238638\n",
      "2197 Train total loss: 2.82156 \tReconstruction loss: 2.7979 \tLatent loss: 0.0236554\n",
      "2198 Train total loss: 2.79976 \tReconstruction loss: 2.77634 \tLatent loss: 0.0234235\n",
      "2199 Train total loss: 2.8341 \tReconstruction loss: 2.81092 \tLatent loss: 0.0231738\n",
      "2200 Train total loss: 3.06834 \tReconstruction loss: 3.0451 \tLatent loss: 0.0232394\n",
      "2201 Train total loss: 2.82981 \tReconstruction loss: 2.80707 \tLatent loss: 0.0227377\n",
      "2202 Train total loss: 2.86139 \tReconstruction loss: 2.83858 \tLatent loss: 0.0228045\n",
      "2203 Train total loss: 2.8895 \tReconstruction loss: 2.86683 \tLatent loss: 0.0226738\n",
      "2204 Train total loss: 3.11058 \tReconstruction loss: 3.08814 \tLatent loss: 0.0224395\n",
      "2205 Train total loss: 2.93878 \tReconstruction loss: 2.91655 \tLatent loss: 0.022226\n",
      "2206 Train total loss: 2.83472 \tReconstruction loss: 2.81245 \tLatent loss: 0.0222739\n",
      "2207 Train total loss: 2.9494 \tReconstruction loss: 2.92733 \tLatent loss: 0.0220687\n",
      "2208 Train total loss: 2.87969 \tReconstruction loss: 2.85789 \tLatent loss: 0.021798\n",
      "2209 Train total loss: 2.88262 \tReconstruction loss: 2.86104 \tLatent loss: 0.0215797\n",
      "2210 Train total loss: 3.00752 \tReconstruction loss: 2.98613 \tLatent loss: 0.0213886\n",
      "2211 Train total loss: 2.96727 \tReconstruction loss: 2.94597 \tLatent loss: 0.0213001\n",
      "2212 Train total loss: 2.9509 \tReconstruction loss: 2.9297 \tLatent loss: 0.0212022\n",
      "2213 Train total loss: 2.99956 \tReconstruction loss: 2.97828 \tLatent loss: 0.0212818\n",
      "2214 Train total loss: 3.04433 \tReconstruction loss: 3.02324 \tLatent loss: 0.0210937\n",
      "2215 Train total loss: 2.84213 \tReconstruction loss: 2.7987 \tLatent loss: 0.0434274\n",
      "2216 Train total loss: 2.84033 \tReconstruction loss: 2.77776 \tLatent loss: 0.062568\n",
      "2217 Train total loss: 2.84476 \tReconstruction loss: 2.7993 \tLatent loss: 0.0454547\n",
      "2218 Train total loss: 2.81101 \tReconstruction loss: 2.7758 \tLatent loss: 0.0352115\n",
      "2219 Train total loss: 2.83565 \tReconstruction loss: 2.80532 \tLatent loss: 0.0303276\n",
      "2220 Train total loss: 3.06902 \tReconstruction loss: 3.04082 \tLatent loss: 0.0281941\n",
      "2221 Train total loss: 2.83314 \tReconstruction loss: 2.80679 \tLatent loss: 0.0263511\n",
      "2222 Train total loss: 2.86404 \tReconstruction loss: 2.83851 \tLatent loss: 0.0255354\n",
      "2223 Train total loss: 2.89245 \tReconstruction loss: 2.86753 \tLatent loss: 0.0249196\n",
      "2224 Train total loss: 3.11556 \tReconstruction loss: 3.09123 \tLatent loss: 0.0243231\n",
      "2225 Train total loss: 2.9375 \tReconstruction loss: 2.91365 \tLatent loss: 0.023849\n",
      "2226 Train total loss: 2.834 \tReconstruction loss: 2.81051 \tLatent loss: 0.0234949\n",
      "2227 Train total loss: 2.949 \tReconstruction loss: 2.92583 \tLatent loss: 0.0231694\n",
      "2228 Train total loss: 2.87783 \tReconstruction loss: 2.85502 \tLatent loss: 0.0228021\n",
      "2229 Train total loss: 2.88259 \tReconstruction loss: 2.86016 \tLatent loss: 0.0224324\n",
      "2230 Train total loss: 3.00805 \tReconstruction loss: 2.98596 \tLatent loss: 0.0220865\n",
      "2231 Train total loss: 2.96777 \tReconstruction loss: 2.946 \tLatent loss: 0.021764\n",
      "2232 Train total loss: 2.95055 \tReconstruction loss: 2.92884 \tLatent loss: 0.0217133\n",
      "2233 Train total loss: 3.00218 \tReconstruction loss: 2.98064 \tLatent loss: 0.02154\n",
      "2234 Train total loss: 3.0441 \tReconstruction loss: 3.02288 \tLatent loss: 0.0212142\n",
      "2235 Train total loss: 2.81694 \tReconstruction loss: 2.79533 \tLatent loss: 0.0216056\n",
      "2236 Train total loss: 2.79543 \tReconstruction loss: 2.77437 \tLatent loss: 0.0210628\n",
      "2237 Train total loss: 2.82105 \tReconstruction loss: 2.80028 \tLatent loss: 0.0207707\n",
      "2238 Train total loss: 2.79805 \tReconstruction loss: 2.77754 \tLatent loss: 0.0205088\n",
      "2239 Train total loss: 2.82822 \tReconstruction loss: 2.80808 \tLatent loss: 0.0201346\n",
      "2240 Train total loss: 3.05905 \tReconstruction loss: 3.03883 \tLatent loss: 0.0202198\n",
      "2241 Train total loss: 2.82882 \tReconstruction loss: 2.80879 \tLatent loss: 0.0200326\n",
      "2242 Train total loss: 2.86081 \tReconstruction loss: 2.84095 \tLatent loss: 0.0198655\n",
      "2243 Train total loss: 2.88988 \tReconstruction loss: 2.87038 \tLatent loss: 0.0195047\n",
      "2244 Train total loss: 3.11172 \tReconstruction loss: 3.09246 \tLatent loss: 0.0192621\n",
      "2245 Train total loss: 2.93148 \tReconstruction loss: 2.91238 \tLatent loss: 0.0190988\n",
      "2246 Train total loss: 2.8308 \tReconstruction loss: 2.81202 \tLatent loss: 0.0187877\n",
      "2247 Train total loss: 2.94541 \tReconstruction loss: 2.92603 \tLatent loss: 0.0193788\n",
      "2248 Train total loss: 2.87486 \tReconstruction loss: 2.85602 \tLatent loss: 0.0188369\n",
      "2249 Train total loss: 2.88319 \tReconstruction loss: 2.86462 \tLatent loss: 0.0185707\n",
      "2250 Train total loss: 3.00397 \tReconstruction loss: 2.98531 \tLatent loss: 0.0186632\n",
      "2251 Train total loss: 2.96529 \tReconstruction loss: 2.94613 \tLatent loss: 0.0191647\n",
      "2252 Train total loss: 2.94824 \tReconstruction loss: 2.92967 \tLatent loss: 0.0185674\n",
      "2253 Train total loss: 2.99646 \tReconstruction loss: 2.97828 \tLatent loss: 0.0181884\n",
      "2254 Train total loss: 3.07342 \tReconstruction loss: 3.05551 \tLatent loss: 0.0179035\n",
      "2255 Train total loss: 2.81281 \tReconstruction loss: 2.79496 \tLatent loss: 0.017848\n",
      "2256 Train total loss: 2.80844 \tReconstruction loss: 2.79094 \tLatent loss: 0.017494\n",
      "2257 Train total loss: 2.81641 \tReconstruction loss: 2.79895 \tLatent loss: 0.0174654\n",
      "2258 Train total loss: 2.79495 \tReconstruction loss: 2.77724 \tLatent loss: 0.0177118\n",
      "2259 Train total loss: 2.82477 \tReconstruction loss: 2.80744 \tLatent loss: 0.01733\n",
      "2260 Train total loss: 3.05605 \tReconstruction loss: 3.03848 \tLatent loss: 0.017571\n",
      "2261 Train total loss: 2.82563 \tReconstruction loss: 2.80858 \tLatent loss: 0.0170525\n",
      "2262 Train total loss: 2.96397 \tReconstruction loss: 2.94737 \tLatent loss: 0.0166049\n",
      "2263 Train total loss: 2.88523 \tReconstruction loss: 2.86896 \tLatent loss: 0.0162674\n",
      "2264 Train total loss: 3.10635 \tReconstruction loss: 3.08974 \tLatent loss: 0.0166009\n",
      "2265 Train total loss: 2.9434 \tReconstruction loss: 2.92654 \tLatent loss: 0.0168638\n",
      "2266 Train total loss: 2.83041 \tReconstruction loss: 2.81403 \tLatent loss: 0.0163731\n",
      "2267 Train total loss: 2.94252 \tReconstruction loss: 2.92593 \tLatent loss: 0.0165927\n",
      "2268 Train total loss: 2.87708 \tReconstruction loss: 2.86067 \tLatent loss: 0.0164119\n",
      "2269 Train total loss: 2.88421 \tReconstruction loss: 2.86081 \tLatent loss: 0.0234028\n",
      "2270 Train total loss: 3.00724 \tReconstruction loss: 2.98737 \tLatent loss: 0.0198783\n",
      "2271 Train total loss: 2.96526 \tReconstruction loss: 2.94719 \tLatent loss: 0.0180715\n",
      "2272 Train total loss: 2.95135 \tReconstruction loss: 2.93396 \tLatent loss: 0.0173921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2273 Train total loss: 2.99372 \tReconstruction loss: 2.97691 \tLatent loss: 0.0168078\n",
      "2274 Train total loss: 3.03694 \tReconstruction loss: 3.02042 \tLatent loss: 0.0165112\n",
      "2275 Train total loss: 2.8106 \tReconstruction loss: 2.79453 \tLatent loss: 0.0160658\n",
      "2276 Train total loss: 2.79518 \tReconstruction loss: 2.77882 \tLatent loss: 0.016353\n",
      "2277 Train total loss: 2.81635 \tReconstruction loss: 2.80003 \tLatent loss: 0.016325\n",
      "2278 Train total loss: 2.79108 \tReconstruction loss: 2.77542 \tLatent loss: 0.0156593\n",
      "2279 Train total loss: 2.84997 \tReconstruction loss: 2.83476 \tLatent loss: 0.0152102\n",
      "2280 Train total loss: 3.06944 \tReconstruction loss: 3.05396 \tLatent loss: 0.0154829\n",
      "2281 Train total loss: 2.82224 \tReconstruction loss: 2.80747 \tLatent loss: 0.0147669\n",
      "2282 Train total loss: 2.85427 \tReconstruction loss: 2.83964 \tLatent loss: 0.0146253\n",
      "2283 Train total loss: 2.8832 \tReconstruction loss: 2.86884 \tLatent loss: 0.0143524\n",
      "2284 Train total loss: 3.10292 \tReconstruction loss: 3.08872 \tLatent loss: 0.0142032\n",
      "2285 Train total loss: 2.9302 \tReconstruction loss: 2.91623 \tLatent loss: 0.0139693\n",
      "2286 Train total loss: 2.82501 \tReconstruction loss: 2.81134 \tLatent loss: 0.0136692\n",
      "2287 Train total loss: 2.93975 \tReconstruction loss: 2.92622 \tLatent loss: 0.0135313\n",
      "2288 Train total loss: 2.86949 \tReconstruction loss: 2.85479 \tLatent loss: 0.0146967\n",
      "2289 Train total loss: 2.87879 \tReconstruction loss: 2.86396 \tLatent loss: 0.0148312\n",
      "2290 Train total loss: 3.0065 \tReconstruction loss: 2.98648 \tLatent loss: 0.0200202\n",
      "2291 Train total loss: 2.96076 \tReconstruction loss: 2.94486 \tLatent loss: 0.0158983\n",
      "2292 Train total loss: 2.94518 \tReconstruction loss: 2.93048 \tLatent loss: 0.0146957\n",
      "2293 Train total loss: 2.98973 \tReconstruction loss: 2.97575 \tLatent loss: 0.0139811\n",
      "2294 Train total loss: 3.03561 \tReconstruction loss: 3.02206 \tLatent loss: 0.0135585\n",
      "2295 Train total loss: 2.81503 \tReconstruction loss: 2.7972 \tLatent loss: 0.0178274\n",
      "2296 Train total loss: 2.79328 \tReconstruction loss: 2.77806 \tLatent loss: 0.015219\n",
      "2297 Train total loss: 2.81246 \tReconstruction loss: 2.79855 \tLatent loss: 0.0139172\n",
      "2298 Train total loss: 2.7933 \tReconstruction loss: 2.7798 \tLatent loss: 0.0134985\n",
      "2299 Train total loss: 2.81908 \tReconstruction loss: 2.80602 \tLatent loss: 0.0130555\n",
      "2300 Train total loss: 3.04883 \tReconstruction loss: 3.03596 \tLatent loss: 0.0128653\n",
      "2301 Train total loss: 2.8191 \tReconstruction loss: 2.80689 \tLatent loss: 0.01221\n",
      "2302 Train total loss: 2.85067 \tReconstruction loss: 2.83878 \tLatent loss: 0.0118824\n",
      "2303 Train total loss: 2.87956 \tReconstruction loss: 2.86798 \tLatent loss: 0.0115838\n",
      "2304 Train total loss: 3.09849 \tReconstruction loss: 3.0869 \tLatent loss: 0.0115909\n",
      "2305 Train total loss: 2.92263 \tReconstruction loss: 2.91104 \tLatent loss: 0.0115957\n",
      "2306 Train total loss: 2.82539 \tReconstruction loss: 2.81423 \tLatent loss: 0.0111646\n",
      "2307 Train total loss: 2.93643 \tReconstruction loss: 2.92536 \tLatent loss: 0.0110675\n",
      "2308 Train total loss: 2.86833 \tReconstruction loss: 2.85744 \tLatent loss: 0.0108944\n",
      "2309 Train total loss: 2.87883 \tReconstruction loss: 2.86825 \tLatent loss: 0.010584\n",
      "2310 Train total loss: 2.99568 \tReconstruction loss: 2.98518 \tLatent loss: 0.0105027\n",
      "2311 Train total loss: 2.96468 \tReconstruction loss: 2.95439 \tLatent loss: 0.0102901\n",
      "2312 Train total loss: 2.93995 \tReconstruction loss: 2.92989 \tLatent loss: 0.0100568\n",
      "2313 Train total loss: 2.99614 \tReconstruction loss: 2.98632 \tLatent loss: 0.00981696\n",
      "2314 Train total loss: 3.03332 \tReconstruction loss: 3.02384 \tLatent loss: 0.00947256\n",
      "2315 Train total loss: 2.80785 \tReconstruction loss: 2.79768 \tLatent loss: 0.0101791\n",
      "2316 Train total loss: 2.78939 \tReconstruction loss: 2.78008 \tLatent loss: 0.00931193\n",
      "2317 Train total loss: 2.80873 \tReconstruction loss: 2.79934 \tLatent loss: 0.00938355\n",
      "2318 Train total loss: 2.78525 \tReconstruction loss: 2.77613 \tLatent loss: 0.00912536\n",
      "2319 Train total loss: 2.81458 \tReconstruction loss: 2.80572 \tLatent loss: 0.00886595\n",
      "2320 Train total loss: 3.04795 \tReconstruction loss: 3.03836 \tLatent loss: 0.00958513\n",
      "2321 Train total loss: 2.82376 \tReconstruction loss: 2.81298 \tLatent loss: 0.0107765\n",
      "2322 Train total loss: 2.8487 \tReconstruction loss: 2.83971 \tLatent loss: 0.00899051\n",
      "2323 Train total loss: 2.87682 \tReconstruction loss: 2.86705 \tLatent loss: 0.00976382\n",
      "2324 Train total loss: 3.10116 \tReconstruction loss: 3.09203 \tLatent loss: 0.00912423\n",
      "2325 Train total loss: 2.92021 \tReconstruction loss: 2.91166 \tLatent loss: 0.00855284\n",
      "2326 Train total loss: 2.82039 \tReconstruction loss: 2.81224 \tLatent loss: 0.00814725\n",
      "2327 Train total loss: 2.93578 \tReconstruction loss: 2.92789 \tLatent loss: 0.00789475\n",
      "2328 Train total loss: 2.87636 \tReconstruction loss: 2.86863 \tLatent loss: 0.0077232\n",
      "2329 Train total loss: 2.86789 \tReconstruction loss: 2.86016 \tLatent loss: 0.00773498\n",
      "2330 Train total loss: 2.99373 \tReconstruction loss: 2.98602 \tLatent loss: 0.00770435\n",
      "2331 Train total loss: 2.95436 \tReconstruction loss: 2.94635 \tLatent loss: 0.00801679\n",
      "2332 Train total loss: 2.93862 \tReconstruction loss: 2.93081 \tLatent loss: 0.00780839\n",
      "2333 Train total loss: 2.98892 \tReconstruction loss: 2.98159 \tLatent loss: 0.00732322\n",
      "2334 Train total loss: 3.02955 \tReconstruction loss: 3.02251 \tLatent loss: 0.00703871\n",
      "2335 Train total loss: 2.81065 \tReconstruction loss: 2.80389 \tLatent loss: 0.006765\n",
      "2336 Train total loss: 2.78109 \tReconstruction loss: 2.77449 \tLatent loss: 0.00660765\n",
      "2337 Train total loss: 2.8071 \tReconstruction loss: 2.80059 \tLatent loss: 0.00651083\n",
      "2338 Train total loss: 2.78464 \tReconstruction loss: 2.77832 \tLatent loss: 0.00632465\n",
      "2339 Train total loss: 2.8194 \tReconstruction loss: 2.8134 \tLatent loss: 0.00600176\n",
      "2340 Train total loss: 3.04532 \tReconstruction loss: 3.03917 \tLatent loss: 0.00614893\n",
      "2341 Train total loss: 2.81647 \tReconstruction loss: 2.81041 \tLatent loss: 0.00605707\n",
      "2342 Train total loss: 2.84798 \tReconstruction loss: 2.84229 \tLatent loss: 0.00569308\n",
      "2343 Train total loss: 2.87459 \tReconstruction loss: 2.86906 \tLatent loss: 0.00552993\n",
      "2344 Train total loss: 3.09543 \tReconstruction loss: 3.08989 \tLatent loss: 0.00553778\n",
      "2345 Train total loss: 2.91795 \tReconstruction loss: 2.91257 \tLatent loss: 0.00537831\n",
      "2346 Train total loss: 2.81779 \tReconstruction loss: 2.81276 \tLatent loss: 0.00503074\n",
      "2347 Train total loss: 2.93096 \tReconstruction loss: 2.92607 \tLatent loss: 0.00489015\n",
      "2348 Train total loss: 2.86067 \tReconstruction loss: 2.85591 \tLatent loss: 0.00476081\n",
      "2349 Train total loss: 2.8658 \tReconstruction loss: 2.86114 \tLatent loss: 0.00465255\n",
      "2350 Train total loss: 2.99102 \tReconstruction loss: 2.98651 \tLatent loss: 0.00450513\n",
      "2351 Train total loss: 2.95245 \tReconstruction loss: 2.94814 \tLatent loss: 0.00431517\n",
      "2352 Train total loss: 2.93775 \tReconstruction loss: 2.93365 \tLatent loss: 0.00410634\n",
      "2353 Train total loss: 2.98289 \tReconstruction loss: 2.97895 \tLatent loss: 0.00393861\n",
      "2354 Train total loss: 3.02664 \tReconstruction loss: 3.02283 \tLatent loss: 0.00381312\n",
      "2355 Train total loss: 2.80037 \tReconstruction loss: 2.79659 \tLatent loss: 0.00378064\n",
      "2356 Train total loss: 2.78931 \tReconstruction loss: 2.78559 \tLatent loss: 0.00371538\n",
      "2357 Train total loss: 2.81172 \tReconstruction loss: 2.8081 \tLatent loss: 0.00361499\n",
      "2358 Train total loss: 2.7799 \tReconstruction loss: 2.77637 \tLatent loss: 0.00352793\n",
      "2359 Train total loss: 2.80836 \tReconstruction loss: 2.80513 \tLatent loss: 0.00322808\n",
      "2360 Train total loss: 3.0437 \tReconstruction loss: 3.04026 \tLatent loss: 0.0034437\n",
      "2361 Train total loss: 2.80999 \tReconstruction loss: 2.80703 \tLatent loss: 0.0029593\n",
      "2362 Train total loss: 2.84163 \tReconstruction loss: 2.83829 \tLatent loss: 0.00333932\n",
      "2363 Train total loss: 2.87454 \tReconstruction loss: 2.87167 \tLatent loss: 0.00287233\n",
      "2364 Train total loss: 3.08957 \tReconstruction loss: 3.08678 \tLatent loss: 0.00279146\n",
      "2365 Train total loss: 2.91771 \tReconstruction loss: 2.915 \tLatent loss: 0.00270797\n",
      "2366 Train total loss: 2.81372 \tReconstruction loss: 2.81116 \tLatent loss: 0.00256332\n",
      "2367 Train total loss: 2.93163 \tReconstruction loss: 2.92914 \tLatent loss: 0.0024926\n",
      "2368 Train total loss: 2.85808 \tReconstruction loss: 2.85574 \tLatent loss: 0.00234796\n",
      "2369 Train total loss: 2.86283 \tReconstruction loss: 2.86061 \tLatent loss: 0.00222105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370 Train total loss: 2.98904 \tReconstruction loss: 2.98692 \tLatent loss: 0.00211756\n",
      "2371 Train total loss: 2.95109 \tReconstruction loss: 2.94907 \tLatent loss: 0.00201735\n",
      "2372 Train total loss: 2.93012 \tReconstruction loss: 2.92806 \tLatent loss: 0.0020608\n",
      "2373 Train total loss: 2.97989 \tReconstruction loss: 2.97801 \tLatent loss: 0.00187689\n",
      "2374 Train total loss: 3.02939 \tReconstruction loss: 3.02768 \tLatent loss: 0.00170628\n",
      "2375 Train total loss: 2.79859 \tReconstruction loss: 2.79692 \tLatent loss: 0.00166967\n",
      "2376 Train total loss: 2.77859 \tReconstruction loss: 2.77687 \tLatent loss: 0.00171465\n",
      "2377 Train total loss: 2.80046 \tReconstruction loss: 2.799 \tLatent loss: 0.001459\n",
      "2378 Train total loss: 2.77714 \tReconstruction loss: 2.77574 \tLatent loss: 0.00140088\n",
      "2379 Train total loss: 2.80799 \tReconstruction loss: 2.80653 \tLatent loss: 0.00145631\n",
      "2380 Train total loss: 3.04155 \tReconstruction loss: 3.03999 \tLatent loss: 0.00155861\n",
      "2381 Train total loss: 2.80997 \tReconstruction loss: 2.80862 \tLatent loss: 0.00134518\n",
      "2382 Train total loss: 2.83893 \tReconstruction loss: 2.83776 \tLatent loss: 0.00116574\n",
      "2383 Train total loss: 2.86937 \tReconstruction loss: 2.86829 \tLatent loss: 0.00107995\n",
      "2384 Train total loss: 3.09408 \tReconstruction loss: 3.09311 \tLatent loss: 0.000974195\n",
      "2385 Train total loss: 2.9144 \tReconstruction loss: 2.91345 \tLatent loss: 0.00095306\n",
      "2386 Train total loss: 2.81097 \tReconstruction loss: 2.81006 \tLatent loss: 0.000909241\n",
      "2387 Train total loss: 2.92725 \tReconstruction loss: 2.92638 \tLatent loss: 0.000869472\n",
      "2388 Train total loss: 2.85531 \tReconstruction loss: 2.85454 \tLatent loss: 0.000772669\n",
      "2389 Train total loss: 2.8613 \tReconstruction loss: 2.86055 \tLatent loss: 0.000755121\n",
      "2390 Train total loss: 2.98816 \tReconstruction loss: 2.98749 \tLatent loss: 0.000671232\n",
      "2391 Train total loss: 2.94819 \tReconstruction loss: 2.94754 \tLatent loss: 0.000646848\n",
      "2392 Train total loss: 2.93132 \tReconstruction loss: 2.93072 \tLatent loss: 0.000597806\n",
      "2393 Train total loss: 2.97725 \tReconstruction loss: 2.97673 \tLatent loss: 0.00052546\n",
      "2394 Train total loss: 3.02298 \tReconstruction loss: 3.02248 \tLatent loss: 0.000499194\n",
      "2395 Train total loss: 2.79536 \tReconstruction loss: 2.79491 \tLatent loss: 0.000456398\n",
      "2396 Train total loss: 2.77621 \tReconstruction loss: 2.77579 \tLatent loss: 0.0004177\n",
      "2397 Train total loss: 2.79901 \tReconstruction loss: 2.79857 \tLatent loss: 0.000435971\n",
      "2398 Train total loss: 2.77565 \tReconstruction loss: 2.77526 \tLatent loss: 0.00038592\n",
      "2399 Train total loss: 2.807 \tReconstruction loss: 2.80664 \tLatent loss: 0.000358821\n",
      "2400 Train total loss: 3.0391 \tReconstruction loss: 3.03823 \tLatent loss: 0.00086726\n",
      "2401 Train total loss: 2.8134 \tReconstruction loss: 2.81308 \tLatent loss: 0.000319719\n",
      "2402 Train total loss: 2.83793 \tReconstruction loss: 2.83763 \tLatent loss: 0.000294218\n",
      "2403 Train total loss: 2.86701 \tReconstruction loss: 2.86674 \tLatent loss: 0.000269214\n",
      "2404 Train total loss: 3.09153 \tReconstruction loss: 3.09128 \tLatent loss: 0.000250642\n",
      "2405 Train total loss: 2.91388 \tReconstruction loss: 2.9136 \tLatent loss: 0.000279739\n",
      "2406 Train total loss: 2.8117 \tReconstruction loss: 2.81148 \tLatent loss: 0.00022306\n",
      "2407 Train total loss: 2.927 \tReconstruction loss: 2.92677 \tLatent loss: 0.00022402\n",
      "2408 Train total loss: 2.85556 \tReconstruction loss: 2.85534 \tLatent loss: 0.000220935\n",
      "2409 Train total loss: 2.8595 \tReconstruction loss: 2.85931 \tLatent loss: 0.000188611\n",
      "2410 Train total loss: 2.98765 \tReconstruction loss: 2.98746 \tLatent loss: 0.000190446\n",
      "2411 Train total loss: 2.9479 \tReconstruction loss: 2.94775 \tLatent loss: 0.000157373\n",
      "2412 Train total loss: 2.92866 \tReconstruction loss: 2.92851 \tLatent loss: 0.000150315\n",
      "2413 Train total loss: 2.97655 \tReconstruction loss: 2.97641 \tLatent loss: 0.000132374\n",
      "2414 Train total loss: 3.03075 \tReconstruction loss: 3.03062 \tLatent loss: 0.000130442\n",
      "2415 Train total loss: 2.79548 \tReconstruction loss: 2.79537 \tLatent loss: 0.000111446\n",
      "2416 Train total loss: 2.77509 \tReconstruction loss: 2.77499 \tLatent loss: 0.000103224\n",
      "2417 Train total loss: 2.80011 \tReconstruction loss: 2.80002 \tLatent loss: 8.95067e-05\n",
      "2418 Train total loss: 2.77561 \tReconstruction loss: 2.77551 \tLatent loss: 9.57004e-05\n",
      "2419 Train total loss: 2.80625 \tReconstruction loss: 2.80617 \tLatent loss: 8.11571e-05\n",
      "2420 Train total loss: 3.04094 \tReconstruction loss: 3.04054 \tLatent loss: 0.000399098\n",
      "2421 Train total loss: 2.80724 \tReconstruction loss: 2.80718 \tLatent loss: 6.36913e-05\n",
      "2422 Train total loss: 2.83757 \tReconstruction loss: 2.83751 \tLatent loss: 6.14303e-05\n",
      "2423 Train total loss: 2.8664 \tReconstruction loss: 2.86635 \tLatent loss: 5.65671e-05\n",
      "2424 Train total loss: 3.09042 \tReconstruction loss: 3.09036 \tLatent loss: 5.75797e-05\n",
      "2425 Train total loss: 2.91561 \tReconstruction loss: 2.91556 \tLatent loss: 5.43106e-05\n",
      "2426 Train total loss: 2.81011 \tReconstruction loss: 2.81006 \tLatent loss: 5.73062e-05\n",
      "2427 Train total loss: 2.9263 \tReconstruction loss: 2.92625 \tLatent loss: 5.30449e-05\n",
      "2428 Train total loss: 2.85715 \tReconstruction loss: 2.8571 \tLatent loss: 4.90348e-05\n",
      "2429 Train total loss: 2.85998 \tReconstruction loss: 2.85993 \tLatent loss: 5.58921e-05\n",
      "2430 Train total loss: 2.98613 \tReconstruction loss: 2.98609 \tLatent loss: 4.32349e-05\n",
      "2431 Train total loss: 2.94627 \tReconstruction loss: 2.94623 \tLatent loss: 3.56603e-05\n",
      "2432 Train total loss: 2.92923 \tReconstruction loss: 2.9292 \tLatent loss: 3.89084e-05\n",
      "2433 Train total loss: 2.97696 \tReconstruction loss: 2.97692 \tLatent loss: 3.68173e-05\n",
      "2434 Train total loss: 3.02171 \tReconstruction loss: 3.02169 \tLatent loss: 2.69071e-05\n",
      "2435 Train total loss: 2.79486 \tReconstruction loss: 2.79483 \tLatent loss: 2.65893e-05\n",
      "2436 Train total loss: 2.77398 \tReconstruction loss: 2.77396 \tLatent loss: 2.48684e-05\n",
      "2437 Train total loss: 2.79796 \tReconstruction loss: 2.79793 \tLatent loss: 2.27617e-05\n",
      "2438 Train total loss: 2.77448 \tReconstruction loss: 2.77446 \tLatent loss: 1.96884e-05\n",
      "2439 Train total loss: 2.80966 \tReconstruction loss: 2.80964 \tLatent loss: 1.94424e-05\n",
      "2440 Train total loss: 3.03999 \tReconstruction loss: 3.03964 \tLatent loss: 0.000346531\n",
      "2441 Train total loss: 2.80611 \tReconstruction loss: 2.80609 \tLatent loss: 1.63371e-05\n",
      "2442 Train total loss: 2.8375 \tReconstruction loss: 2.83747 \tLatent loss: 3.16981e-05\n",
      "2443 Train total loss: 2.86848 \tReconstruction loss: 2.86847 \tLatent loss: 1.63904e-05\n",
      "2444 Train total loss: 3.09125 \tReconstruction loss: 3.09122 \tLatent loss: 2.14075e-05\n",
      "2445 Train total loss: 2.91326 \tReconstruction loss: 2.91324 \tLatent loss: 1.9446e-05\n",
      "2446 Train total loss: 2.80947 \tReconstruction loss: 2.80945 \tLatent loss: 2.09193e-05\n",
      "2447 Train total loss: 2.92574 \tReconstruction loss: 2.92571 \tLatent loss: 2.43353e-05\n",
      "2448 Train total loss: 2.85465 \tReconstruction loss: 2.85463 \tLatent loss: 1.94003e-05\n",
      "2449 Train total loss: 2.85921 \tReconstruction loss: 2.85919 \tLatent loss: 1.86832e-05\n",
      "2450 Train total loss: 2.98626 \tReconstruction loss: 2.98625 \tLatent loss: 1.39203e-05\n",
      "2451 Train total loss: 2.94696 \tReconstruction loss: 2.94695 \tLatent loss: 1.20999e-05\n",
      "2452 Train total loss: 2.92863 \tReconstruction loss: 2.92861 \tLatent loss: 1.15286e-05\n",
      "2453 Train total loss: 2.9761 \tReconstruction loss: 2.97609 \tLatent loss: 8.30315e-06\n",
      "2454 Train total loss: 3.02151 \tReconstruction loss: 3.0215 \tLatent loss: 7.93686e-06\n",
      "2455 Train total loss: 2.79384 \tReconstruction loss: 2.79383 \tLatent loss: 7.84374e-06\n",
      "2456 Train total loss: 2.77487 \tReconstruction loss: 2.77486 \tLatent loss: 8.69989e-06\n",
      "2457 Train total loss: 2.798 \tReconstruction loss: 2.79799 \tLatent loss: 7.60996e-06\n",
      "2458 Train total loss: 2.77467 \tReconstruction loss: 2.77466 \tLatent loss: 8.34813e-06\n",
      "2459 Train total loss: 2.80577 \tReconstruction loss: 2.80576 \tLatent loss: 8.59775e-06\n",
      "2460 Train total loss: 3.03996 \tReconstruction loss: 3.03962 \tLatent loss: 0.000338774\n",
      "2461 Train total loss: 2.80698 \tReconstruction loss: 2.80697 \tLatent loss: 6.23786e-06\n",
      "2462 Train total loss: 2.83755 \tReconstruction loss: 2.83754 \tLatent loss: 8.1724e-06\n",
      "2463 Train total loss: 2.86655 \tReconstruction loss: 2.86654 \tLatent loss: 5.46628e-06\n",
      "2464 Train total loss: 3.09176 \tReconstruction loss: 3.09176 \tLatent loss: 7.42386e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2465 Train total loss: 2.91487 \tReconstruction loss: 2.91486 \tLatent loss: 8.9308e-06\n",
      "2466 Train total loss: 2.80935 \tReconstruction loss: 2.80934 \tLatent loss: 1.04861e-05\n",
      "2467 Train total loss: 2.92551 \tReconstruction loss: 2.92551 \tLatent loss: 8.08997e-06\n",
      "2468 Train total loss: 2.85589 \tReconstruction loss: 2.85588 \tLatent loss: 8.03877e-06\n",
      "2469 Train total loss: 2.8591 \tReconstruction loss: 2.85909 \tLatent loss: 5.64417e-06\n",
      "2470 Train total loss: 2.98637 \tReconstruction loss: 2.98636 \tLatent loss: 6.0081e-06\n",
      "2471 Train total loss: 2.94719 \tReconstruction loss: 2.94718 \tLatent loss: 7.19472e-06\n",
      "2472 Train total loss: 2.92897 \tReconstruction loss: 2.92896 \tLatent loss: 5.81892e-06\n",
      "2473 Train total loss: 2.97547 \tReconstruction loss: 2.97547 \tLatent loss: 3.3829e-06\n",
      "2474 Train total loss: 3.02021 \tReconstruction loss: 3.02021 \tLatent loss: 5.35531e-06\n",
      "2475 Train total loss: 2.79487 \tReconstruction loss: 2.79487 \tLatent loss: 3.65429e-06\n",
      "2476 Train total loss: 2.77407 \tReconstruction loss: 2.77407 \tLatent loss: 3.63927e-06\n",
      "2477 Train total loss: 2.79778 \tReconstruction loss: 2.79778 \tLatent loss: 2.24434e-06\n",
      "2478 Train total loss: 2.77482 \tReconstruction loss: 2.77482 \tLatent loss: 3.38451e-06\n",
      "2479 Train total loss: 2.80496 \tReconstruction loss: 2.80496 \tLatent loss: 4.65472e-06\n",
      "2480 Train total loss: 3.03976 \tReconstruction loss: 3.03943 \tLatent loss: 0.000332634\n",
      "2481 Train total loss: 2.80579 \tReconstruction loss: 2.80579 \tLatent loss: 3.87112e-06\n",
      "2482 Train total loss: 2.83729 \tReconstruction loss: 2.83729 \tLatent loss: 1.6353e-06\n",
      "2483 Train total loss: 2.86624 \tReconstruction loss: 2.86624 \tLatent loss: 5.45465e-06\n",
      "2484 Train total loss: 3.09165 \tReconstruction loss: 3.09165 \tLatent loss: 3.10984e-06\n",
      "2485 Train total loss: 2.91525 \tReconstruction loss: 2.91525 \tLatent loss: 5.56384e-06\n",
      "2486 Train total loss: 2.80938 \tReconstruction loss: 2.80937 \tLatent loss: 5.45646e-06\n",
      "2487 Train total loss: 2.92563 \tReconstruction loss: 2.92562 \tLatent loss: 1.01574e-05\n",
      "2488 Train total loss: 2.85467 \tReconstruction loss: 2.85467 \tLatent loss: 7.84357e-06\n",
      "2489 Train total loss: 2.85875 \tReconstruction loss: 2.85875 \tLatent loss: 7.57749e-06\n",
      "2490 Train total loss: 2.98649 \tReconstruction loss: 2.98649 \tLatent loss: 7.02092e-06\n",
      "2491 Train total loss: 2.94672 \tReconstruction loss: 2.94672 \tLatent loss: 4.34217e-06\n",
      "2492 Train total loss: 2.9286 \tReconstruction loss: 2.9286 \tLatent loss: 4.88814e-06\n",
      "2493 Train total loss: 2.9763 \tReconstruction loss: 2.9763 \tLatent loss: 3.10588e-06\n",
      "2494 Train total loss: 3.02464 \tReconstruction loss: 3.02464 \tLatent loss: 1.82492e-06\n",
      "2495 Train total loss: 2.7939 \tReconstruction loss: 2.7939 \tLatent loss: 2.0524e-06\n",
      "2496 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 3.54657e-06\n",
      "2497 Train total loss: 2.79725 \tReconstruction loss: 2.79725 \tLatent loss: 2.88239e-06\n",
      "2498 Train total loss: 2.77459 \tReconstruction loss: 2.77459 \tLatent loss: 2.02077e-06\n",
      "2499 Train total loss: 2.80569 \tReconstruction loss: 2.80569 \tLatent loss: 3.01187e-06\n",
      "2500 Train total loss: 3.04007 \tReconstruction loss: 3.03974 \tLatent loss: 0.000332461\n",
      "2501 Train total loss: 2.80639 \tReconstruction loss: 2.80639 \tLatent loss: 2.9808e-06\n",
      "2502 Train total loss: 2.83723 \tReconstruction loss: 2.83722 \tLatent loss: 2.50866e-06\n",
      "2503 Train total loss: 2.86586 \tReconstruction loss: 2.86586 \tLatent loss: 2.01505e-06\n",
      "2504 Train total loss: 3.09118 \tReconstruction loss: 3.09117 \tLatent loss: 3.28662e-06\n",
      "2505 Train total loss: 2.91425 \tReconstruction loss: 2.91424 \tLatent loss: 6.11517e-06\n",
      "2506 Train total loss: 2.81011 \tReconstruction loss: 2.8101 \tLatent loss: 7.32508e-06\n",
      "2507 Train total loss: 2.92511 \tReconstruction loss: 2.9251 \tLatent loss: 9.65055e-06\n",
      "2508 Train total loss: 2.8546 \tReconstruction loss: 2.8546 \tLatent loss: 5.40244e-06\n",
      "2509 Train total loss: 2.85938 \tReconstruction loss: 2.85938 \tLatent loss: 6.58408e-06\n",
      "2510 Train total loss: 2.98636 \tReconstruction loss: 2.98635 \tLatent loss: 7.70699e-06\n",
      "2511 Train total loss: 2.94652 \tReconstruction loss: 2.94651 \tLatent loss: 1.03421e-05\n",
      "2512 Train total loss: 2.92825 \tReconstruction loss: 2.92824 \tLatent loss: 2.65918e-06\n",
      "2513 Train total loss: 2.97529 \tReconstruction loss: 2.97529 \tLatent loss: 3.62e-06\n",
      "2514 Train total loss: 3.02228 \tReconstruction loss: 3.02228 \tLatent loss: 1.71117e-06\n",
      "2515 Train total loss: 2.794 \tReconstruction loss: 2.794 \tLatent loss: 2.53214e-06\n",
      "2516 Train total loss: 2.77387 \tReconstruction loss: 2.77387 \tLatent loss: 2.0359e-06\n",
      "2517 Train total loss: 2.79736 \tReconstruction loss: 2.79735 \tLatent loss: 2.32767e-06\n",
      "2518 Train total loss: 2.77588 \tReconstruction loss: 2.77588 \tLatent loss: 3.23409e-06\n",
      "2519 Train total loss: 2.8058 \tReconstruction loss: 2.80579 \tLatent loss: 2.65392e-06\n",
      "2520 Train total loss: 3.03998 \tReconstruction loss: 3.03964 \tLatent loss: 0.000332349\n",
      "2521 Train total loss: 2.80573 \tReconstruction loss: 2.80573 \tLatent loss: 2.46968e-06\n",
      "2522 Train total loss: 2.83713 \tReconstruction loss: 2.83712 \tLatent loss: 3.28146e-06\n",
      "2523 Train total loss: 2.86589 \tReconstruction loss: 2.86589 \tLatent loss: 1.74531e-06\n",
      "2524 Train total loss: 3.09122 \tReconstruction loss: 3.09122 \tLatent loss: 2.53711e-06\n",
      "2525 Train total loss: 2.91512 \tReconstruction loss: 2.91511 \tLatent loss: 2.85009e-06\n",
      "2526 Train total loss: 2.80943 \tReconstruction loss: 2.80942 \tLatent loss: 5.55515e-06\n",
      "2527 Train total loss: 2.92501 \tReconstruction loss: 2.925 \tLatent loss: 8.74831e-06\n",
      "2528 Train total loss: 2.85425 \tReconstruction loss: 2.85425 \tLatent loss: 4.90413e-06\n",
      "2529 Train total loss: 2.85875 \tReconstruction loss: 2.85874 \tLatent loss: 6.68939e-06\n",
      "2530 Train total loss: 2.98641 \tReconstruction loss: 2.9864 \tLatent loss: 7.90477e-06\n",
      "2531 Train total loss: 2.94745 \tReconstruction loss: 2.94744 \tLatent loss: 7.63704e-06\n",
      "2532 Train total loss: 2.92847 \tReconstruction loss: 2.92846 \tLatent loss: 5.22205e-06\n",
      "2533 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 1.57283e-06\n",
      "2534 Train total loss: 3.02149 \tReconstruction loss: 3.02149 \tLatent loss: 1.09467e-06\n",
      "2535 Train total loss: 2.79384 \tReconstruction loss: 2.79384 \tLatent loss: 1.80629e-06\n",
      "2536 Train total loss: 2.77375 \tReconstruction loss: 2.77375 \tLatent loss: 1.57561e-06\n",
      "2537 Train total loss: 2.79781 \tReconstruction loss: 2.79781 \tLatent loss: 2.56686e-06\n",
      "2538 Train total loss: 2.77441 \tReconstruction loss: 2.77441 \tLatent loss: 2.61863e-06\n",
      "2539 Train total loss: 2.80537 \tReconstruction loss: 2.80537 \tLatent loss: 3.07849e-06\n",
      "2540 Train total loss: 3.04099 \tReconstruction loss: 3.04065 \tLatent loss: 0.000332282\n",
      "2541 Train total loss: 2.80632 \tReconstruction loss: 2.80631 \tLatent loss: 5.4286e-06\n",
      "2542 Train total loss: 2.83713 \tReconstruction loss: 2.83712 \tLatent loss: 4.24154e-06\n",
      "2543 Train total loss: 2.86616 \tReconstruction loss: 2.86616 \tLatent loss: 3.62557e-06\n",
      "2544 Train total loss: 3.09101 \tReconstruction loss: 3.09101 \tLatent loss: 2.71841e-06\n",
      "2545 Train total loss: 2.91466 \tReconstruction loss: 2.91465 \tLatent loss: 2.82379e-06\n",
      "2546 Train total loss: 2.8096 \tReconstruction loss: 2.80959 \tLatent loss: 3.43925e-06\n",
      "2547 Train total loss: 2.92504 \tReconstruction loss: 2.92503 \tLatent loss: 5.80407e-06\n",
      "2548 Train total loss: 2.85384 \tReconstruction loss: 2.85383 \tLatent loss: 8.91366e-06\n",
      "2549 Train total loss: 2.85885 \tReconstruction loss: 2.85884 \tLatent loss: 8.86341e-06\n",
      "2550 Train total loss: 2.98597 \tReconstruction loss: 2.98596 \tLatent loss: 1.11058e-05\n",
      "2551 Train total loss: 2.9466 \tReconstruction loss: 2.94659 \tLatent loss: 1.09032e-05\n",
      "2552 Train total loss: 2.92858 \tReconstruction loss: 2.92858 \tLatent loss: 7.04442e-06\n",
      "2553 Train total loss: 2.97535 \tReconstruction loss: 2.97534 \tLatent loss: 3.46639e-06\n",
      "2554 Train total loss: 3.02259 \tReconstruction loss: 3.02259 \tLatent loss: 4.0854e-06\n",
      "2555 Train total loss: 2.79373 \tReconstruction loss: 2.79372 \tLatent loss: 3.69258e-06\n",
      "2556 Train total loss: 2.77365 \tReconstruction loss: 2.77365 \tLatent loss: 2.28381e-06\n",
      "2557 Train total loss: 2.79734 \tReconstruction loss: 2.79734 \tLatent loss: 2.00575e-06\n",
      "2558 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.39745e-06\n",
      "2559 Train total loss: 2.80538 \tReconstruction loss: 2.80538 \tLatent loss: 3.73004e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560 Train total loss: 3.03993 \tReconstruction loss: 3.0396 \tLatent loss: 0.000329978\n",
      "2561 Train total loss: 2.80623 \tReconstruction loss: 2.80622 \tLatent loss: 8.64827e-06\n",
      "2562 Train total loss: 2.83706 \tReconstruction loss: 2.83705 \tLatent loss: 7.49637e-06\n",
      "2563 Train total loss: 2.86591 \tReconstruction loss: 2.8659 \tLatent loss: 5.03945e-06\n",
      "2564 Train total loss: 3.09061 \tReconstruction loss: 3.09061 \tLatent loss: 2.70602e-06\n",
      "2565 Train total loss: 2.91469 \tReconstruction loss: 2.91469 \tLatent loss: 2.47488e-06\n",
      "2566 Train total loss: 2.80944 \tReconstruction loss: 2.80943 \tLatent loss: 5.10111e-06\n",
      "2567 Train total loss: 2.92618 \tReconstruction loss: 2.92618 \tLatent loss: 5.95272e-06\n",
      "2568 Train total loss: 2.85413 \tReconstruction loss: 2.85412 \tLatent loss: 6.10381e-06\n",
      "2569 Train total loss: 2.85886 \tReconstruction loss: 2.85885 \tLatent loss: 1.40313e-05\n",
      "2570 Train total loss: 2.98611 \tReconstruction loss: 2.9861 \tLatent loss: 8.43684e-06\n",
      "2571 Train total loss: 2.94658 \tReconstruction loss: 2.94657 \tLatent loss: 1.07646e-05\n",
      "2572 Train total loss: 2.92862 \tReconstruction loss: 2.92861 \tLatent loss: 1.45452e-05\n",
      "2573 Train total loss: 2.97537 \tReconstruction loss: 2.97536 \tLatent loss: 9.85986e-06\n",
      "2574 Train total loss: 3.02156 \tReconstruction loss: 3.02155 \tLatent loss: 5.2538e-06\n",
      "2575 Train total loss: 2.79371 \tReconstruction loss: 2.7937 \tLatent loss: 3.72385e-06\n",
      "2576 Train total loss: 2.77366 \tReconstruction loss: 2.77366 \tLatent loss: 3.83161e-06\n",
      "2577 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.09477e-06\n",
      "2578 Train total loss: 2.77453 \tReconstruction loss: 2.77453 \tLatent loss: 3.22084e-06\n",
      "2579 Train total loss: 2.80528 \tReconstruction loss: 2.80528 \tLatent loss: 3.24902e-06\n",
      "2580 Train total loss: 3.03972 \tReconstruction loss: 3.03939 \tLatent loss: 0.000329081\n",
      "2581 Train total loss: 2.80633 \tReconstruction loss: 2.80632 \tLatent loss: 1.07447e-05\n",
      "2582 Train total loss: 2.8371 \tReconstruction loss: 2.83709 \tLatent loss: 7.63259e-06\n",
      "2583 Train total loss: 2.86604 \tReconstruction loss: 2.86604 \tLatent loss: 8.15528e-06\n",
      "2584 Train total loss: 3.09088 \tReconstruction loss: 3.09088 \tLatent loss: 6.30415e-06\n",
      "2585 Train total loss: 2.9151 \tReconstruction loss: 2.9151 \tLatent loss: 4.01768e-06\n",
      "2586 Train total loss: 2.80944 \tReconstruction loss: 2.80944 \tLatent loss: 5.88842e-06\n",
      "2587 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 4.61638e-06\n",
      "2588 Train total loss: 2.85413 \tReconstruction loss: 2.85412 \tLatent loss: 4.93099e-06\n",
      "2589 Train total loss: 2.85884 \tReconstruction loss: 2.85884 \tLatent loss: 5.20202e-06\n",
      "2590 Train total loss: 2.98593 \tReconstruction loss: 2.98592 \tLatent loss: 8.17231e-06\n",
      "2591 Train total loss: 2.94653 \tReconstruction loss: 2.94652 \tLatent loss: 1.21559e-05\n",
      "2592 Train total loss: 2.92855 \tReconstruction loss: 2.92854 \tLatent loss: 1.18436e-05\n",
      "2593 Train total loss: 2.97562 \tReconstruction loss: 2.97562 \tLatent loss: 5.90476e-06\n",
      "2594 Train total loss: 3.02159 \tReconstruction loss: 3.02159 \tLatent loss: 4.49237e-06\n",
      "2595 Train total loss: 2.79377 \tReconstruction loss: 2.79377 \tLatent loss: 3.03007e-06\n",
      "2596 Train total loss: 2.77371 \tReconstruction loss: 2.77371 \tLatent loss: 3.609e-06\n",
      "2597 Train total loss: 2.79762 \tReconstruction loss: 2.79761 \tLatent loss: 4.88465e-06\n",
      "2598 Train total loss: 2.77424 \tReconstruction loss: 2.77423 \tLatent loss: 3.42131e-06\n",
      "2599 Train total loss: 2.80511 \tReconstruction loss: 2.8051 \tLatent loss: 4.30871e-06\n",
      "2600 Train total loss: 3.04011 \tReconstruction loss: 3.03978 \tLatent loss: 0.000331771\n",
      "2601 Train total loss: 2.80586 \tReconstruction loss: 2.80585 \tLatent loss: 1.1463e-05\n",
      "2602 Train total loss: 2.83692 \tReconstruction loss: 2.83691 \tLatent loss: 7.48787e-06\n",
      "2603 Train total loss: 2.86586 \tReconstruction loss: 2.86586 \tLatent loss: 5.68853e-06\n",
      "2604 Train total loss: 3.09104 \tReconstruction loss: 3.09104 \tLatent loss: 5.7638e-06\n",
      "2605 Train total loss: 2.9147 \tReconstruction loss: 2.91469 \tLatent loss: 6.86256e-06\n",
      "2606 Train total loss: 2.80927 \tReconstruction loss: 2.80927 \tLatent loss: 4.97044e-06\n",
      "2607 Train total loss: 2.92571 \tReconstruction loss: 2.92571 \tLatent loss: 3.82762e-06\n",
      "2608 Train total loss: 2.85445 \tReconstruction loss: 2.85445 \tLatent loss: 6.21191e-06\n",
      "2609 Train total loss: 2.85868 \tReconstruction loss: 2.85867 \tLatent loss: 7.45992e-06\n",
      "2610 Train total loss: 2.98642 \tReconstruction loss: 2.98641 \tLatent loss: 7.90873e-06\n",
      "2611 Train total loss: 2.9468 \tReconstruction loss: 2.94679 \tLatent loss: 1.15662e-05\n",
      "2612 Train total loss: 2.92851 \tReconstruction loss: 2.9285 \tLatent loss: 9.77024e-06\n",
      "2613 Train total loss: 2.97596 \tReconstruction loss: 2.97595 \tLatent loss: 9.6227e-06\n",
      "2614 Train total loss: 3.0219 \tReconstruction loss: 3.02189 \tLatent loss: 5.19387e-06\n",
      "2615 Train total loss: 2.7937 \tReconstruction loss: 2.7937 \tLatent loss: 4.68442e-06\n",
      "2616 Train total loss: 2.77368 \tReconstruction loss: 2.77367 \tLatent loss: 5.46511e-06\n",
      "2617 Train total loss: 2.79735 \tReconstruction loss: 2.79734 \tLatent loss: 5.24763e-06\n",
      "2618 Train total loss: 2.774 \tReconstruction loss: 2.774 \tLatent loss: 3.86099e-06\n",
      "2619 Train total loss: 2.80554 \tReconstruction loss: 2.80553 \tLatent loss: 4.00801e-06\n",
      "2620 Train total loss: 3.04008 \tReconstruction loss: 3.03975 \tLatent loss: 0.000326706\n",
      "2621 Train total loss: 2.80586 \tReconstruction loss: 2.80585 \tLatent loss: 8.60441e-06\n",
      "2622 Train total loss: 2.83704 \tReconstruction loss: 2.83703 \tLatent loss: 8.68647e-06\n",
      "2623 Train total loss: 2.86593 \tReconstruction loss: 2.86592 \tLatent loss: 1.73807e-05\n",
      "2624 Train total loss: 3.09068 \tReconstruction loss: 3.09067 \tLatent loss: 8.83449e-06\n",
      "2625 Train total loss: 2.9142 \tReconstruction loss: 2.9142 \tLatent loss: 3.60392e-06\n",
      "2626 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 3.09836e-06\n",
      "2627 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.87334e-06\n",
      "2628 Train total loss: 2.85458 \tReconstruction loss: 2.85457 \tLatent loss: 4.68504e-06\n",
      "2629 Train total loss: 2.85858 \tReconstruction loss: 2.85858 \tLatent loss: 5.80168e-06\n",
      "2630 Train total loss: 2.98601 \tReconstruction loss: 2.986 \tLatent loss: 6.06576e-06\n",
      "2631 Train total loss: 2.94677 \tReconstruction loss: 2.94676 \tLatent loss: 9.77046e-06\n",
      "2632 Train total loss: 2.9282 \tReconstruction loss: 2.92819 \tLatent loss: 8.94919e-06\n",
      "2633 Train total loss: 2.97577 \tReconstruction loss: 2.97576 \tLatent loss: 8.98169e-06\n",
      "2634 Train total loss: 3.02231 \tReconstruction loss: 3.0223 \tLatent loss: 8.23908e-06\n",
      "2635 Train total loss: 2.79369 \tReconstruction loss: 2.79369 \tLatent loss: 4.31974e-06\n",
      "2636 Train total loss: 2.77372 \tReconstruction loss: 2.77372 \tLatent loss: 3.62048e-06\n",
      "2637 Train total loss: 2.79724 \tReconstruction loss: 2.79723 \tLatent loss: 5.40856e-06\n",
      "2638 Train total loss: 2.77445 \tReconstruction loss: 2.77444 \tLatent loss: 6.09367e-06\n",
      "2639 Train total loss: 2.80509 \tReconstruction loss: 2.80508 \tLatent loss: 3.29651e-06\n",
      "2640 Train total loss: 3.04 \tReconstruction loss: 3.03968 \tLatent loss: 0.000323925\n",
      "2641 Train total loss: 2.8059 \tReconstruction loss: 2.80589 \tLatent loss: 1.03664e-05\n",
      "2642 Train total loss: 2.837 \tReconstruction loss: 2.83699 \tLatent loss: 1.07042e-05\n",
      "2643 Train total loss: 2.86594 \tReconstruction loss: 2.86593 \tLatent loss: 1.13171e-05\n",
      "2644 Train total loss: 3.09088 \tReconstruction loss: 3.09087 \tLatent loss: 1.0342e-05\n",
      "2645 Train total loss: 2.91494 \tReconstruction loss: 2.91493 \tLatent loss: 5.66404e-06\n",
      "2646 Train total loss: 2.80899 \tReconstruction loss: 2.80898 \tLatent loss: 2.55527e-06\n",
      "2647 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 3.26809e-06\n",
      "2648 Train total loss: 2.85405 \tReconstruction loss: 2.85405 \tLatent loss: 1.88742e-06\n",
      "2649 Train total loss: 2.85853 \tReconstruction loss: 2.85853 \tLatent loss: 2.87346e-06\n",
      "2650 Train total loss: 2.98617 \tReconstruction loss: 2.98616 \tLatent loss: 7.82766e-06\n",
      "2651 Train total loss: 2.94662 \tReconstruction loss: 2.94661 \tLatent loss: 7.09508e-06\n",
      "2652 Train total loss: 2.9289 \tReconstruction loss: 2.92889 \tLatent loss: 1.04629e-05\n",
      "2653 Train total loss: 2.97561 \tReconstruction loss: 2.9756 \tLatent loss: 8.70447e-06\n",
      "2654 Train total loss: 3.02132 \tReconstruction loss: 3.02131 \tLatent loss: 5.41127e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2655 Train total loss: 2.79381 \tReconstruction loss: 2.7938 \tLatent loss: 7.24308e-06\n",
      "2656 Train total loss: 2.7745 \tReconstruction loss: 2.77363 \tLatent loss: 0.000873521\n",
      "2657 Train total loss: 2.79733 \tReconstruction loss: 2.79724 \tLatent loss: 9.27839e-05\n",
      "2658 Train total loss: 2.77411 \tReconstruction loss: 2.7741 \tLatent loss: 1.46711e-05\n",
      "2659 Train total loss: 2.80523 \tReconstruction loss: 2.80523 \tLatent loss: 5.74286e-06\n",
      "2660 Train total loss: 3.03982 \tReconstruction loss: 3.03982 \tLatent loss: 3.53782e-06\n",
      "2661 Train total loss: 2.80608 \tReconstruction loss: 2.80607 \tLatent loss: 4.44669e-06\n",
      "2662 Train total loss: 2.83707 \tReconstruction loss: 2.83707 \tLatent loss: 7.39131e-06\n",
      "2663 Train total loss: 2.86588 \tReconstruction loss: 2.86587 \tLatent loss: 8.68997e-06\n",
      "2664 Train total loss: 3.09092 \tReconstruction loss: 3.09091 \tLatent loss: 9.47617e-06\n",
      "2665 Train total loss: 2.91465 \tReconstruction loss: 2.91464 \tLatent loss: 6.40251e-06\n",
      "2666 Train total loss: 2.80927 \tReconstruction loss: 2.80927 \tLatent loss: 1.95617e-06\n",
      "2667 Train total loss: 2.92529 \tReconstruction loss: 2.92529 \tLatent loss: 1.3783e-06\n",
      "2668 Train total loss: 2.85407 \tReconstruction loss: 2.85407 \tLatent loss: 1.72219e-06\n",
      "2669 Train total loss: 2.85868 \tReconstruction loss: 2.85868 \tLatent loss: 2.289e-06\n",
      "2670 Train total loss: 2.98596 \tReconstruction loss: 2.98596 \tLatent loss: 1.49745e-06\n",
      "2671 Train total loss: 2.9461 \tReconstruction loss: 2.9461 \tLatent loss: 2.63707e-06\n",
      "2672 Train total loss: 2.92828 \tReconstruction loss: 2.92827 \tLatent loss: 7.01597e-06\n",
      "2673 Train total loss: 2.97597 \tReconstruction loss: 2.97596 \tLatent loss: 4.2638e-06\n",
      "2674 Train total loss: 3.02168 \tReconstruction loss: 3.02167 \tLatent loss: 4.04521e-06\n",
      "2675 Train total loss: 2.79372 \tReconstruction loss: 2.79372 \tLatent loss: 3.50128e-06\n",
      "2676 Train total loss: 2.77377 \tReconstruction loss: 2.77376 \tLatent loss: 5.43693e-06\n",
      "2677 Train total loss: 2.79732 \tReconstruction loss: 2.79731 \tLatent loss: 5.25878e-06\n",
      "2678 Train total loss: 2.77418 \tReconstruction loss: 2.77417 \tLatent loss: 3.07423e-06\n",
      "2679 Train total loss: 2.80516 \tReconstruction loss: 2.80516 \tLatent loss: 2.72116e-06\n",
      "2680 Train total loss: 3.03953 \tReconstruction loss: 3.03953 \tLatent loss: 6.96498e-07\n",
      "2681 Train total loss: 2.80576 \tReconstruction loss: 2.80576 \tLatent loss: 3.45472e-06\n",
      "2682 Train total loss: 2.83703 \tReconstruction loss: 2.83702 \tLatent loss: 7.02492e-06\n",
      "2683 Train total loss: 2.86588 \tReconstruction loss: 2.86587 \tLatent loss: 3.18412e-06\n",
      "2684 Train total loss: 3.09108 \tReconstruction loss: 3.09108 \tLatent loss: 5.87468e-06\n",
      "2685 Train total loss: 2.91495 \tReconstruction loss: 2.91495 \tLatent loss: 3.94333e-06\n",
      "2686 Train total loss: 2.80918 \tReconstruction loss: 2.80917 \tLatent loss: 1.99156e-06\n",
      "2687 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.25934e-06\n",
      "2688 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.58913e-06\n",
      "2689 Train total loss: 2.85866 \tReconstruction loss: 2.85866 \tLatent loss: 2.21448e-06\n",
      "2690 Train total loss: 2.98607 \tReconstruction loss: 2.98607 \tLatent loss: 2.99447e-06\n",
      "2691 Train total loss: 2.9466 \tReconstruction loss: 2.94659 \tLatent loss: 2.54747e-06\n",
      "2692 Train total loss: 2.92843 \tReconstruction loss: 2.92842 \tLatent loss: 4.58114e-06\n",
      "2693 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 2.90679e-06\n",
      "2694 Train total loss: 3.02201 \tReconstruction loss: 3.02201 \tLatent loss: 2.17658e-06\n",
      "2695 Train total loss: 2.79369 \tReconstruction loss: 2.79369 \tLatent loss: 1.40287e-06\n",
      "2696 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 4.36374e-06\n",
      "2697 Train total loss: 2.79724 \tReconstruction loss: 2.79723 \tLatent loss: 4.90831e-06\n",
      "2698 Train total loss: 2.77418 \tReconstruction loss: 2.77418 \tLatent loss: 7.03511e-06\n",
      "2699 Train total loss: 2.80509 \tReconstruction loss: 2.80509 \tLatent loss: 2.55035e-06\n",
      "2700 Train total loss: 3.03964 \tReconstruction loss: 3.03963 \tLatent loss: 1.66976e-06\n",
      "2701 Train total loss: 2.80588 \tReconstruction loss: 2.80588 \tLatent loss: 2.24536e-06\n",
      "2702 Train total loss: 2.83705 \tReconstruction loss: 2.83704 \tLatent loss: 3.90452e-06\n",
      "2703 Train total loss: 2.86599 \tReconstruction loss: 2.86598 \tLatent loss: 8.14545e-06\n",
      "2704 Train total loss: 3.09084 \tReconstruction loss: 3.09084 \tLatent loss: 3.95136e-06\n",
      "2705 Train total loss: 2.91465 \tReconstruction loss: 2.91464 \tLatent loss: 2.74291e-06\n",
      "2706 Train total loss: 2.80933 \tReconstruction loss: 2.80933 \tLatent loss: 2.73826e-06\n",
      "2707 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.17402e-06\n",
      "2708 Train total loss: 2.85404 \tReconstruction loss: 2.85404 \tLatent loss: 1.27559e-06\n",
      "2709 Train total loss: 2.85859 \tReconstruction loss: 2.85859 \tLatent loss: 1.77355e-06\n",
      "2710 Train total loss: 2.98605 \tReconstruction loss: 2.98605 \tLatent loss: 1.48525e-06\n",
      "2711 Train total loss: 2.94639 \tReconstruction loss: 2.94639 \tLatent loss: 2.02127e-06\n",
      "2712 Train total loss: 2.92846 \tReconstruction loss: 2.92846 \tLatent loss: 2.40152e-06\n",
      "2713 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.57151e-06\n",
      "2714 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.23831e-06\n",
      "2715 Train total loss: 2.79371 \tReconstruction loss: 2.79371 \tLatent loss: 1.03185e-06\n",
      "2716 Train total loss: 2.77362 \tReconstruction loss: 2.77362 \tLatent loss: 3.8451e-06\n",
      "2717 Train total loss: 2.79723 \tReconstruction loss: 2.79722 \tLatent loss: 5.02373e-06\n",
      "2718 Train total loss: 2.7744 \tReconstruction loss: 2.77439 \tLatent loss: 7.32186e-06\n",
      "2719 Train total loss: 2.80513 \tReconstruction loss: 2.80513 \tLatent loss: 3.98041e-06\n",
      "2720 Train total loss: 3.03987 \tReconstruction loss: 3.03987 \tLatent loss: 1.92267e-06\n",
      "2721 Train total loss: 2.80582 \tReconstruction loss: 2.80582 \tLatent loss: 1.23839e-06\n",
      "2722 Train total loss: 2.83696 \tReconstruction loss: 2.83695 \tLatent loss: 2.68778e-06\n",
      "2723 Train total loss: 2.86602 \tReconstruction loss: 2.86601 \tLatent loss: 5.23702e-06\n",
      "2724 Train total loss: 3.0908 \tReconstruction loss: 3.0908 \tLatent loss: 4.63378e-06\n",
      "2725 Train total loss: 2.91444 \tReconstruction loss: 2.91443 \tLatent loss: 3.08356e-06\n",
      "2726 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 1.5171e-06\n",
      "2727 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.54424e-06\n",
      "2728 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.22514e-06\n",
      "2729 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.88635e-06\n",
      "2730 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.34941e-06\n",
      "2731 Train total loss: 2.94667 \tReconstruction loss: 2.94667 \tLatent loss: 2.73977e-06\n",
      "2732 Train total loss: 2.92841 \tReconstruction loss: 2.9284 \tLatent loss: 1.95391e-06\n",
      "2733 Train total loss: 2.97576 \tReconstruction loss: 2.97576 \tLatent loss: 1.55131e-06\n",
      "2734 Train total loss: 3.02206 \tReconstruction loss: 3.02206 \tLatent loss: 1.61643e-06\n",
      "2735 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 9.51468e-07\n",
      "2736 Train total loss: 2.77389 \tReconstruction loss: 2.77389 \tLatent loss: 2.6724e-06\n",
      "2737 Train total loss: 2.79718 \tReconstruction loss: 2.79718 \tLatent loss: 3.1134e-06\n",
      "2738 Train total loss: 2.77421 \tReconstruction loss: 2.7742 \tLatent loss: 9.97331e-06\n",
      "2739 Train total loss: 2.80518 \tReconstruction loss: 2.80518 \tLatent loss: 5.05251e-06\n",
      "2740 Train total loss: 3.03998 \tReconstruction loss: 3.03998 \tLatent loss: 1.74297e-06\n",
      "2741 Train total loss: 2.80596 \tReconstruction loss: 2.80595 \tLatent loss: 2.46018e-06\n",
      "2742 Train total loss: 2.83702 \tReconstruction loss: 2.83702 \tLatent loss: 3.23632e-06\n",
      "2743 Train total loss: 2.86589 \tReconstruction loss: 2.86589 \tLatent loss: 4.70355e-06\n",
      "2744 Train total loss: 3.09076 \tReconstruction loss: 3.09075 \tLatent loss: 5.29671e-06\n",
      "2745 Train total loss: 2.91473 \tReconstruction loss: 2.91473 \tLatent loss: 3.80218e-06\n",
      "2746 Train total loss: 2.80914 \tReconstruction loss: 2.80914 \tLatent loss: 5.7101e-07\n",
      "2747 Train total loss: 2.92494 \tReconstruction loss: 2.92494 \tLatent loss: 1.39835e-06\n",
      "2748 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 1.52365e-06\n",
      "2749 Train total loss: 2.85846 \tReconstruction loss: 2.85846 \tLatent loss: 1.70419e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2750 Train total loss: 2.98599 \tReconstruction loss: 2.98598 \tLatent loss: 2.62854e-06\n",
      "2751 Train total loss: 2.94669 \tReconstruction loss: 2.94668 \tLatent loss: 4.18819e-06\n",
      "2752 Train total loss: 2.92836 \tReconstruction loss: 2.92836 \tLatent loss: 2.64308e-06\n",
      "2753 Train total loss: 2.97576 \tReconstruction loss: 2.97576 \tLatent loss: 1.29303e-06\n",
      "2754 Train total loss: 3.0219 \tReconstruction loss: 3.0219 \tLatent loss: 1.95008e-06\n",
      "2755 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 1.46545e-06\n",
      "2756 Train total loss: 2.77369 \tReconstruction loss: 2.77369 \tLatent loss: 1.67213e-06\n",
      "2757 Train total loss: 2.7972 \tReconstruction loss: 2.7972 \tLatent loss: 4.12983e-06\n",
      "2758 Train total loss: 2.7742 \tReconstruction loss: 2.77419 \tLatent loss: 8.11054e-06\n",
      "2759 Train total loss: 2.80514 \tReconstruction loss: 2.80513 \tLatent loss: 9.10374e-06\n",
      "2760 Train total loss: 3.03991 \tReconstruction loss: 3.03991 \tLatent loss: 4.56756e-06\n",
      "2761 Train total loss: 2.80599 \tReconstruction loss: 2.80599 \tLatent loss: 2.01905e-06\n",
      "2762 Train total loss: 2.83698 \tReconstruction loss: 2.83697 \tLatent loss: 3.85832e-06\n",
      "2763 Train total loss: 2.86589 \tReconstruction loss: 2.86588 \tLatent loss: 5.66653e-06\n",
      "2764 Train total loss: 3.09096 \tReconstruction loss: 3.09096 \tLatent loss: 3.35912e-06\n",
      "2765 Train total loss: 2.91481 \tReconstruction loss: 2.9148 \tLatent loss: 3.36973e-06\n",
      "2766 Train total loss: 2.80921 \tReconstruction loss: 2.8092 \tLatent loss: 3.5394e-06\n",
      "2767 Train total loss: 2.92544 \tReconstruction loss: 2.92544 \tLatent loss: 1.74522e-06\n",
      "2768 Train total loss: 2.85417 \tReconstruction loss: 2.85417 \tLatent loss: 9.58767e-07\n",
      "2769 Train total loss: 2.85857 \tReconstruction loss: 2.85857 \tLatent loss: 1.52424e-06\n",
      "2770 Train total loss: 2.98596 \tReconstruction loss: 2.98596 \tLatent loss: 2.83581e-06\n",
      "2771 Train total loss: 2.94646 \tReconstruction loss: 2.94645 \tLatent loss: 3.21307e-06\n",
      "2772 Train total loss: 2.92842 \tReconstruction loss: 2.92842 \tLatent loss: 3.23018e-06\n",
      "2773 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 1.17168e-06\n",
      "2774 Train total loss: 3.02178 \tReconstruction loss: 3.02177 \tLatent loss: 3.59958e-06\n",
      "2775 Train total loss: 2.79383 \tReconstruction loss: 2.79382 \tLatent loss: 2.36734e-06\n",
      "2776 Train total loss: 2.7737 \tReconstruction loss: 2.7737 \tLatent loss: 1.0301e-06\n",
      "2777 Train total loss: 2.79722 \tReconstruction loss: 2.79722 \tLatent loss: 3.47592e-06\n",
      "2778 Train total loss: 2.77449 \tReconstruction loss: 2.77449 \tLatent loss: 4.58264e-06\n",
      "2779 Train total loss: 2.80519 \tReconstruction loss: 2.80518 \tLatent loss: 1.08616e-05\n",
      "2780 Train total loss: 3.03972 \tReconstruction loss: 3.03971 \tLatent loss: 5.05427e-06\n",
      "2781 Train total loss: 2.80586 \tReconstruction loss: 2.80585 \tLatent loss: 1.17599e-06\n",
      "2782 Train total loss: 2.83692 \tReconstruction loss: 2.83692 \tLatent loss: 3.18311e-06\n",
      "2783 Train total loss: 2.86607 \tReconstruction loss: 2.86606 \tLatent loss: 4.36229e-06\n",
      "2784 Train total loss: 3.0909 \tReconstruction loss: 3.0909 \tLatent loss: 8.03011e-06\n",
      "2785 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 5.61355e-06\n",
      "2786 Train total loss: 2.80899 \tReconstruction loss: 2.80899 \tLatent loss: 3.54642e-06\n",
      "2787 Train total loss: 2.92522 \tReconstruction loss: 2.92521 \tLatent loss: 5.70118e-07\n",
      "2788 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 6.92048e-07\n",
      "2789 Train total loss: 2.85865 \tReconstruction loss: 2.85865 \tLatent loss: 2.65857e-06\n",
      "2790 Train total loss: 2.9862 \tReconstruction loss: 2.98619 \tLatent loss: 4.08409e-06\n",
      "2791 Train total loss: 2.94654 \tReconstruction loss: 2.94654 \tLatent loss: 4.7532e-06\n",
      "2792 Train total loss: 2.92837 \tReconstruction loss: 2.92836 \tLatent loss: 2.4001e-06\n",
      "2793 Train total loss: 2.97572 \tReconstruction loss: 2.97572 \tLatent loss: 1.8745e-06\n",
      "2794 Train total loss: 3.02198 \tReconstruction loss: 3.02198 \tLatent loss: 3.04967e-06\n",
      "2795 Train total loss: 2.79371 \tReconstruction loss: 2.79371 \tLatent loss: 5.37509e-06\n",
      "2796 Train total loss: 2.77375 \tReconstruction loss: 2.77375 \tLatent loss: 1.61849e-06\n",
      "2797 Train total loss: 2.79713 \tReconstruction loss: 2.79713 \tLatent loss: 2.81373e-06\n",
      "2798 Train total loss: 2.77425 \tReconstruction loss: 2.77424 \tLatent loss: 5.7297e-06\n",
      "2799 Train total loss: 2.8052 \tReconstruction loss: 2.80519 \tLatent loss: 8.8503e-06\n",
      "2800 Train total loss: 3.03963 \tReconstruction loss: 3.03963 \tLatent loss: 4.71383e-06\n",
      "2801 Train total loss: 2.80583 \tReconstruction loss: 2.80583 \tLatent loss: 1.70369e-06\n",
      "2802 Train total loss: 2.83697 \tReconstruction loss: 2.83697 \tLatent loss: 1.3057e-06\n",
      "2803 Train total loss: 2.86605 \tReconstruction loss: 2.86604 \tLatent loss: 4.19357e-06\n",
      "2804 Train total loss: 3.09086 \tReconstruction loss: 3.09085 \tLatent loss: 6.42285e-06\n",
      "2805 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 7.31417e-06\n",
      "2806 Train total loss: 2.80915 \tReconstruction loss: 2.80914 \tLatent loss: 4.85033e-06\n",
      "2807 Train total loss: 2.92523 \tReconstruction loss: 2.92523 \tLatent loss: 1.31776e-06\n",
      "2808 Train total loss: 2.85436 \tReconstruction loss: 2.85436 \tLatent loss: 1.05977e-06\n",
      "2809 Train total loss: 2.85868 \tReconstruction loss: 2.85868 \tLatent loss: 1.95352e-06\n",
      "2810 Train total loss: 2.98608 \tReconstruction loss: 2.98608 \tLatent loss: 6.03152e-06\n",
      "2811 Train total loss: 2.94659 \tReconstruction loss: 2.94658 \tLatent loss: 7.82227e-06\n",
      "2812 Train total loss: 2.92845 \tReconstruction loss: 2.92845 \tLatent loss: 5.02261e-06\n",
      "2813 Train total loss: 2.97574 \tReconstruction loss: 2.97574 \tLatent loss: 1.321e-06\n",
      "2814 Train total loss: 3.02175 \tReconstruction loss: 3.02174 \tLatent loss: 3.92849e-06\n",
      "2815 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 7.23454e-06\n",
      "2816 Train total loss: 2.77372 \tReconstruction loss: 2.77372 \tLatent loss: 3.92179e-06\n",
      "2817 Train total loss: 2.79718 \tReconstruction loss: 2.79718 \tLatent loss: 9.5764e-07\n",
      "2818 Train total loss: 2.77446 \tReconstruction loss: 2.77445 \tLatent loss: 4.92375e-06\n",
      "2819 Train total loss: 2.80524 \tReconstruction loss: 2.80523 \tLatent loss: 1.08082e-05\n",
      "2820 Train total loss: 3.0397 \tReconstruction loss: 3.03969 \tLatent loss: 6.37348e-06\n",
      "2821 Train total loss: 2.80588 \tReconstruction loss: 2.80588 \tLatent loss: 1.90641e-06\n",
      "2822 Train total loss: 2.83698 \tReconstruction loss: 2.83698 \tLatent loss: 9.91227e-07\n",
      "2823 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 2.33223e-06\n",
      "2824 Train total loss: 3.09073 \tReconstruction loss: 3.09072 \tLatent loss: 6.44111e-06\n",
      "2825 Train total loss: 2.91478 \tReconstruction loss: 2.91477 \tLatent loss: 5.5592e-06\n",
      "2826 Train total loss: 2.80905 \tReconstruction loss: 2.80904 \tLatent loss: 4.51087e-06\n",
      "2827 Train total loss: 2.92519 \tReconstruction loss: 2.92518 \tLatent loss: 3.89079e-06\n",
      "2828 Train total loss: 2.85434 \tReconstruction loss: 2.85434 \tLatent loss: 1.70825e-06\n",
      "2829 Train total loss: 2.85857 \tReconstruction loss: 2.85856 \tLatent loss: 1.55217e-06\n",
      "2830 Train total loss: 2.98602 \tReconstruction loss: 2.98602 \tLatent loss: 5.51457e-06\n",
      "2831 Train total loss: 2.94658 \tReconstruction loss: 2.94658 \tLatent loss: 6.85263e-06\n",
      "2832 Train total loss: 2.92848 \tReconstruction loss: 2.92848 \tLatent loss: 6.43354e-06\n",
      "2833 Train total loss: 2.97602 \tReconstruction loss: 2.97602 \tLatent loss: 2.26908e-06\n",
      "2834 Train total loss: 3.02167 \tReconstruction loss: 3.02167 \tLatent loss: 2.21845e-06\n",
      "2835 Train total loss: 2.79367 \tReconstruction loss: 2.79366 \tLatent loss: 5.61179e-06\n",
      "2836 Train total loss: 2.77372 \tReconstruction loss: 2.77371 \tLatent loss: 3.90355e-06\n",
      "2837 Train total loss: 2.79707 \tReconstruction loss: 2.79707 \tLatent loss: 1.01003e-06\n",
      "2838 Train total loss: 2.7744 \tReconstruction loss: 2.77439 \tLatent loss: 3.2744e-06\n",
      "2839 Train total loss: 2.80527 \tReconstruction loss: 2.80527 \tLatent loss: 5.60326e-06\n",
      "2840 Train total loss: 3.03968 \tReconstruction loss: 3.03967 \tLatent loss: 8.33118e-06\n",
      "2841 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.7159e-06\n",
      "2842 Train total loss: 2.837 \tReconstruction loss: 2.837 \tLatent loss: 1.04621e-06\n",
      "2843 Train total loss: 2.86599 \tReconstruction loss: 2.86598 \tLatent loss: 2.49372e-06\n",
      "2844 Train total loss: 3.09087 \tReconstruction loss: 3.09087 \tLatent loss: 5.72549e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845 Train total loss: 2.91476 \tReconstruction loss: 2.91475 \tLatent loss: 7.87093e-06\n",
      "2846 Train total loss: 2.80921 \tReconstruction loss: 2.8092 \tLatent loss: 6.42891e-06\n",
      "2847 Train total loss: 2.92515 \tReconstruction loss: 2.92514 \tLatent loss: 5.50317e-06\n",
      "2848 Train total loss: 2.85451 \tReconstruction loss: 2.8545 \tLatent loss: 3.00318e-06\n",
      "2849 Train total loss: 2.85862 \tReconstruction loss: 2.85862 \tLatent loss: 2.33407e-06\n",
      "2850 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 4.77104e-06\n",
      "2851 Train total loss: 2.94645 \tReconstruction loss: 2.94644 \tLatent loss: 9.21537e-06\n",
      "2852 Train total loss: 2.92845 \tReconstruction loss: 2.92844 \tLatent loss: 9.72548e-06\n",
      "2853 Train total loss: 2.97577 \tReconstruction loss: 2.97577 \tLatent loss: 2.37726e-06\n",
      "2854 Train total loss: 3.02175 \tReconstruction loss: 3.02175 \tLatent loss: 3.76122e-06\n",
      "2855 Train total loss: 2.79369 \tReconstruction loss: 2.79369 \tLatent loss: 5.17997e-06\n",
      "2856 Train total loss: 2.77373 \tReconstruction loss: 2.77372 \tLatent loss: 4.92879e-06\n",
      "2857 Train total loss: 2.79709 \tReconstruction loss: 2.79709 \tLatent loss: 1.24853e-06\n",
      "2858 Train total loss: 2.77431 \tReconstruction loss: 2.77431 \tLatent loss: 2.3284e-06\n",
      "2859 Train total loss: 2.80548 \tReconstruction loss: 2.80548 \tLatent loss: 5.2458e-06\n",
      "2860 Train total loss: 3.03984 \tReconstruction loss: 3.03983 \tLatent loss: 5.10539e-06\n",
      "2861 Train total loss: 2.80582 \tReconstruction loss: 2.80582 \tLatent loss: 2.38004e-06\n",
      "2862 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 2.06225e-06\n",
      "2863 Train total loss: 2.86592 \tReconstruction loss: 2.86592 \tLatent loss: 1.0453e-06\n",
      "2864 Train total loss: 3.09079 \tReconstruction loss: 3.09078 \tLatent loss: 1.89353e-06\n",
      "2865 Train total loss: 2.91454 \tReconstruction loss: 2.91453 \tLatent loss: 3.36446e-06\n",
      "2866 Train total loss: 2.80909 \tReconstruction loss: 2.80909 \tLatent loss: 4.37541e-06\n",
      "2867 Train total loss: 2.92516 \tReconstruction loss: 2.92515 \tLatent loss: 7.22473e-06\n",
      "2868 Train total loss: 2.85407 \tReconstruction loss: 2.85407 \tLatent loss: 3.43281e-06\n",
      "2869 Train total loss: 2.8586 \tReconstruction loss: 2.85859 \tLatent loss: 2.39939e-06\n",
      "2870 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 1.84547e-06\n",
      "2871 Train total loss: 2.94662 \tReconstruction loss: 2.94661 \tLatent loss: 5.48785e-06\n",
      "2872 Train total loss: 2.92848 \tReconstruction loss: 2.92847 \tLatent loss: 8.12453e-06\n",
      "2873 Train total loss: 2.97575 \tReconstruction loss: 2.97574 \tLatent loss: 5.45811e-06\n",
      "2874 Train total loss: 3.02158 \tReconstruction loss: 3.02158 \tLatent loss: 1.5268e-06\n",
      "2875 Train total loss: 2.79372 \tReconstruction loss: 2.79372 \tLatent loss: 2.33776e-06\n",
      "2876 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.99126e-06\n",
      "2877 Train total loss: 2.79721 \tReconstruction loss: 2.79721 \tLatent loss: 1.66165e-06\n",
      "2878 Train total loss: 2.77435 \tReconstruction loss: 2.77435 \tLatent loss: 2.00878e-06\n",
      "2879 Train total loss: 2.80516 \tReconstruction loss: 2.80515 \tLatent loss: 2.92544e-06\n",
      "2880 Train total loss: 3.03964 \tReconstruction loss: 3.03964 \tLatent loss: 1.98342e-06\n",
      "2881 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.21084e-06\n",
      "2882 Train total loss: 2.83691 \tReconstruction loss: 2.83691 \tLatent loss: 7.23841e-07\n",
      "2883 Train total loss: 2.866 \tReconstruction loss: 2.866 \tLatent loss: 2.0323e-06\n",
      "2884 Train total loss: 3.09087 \tReconstruction loss: 3.09087 \tLatent loss: 2.44653e-06\n",
      "2885 Train total loss: 2.91488 \tReconstruction loss: 2.91488 \tLatent loss: 1.28982e-06\n",
      "2886 Train total loss: 2.80914 \tReconstruction loss: 2.80914 \tLatent loss: 1.97217e-06\n",
      "2887 Train total loss: 2.92512 \tReconstruction loss: 2.92511 \tLatent loss: 6.07557e-06\n",
      "2888 Train total loss: 2.8542 \tReconstruction loss: 2.85419 \tLatent loss: 5.25646e-06\n",
      "2889 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.23668e-06\n",
      "2890 Train total loss: 2.98619 \tReconstruction loss: 2.98619 \tLatent loss: 1.17855e-06\n",
      "2891 Train total loss: 2.94656 \tReconstruction loss: 2.94655 \tLatent loss: 4.24237e-06\n",
      "2892 Train total loss: 2.92854 \tReconstruction loss: 2.92854 \tLatent loss: 5.46428e-06\n",
      "2893 Train total loss: 2.97581 \tReconstruction loss: 2.9758 \tLatent loss: 3.82094e-06\n",
      "2894 Train total loss: 3.02196 \tReconstruction loss: 3.02196 \tLatent loss: 1.33745e-06\n",
      "2895 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.66428e-06\n",
      "2896 Train total loss: 2.7737 \tReconstruction loss: 2.7737 \tLatent loss: 1.93512e-06\n",
      "2897 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 1.25791e-06\n",
      "2898 Train total loss: 2.77431 \tReconstruction loss: 2.77431 \tLatent loss: 1.13905e-06\n",
      "2899 Train total loss: 2.80518 \tReconstruction loss: 2.80517 \tLatent loss: 3.23795e-06\n",
      "2900 Train total loss: 3.03989 \tReconstruction loss: 3.03989 \tLatent loss: 1.72604e-06\n",
      "2901 Train total loss: 2.80588 \tReconstruction loss: 2.80588 \tLatent loss: 8.50944e-07\n",
      "2902 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.02436e-06\n",
      "2903 Train total loss: 2.86601 \tReconstruction loss: 2.866 \tLatent loss: 1.59369e-06\n",
      "2904 Train total loss: 3.09081 \tReconstruction loss: 3.09081 \tLatent loss: 8.71881e-07\n",
      "2905 Train total loss: 2.91472 \tReconstruction loss: 2.91472 \tLatent loss: 1.15949e-06\n",
      "2906 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 2.19691e-06\n",
      "2907 Train total loss: 2.92519 \tReconstruction loss: 2.92519 \tLatent loss: 2.44584e-06\n",
      "2908 Train total loss: 2.85417 \tReconstruction loss: 2.85416 \tLatent loss: 4.93323e-06\n",
      "2909 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 3.6011e-06\n",
      "2910 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.39777e-06\n",
      "2911 Train total loss: 2.94657 \tReconstruction loss: 2.94656 \tLatent loss: 3.88588e-06\n",
      "2912 Train total loss: 2.92841 \tReconstruction loss: 2.9284 \tLatent loss: 6.31701e-06\n",
      "2913 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 5.57857e-06\n",
      "2914 Train total loss: 3.02163 \tReconstruction loss: 3.02163 \tLatent loss: 1.97468e-06\n",
      "2915 Train total loss: 2.79365 \tReconstruction loss: 2.79365 \tLatent loss: 2.69894e-06\n",
      "2916 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.82345e-06\n",
      "2917 Train total loss: 2.7971 \tReconstruction loss: 2.7971 \tLatent loss: 9.68584e-07\n",
      "2918 Train total loss: 2.77439 \tReconstruction loss: 2.77439 \tLatent loss: 1.11451e-06\n",
      "2919 Train total loss: 2.80514 \tReconstruction loss: 2.80514 \tLatent loss: 1.41047e-06\n",
      "2920 Train total loss: 3.03967 \tReconstruction loss: 3.03967 \tLatent loss: 1.7316e-06\n",
      "2921 Train total loss: 2.80583 \tReconstruction loss: 2.80583 \tLatent loss: 1.55781e-06\n",
      "2922 Train total loss: 2.83692 \tReconstruction loss: 2.83692 \tLatent loss: 1.00985e-06\n",
      "2923 Train total loss: 2.86596 \tReconstruction loss: 2.86595 \tLatent loss: 1.34189e-06\n",
      "2924 Train total loss: 3.09082 \tReconstruction loss: 3.09082 \tLatent loss: 1.01139e-06\n",
      "2925 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 9.67248e-07\n",
      "2926 Train total loss: 2.80915 \tReconstruction loss: 2.80915 \tLatent loss: 1.09207e-06\n",
      "2927 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.96688e-06\n",
      "2928 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 5.30805e-06\n",
      "2929 Train total loss: 2.85858 \tReconstruction loss: 2.85857 \tLatent loss: 3.20774e-06\n",
      "2930 Train total loss: 2.9861 \tReconstruction loss: 2.98609 \tLatent loss: 9.00895e-07\n",
      "2931 Train total loss: 2.94648 \tReconstruction loss: 2.94648 \tLatent loss: 3.43178e-06\n",
      "2932 Train total loss: 2.92837 \tReconstruction loss: 2.92837 \tLatent loss: 6.69081e-06\n",
      "2933 Train total loss: 2.97586 \tReconstruction loss: 2.97585 \tLatent loss: 5.89995e-06\n",
      "2934 Train total loss: 3.02164 \tReconstruction loss: 3.02164 \tLatent loss: 1.54079e-06\n",
      "2935 Train total loss: 2.79365 \tReconstruction loss: 2.79365 \tLatent loss: 1.85868e-06\n",
      "2936 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.71737e-06\n",
      "2937 Train total loss: 2.79717 \tReconstruction loss: 2.79717 \tLatent loss: 6.62915e-07\n",
      "2938 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 1.64936e-06\n",
      "2939 Train total loss: 2.80525 \tReconstruction loss: 2.80525 \tLatent loss: 2.66145e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2940 Train total loss: 3.03986 \tReconstruction loss: 3.03986 \tLatent loss: 2.65649e-06\n",
      "2941 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.29641e-06\n",
      "2942 Train total loss: 2.83696 \tReconstruction loss: 2.83696 \tLatent loss: 1.26228e-06\n",
      "2943 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.41216e-06\n",
      "2944 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.15509e-06\n",
      "2945 Train total loss: 2.91469 \tReconstruction loss: 2.91469 \tLatent loss: 2.1664e-06\n",
      "2946 Train total loss: 2.80911 \tReconstruction loss: 2.80911 \tLatent loss: 1.25527e-06\n",
      "2947 Train total loss: 2.92517 \tReconstruction loss: 2.92517 \tLatent loss: 1.54414e-06\n",
      "2948 Train total loss: 2.85419 \tReconstruction loss: 2.85418 \tLatent loss: 7.82319e-06\n",
      "2949 Train total loss: 2.85857 \tReconstruction loss: 2.85857 \tLatent loss: 4.99257e-06\n",
      "2950 Train total loss: 2.98609 \tReconstruction loss: 2.98609 \tLatent loss: 1.369e-06\n",
      "2951 Train total loss: 2.94644 \tReconstruction loss: 2.94644 \tLatent loss: 3.45087e-06\n",
      "2952 Train total loss: 2.92838 \tReconstruction loss: 2.92837 \tLatent loss: 6.67623e-06\n",
      "2953 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 6.0306e-06\n",
      "2954 Train total loss: 3.02189 \tReconstruction loss: 3.02189 \tLatent loss: 1.92602e-06\n",
      "2955 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.52851e-06\n",
      "2956 Train total loss: 2.77376 \tReconstruction loss: 2.77376 \tLatent loss: 1.54389e-06\n",
      "2957 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 1.25726e-06\n",
      "2958 Train total loss: 2.77434 \tReconstruction loss: 2.77434 \tLatent loss: 2.32731e-06\n",
      "2959 Train total loss: 2.80515 \tReconstruction loss: 2.80515 \tLatent loss: 2.33824e-06\n",
      "2960 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.29622e-06\n",
      "2961 Train total loss: 2.80585 \tReconstruction loss: 2.80585 \tLatent loss: 1.15661e-06\n",
      "2962 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.46196e-06\n",
      "2963 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.31646e-06\n",
      "2964 Train total loss: 3.09083 \tReconstruction loss: 3.09082 \tLatent loss: 1.70417e-06\n",
      "2965 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 1.295e-06\n",
      "2966 Train total loss: 2.8091 \tReconstruction loss: 2.8091 \tLatent loss: 9.42181e-07\n",
      "2967 Train total loss: 2.92532 \tReconstruction loss: 2.92532 \tLatent loss: 2.02935e-06\n",
      "2968 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 5.00855e-06\n",
      "2969 Train total loss: 2.85858 \tReconstruction loss: 2.85857 \tLatent loss: 4.6107e-06\n",
      "2970 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.87001e-06\n",
      "2971 Train total loss: 2.94639 \tReconstruction loss: 2.94639 \tLatent loss: 2.14915e-06\n",
      "2972 Train total loss: 2.92845 \tReconstruction loss: 2.92844 \tLatent loss: 6.08967e-06\n",
      "2973 Train total loss: 2.97587 \tReconstruction loss: 2.97587 \tLatent loss: 4.95216e-06\n",
      "2974 Train total loss: 3.02171 \tReconstruction loss: 3.02171 \tLatent loss: 1.32715e-06\n",
      "2975 Train total loss: 2.79365 \tReconstruction loss: 2.79365 \tLatent loss: 2.80194e-06\n",
      "2976 Train total loss: 2.77372 \tReconstruction loss: 2.77372 \tLatent loss: 1.70855e-06\n",
      "2977 Train total loss: 2.79713 \tReconstruction loss: 2.79713 \tLatent loss: 1.45635e-06\n",
      "2978 Train total loss: 2.77431 \tReconstruction loss: 2.77431 \tLatent loss: 2.74537e-06\n",
      "2979 Train total loss: 2.80518 \tReconstruction loss: 2.80518 \tLatent loss: 2.55996e-06\n",
      "2980 Train total loss: 3.03974 \tReconstruction loss: 3.03974 \tLatent loss: 1.53979e-06\n",
      "2981 Train total loss: 2.80589 \tReconstruction loss: 2.80589 \tLatent loss: 1.60907e-06\n",
      "2982 Train total loss: 2.83697 \tReconstruction loss: 2.83696 \tLatent loss: 2.05097e-06\n",
      "2983 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.61596e-06\n",
      "2984 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 2.01807e-06\n",
      "2985 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.2374e-06\n",
      "2986 Train total loss: 2.80912 \tReconstruction loss: 2.80912 \tLatent loss: 2.20856e-06\n",
      "2987 Train total loss: 2.92521 \tReconstruction loss: 2.92521 \tLatent loss: 1.26627e-06\n",
      "2988 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 7.8337e-06\n",
      "2989 Train total loss: 2.85857 \tReconstruction loss: 2.85857 \tLatent loss: 5.79226e-06\n",
      "2990 Train total loss: 2.98592 \tReconstruction loss: 2.98592 \tLatent loss: 1.73497e-06\n",
      "2991 Train total loss: 2.94651 \tReconstruction loss: 2.94651 \tLatent loss: 2.53393e-06\n",
      "2992 Train total loss: 2.92855 \tReconstruction loss: 2.92855 \tLatent loss: 5.76625e-06\n",
      "2993 Train total loss: 2.97575 \tReconstruction loss: 2.97574 \tLatent loss: 6.07542e-06\n",
      "2994 Train total loss: 3.02176 \tReconstruction loss: 3.02176 \tLatent loss: 1.46929e-06\n",
      "2995 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.18992e-06\n",
      "2996 Train total loss: 2.77372 \tReconstruction loss: 2.77372 \tLatent loss: 2.63209e-06\n",
      "2997 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 9.67734e-07\n",
      "2998 Train total loss: 2.77428 \tReconstruction loss: 2.77427 \tLatent loss: 1.62504e-06\n",
      "2999 Train total loss: 2.80516 \tReconstruction loss: 2.80516 \tLatent loss: 4.25996e-06\n",
      "3000 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.6829e-06\n",
      "3001 Train total loss: 2.80582 \tReconstruction loss: 2.80582 \tLatent loss: 1.83497e-06\n",
      "3002 Train total loss: 2.83697 \tReconstruction loss: 2.83697 \tLatent loss: 2.09848e-06\n",
      "3003 Train total loss: 2.86592 \tReconstruction loss: 2.86592 \tLatent loss: 1.14868e-06\n",
      "3004 Train total loss: 3.09084 \tReconstruction loss: 3.09084 \tLatent loss: 1.24281e-06\n",
      "3005 Train total loss: 2.91479 \tReconstruction loss: 2.91478 \tLatent loss: 2.99354e-06\n",
      "3006 Train total loss: 2.80904 \tReconstruction loss: 2.80903 \tLatent loss: 2.66376e-06\n",
      "3007 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.77891e-06\n",
      "3008 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 4.69639e-06\n",
      "3009 Train total loss: 2.85857 \tReconstruction loss: 2.85856 \tLatent loss: 5.29803e-06\n",
      "3010 Train total loss: 2.98609 \tReconstruction loss: 2.98608 \tLatent loss: 1.34232e-06\n",
      "3011 Train total loss: 2.94648 \tReconstruction loss: 2.94647 \tLatent loss: 1.86123e-06\n",
      "3012 Train total loss: 2.92834 \tReconstruction loss: 2.92833 \tLatent loss: 5.36812e-06\n",
      "3013 Train total loss: 2.97579 \tReconstruction loss: 2.97578 \tLatent loss: 4.77011e-06\n",
      "3014 Train total loss: 3.02171 \tReconstruction loss: 3.02171 \tLatent loss: 1.34493e-06\n",
      "3015 Train total loss: 2.79369 \tReconstruction loss: 2.79369 \tLatent loss: 1.86429e-06\n",
      "3016 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.48485e-06\n",
      "3017 Train total loss: 2.79713 \tReconstruction loss: 2.79713 \tLatent loss: 9.78743e-07\n",
      "3018 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.46143e-06\n",
      "3019 Train total loss: 2.80522 \tReconstruction loss: 2.80521 \tLatent loss: 2.99645e-06\n",
      "3020 Train total loss: 3.0397 \tReconstruction loss: 3.0397 \tLatent loss: 1.58713e-06\n",
      "3021 Train total loss: 2.80581 \tReconstruction loss: 2.80581 \tLatent loss: 1.40501e-06\n",
      "3022 Train total loss: 2.83697 \tReconstruction loss: 2.83696 \tLatent loss: 1.37545e-06\n",
      "3023 Train total loss: 2.86601 \tReconstruction loss: 2.866 \tLatent loss: 2.6221e-06\n",
      "3024 Train total loss: 3.09084 \tReconstruction loss: 3.09084 \tLatent loss: 9.20423e-07\n",
      "3025 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.559e-06\n",
      "3026 Train total loss: 2.80914 \tReconstruction loss: 2.80914 \tLatent loss: 1.43529e-06\n",
      "3027 Train total loss: 2.9252 \tReconstruction loss: 2.92519 \tLatent loss: 8.19929e-07\n",
      "3028 Train total loss: 2.85421 \tReconstruction loss: 2.8542 \tLatent loss: 4.02667e-06\n",
      "3029 Train total loss: 2.85856 \tReconstruction loss: 2.85855 \tLatent loss: 3.79735e-06\n",
      "3030 Train total loss: 2.98606 \tReconstruction loss: 2.98605 \tLatent loss: 2.00532e-06\n",
      "3031 Train total loss: 2.94643 \tReconstruction loss: 2.94643 \tLatent loss: 2.19099e-06\n",
      "3032 Train total loss: 2.92843 \tReconstruction loss: 2.92843 \tLatent loss: 3.68467e-06\n",
      "3033 Train total loss: 2.97586 \tReconstruction loss: 2.97586 \tLatent loss: 5.81332e-06\n",
      "3034 Train total loss: 3.0217 \tReconstruction loss: 3.0217 \tLatent loss: 1.11354e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3035 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.76949e-06\n",
      "3036 Train total loss: 2.77374 \tReconstruction loss: 2.77373 \tLatent loss: 2.11346e-06\n",
      "3037 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 7.77696e-07\n",
      "3038 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.86627e-06\n",
      "3039 Train total loss: 2.80526 \tReconstruction loss: 2.80525 \tLatent loss: 2.44368e-06\n",
      "3040 Train total loss: 3.03974 \tReconstruction loss: 3.03974 \tLatent loss: 1.11846e-06\n",
      "3041 Train total loss: 2.80583 \tReconstruction loss: 2.80583 \tLatent loss: 1.38934e-06\n",
      "3042 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 2.79354e-06\n",
      "3043 Train total loss: 2.86598 \tReconstruction loss: 2.86598 \tLatent loss: 1.18589e-06\n",
      "3044 Train total loss: 3.0909 \tReconstruction loss: 3.0909 \tLatent loss: 7.76746e-07\n",
      "3045 Train total loss: 2.91472 \tReconstruction loss: 2.91472 \tLatent loss: 1.8568e-06\n",
      "3046 Train total loss: 2.80904 \tReconstruction loss: 2.80904 \tLatent loss: 1.06753e-06\n",
      "3047 Train total loss: 2.9252 \tReconstruction loss: 2.9252 \tLatent loss: 4.05162e-07\n",
      "3048 Train total loss: 2.85419 \tReconstruction loss: 2.85419 \tLatent loss: 3.99861e-06\n",
      "3049 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 3.61534e-06\n",
      "3050 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 1.8244e-06\n",
      "3051 Train total loss: 2.94648 \tReconstruction loss: 2.94648 \tLatent loss: 1.80358e-06\n",
      "3052 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 4.11179e-06\n",
      "3053 Train total loss: 2.9758 \tReconstruction loss: 2.9758 \tLatent loss: 2.6065e-06\n",
      "3054 Train total loss: 3.02185 \tReconstruction loss: 3.02185 \tLatent loss: 7.00679e-07\n",
      "3055 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 2.39686e-06\n",
      "3056 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.11575e-06\n",
      "3057 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 1.05675e-06\n",
      "3058 Train total loss: 2.77431 \tReconstruction loss: 2.77431 \tLatent loss: 1.99696e-06\n",
      "3059 Train total loss: 2.80519 \tReconstruction loss: 2.80519 \tLatent loss: 3.88757e-06\n",
      "3060 Train total loss: 3.03986 \tReconstruction loss: 3.03986 \tLatent loss: 1.71314e-06\n",
      "3061 Train total loss: 2.80589 \tReconstruction loss: 2.80589 \tLatent loss: 9.2111e-07\n",
      "3062 Train total loss: 2.83691 \tReconstruction loss: 2.83691 \tLatent loss: 1.75384e-06\n",
      "3063 Train total loss: 2.86594 \tReconstruction loss: 2.86593 \tLatent loss: 1.66849e-06\n",
      "3064 Train total loss: 3.09084 \tReconstruction loss: 3.09084 \tLatent loss: 7.15259e-07\n",
      "3065 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.31472e-06\n",
      "3066 Train total loss: 2.8091 \tReconstruction loss: 2.8091 \tLatent loss: 1.95172e-06\n",
      "3067 Train total loss: 2.92518 \tReconstruction loss: 2.92518 \tLatent loss: 4.92163e-07\n",
      "3068 Train total loss: 2.85419 \tReconstruction loss: 2.85419 \tLatent loss: 3.47354e-06\n",
      "3069 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.82153e-06\n",
      "3070 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.3231e-06\n",
      "3071 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 1.47727e-06\n",
      "3072 Train total loss: 2.92837 \tReconstruction loss: 2.92836 \tLatent loss: 7.09463e-06\n",
      "3073 Train total loss: 2.97576 \tReconstruction loss: 2.97576 \tLatent loss: 3.2553e-06\n",
      "3074 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 8.16263e-07\n",
      "3075 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.91077e-06\n",
      "3076 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.83396e-06\n",
      "3077 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.05978e-06\n",
      "3078 Train total loss: 2.77427 \tReconstruction loss: 2.77427 \tLatent loss: 2.19069e-06\n",
      "3079 Train total loss: 2.80519 \tReconstruction loss: 2.80518 \tLatent loss: 3.6567e-06\n",
      "3080 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 1.21213e-06\n",
      "3081 Train total loss: 2.80584 \tReconstruction loss: 2.80584 \tLatent loss: 4.91107e-07\n",
      "3082 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 2.44399e-06\n",
      "3083 Train total loss: 2.86599 \tReconstruction loss: 2.86598 \tLatent loss: 1.74014e-06\n",
      "3084 Train total loss: 3.0908 \tReconstruction loss: 3.0908 \tLatent loss: 1.23541e-06\n",
      "3085 Train total loss: 2.91472 \tReconstruction loss: 2.91472 \tLatent loss: 3.59207e-06\n",
      "3086 Train total loss: 2.80914 \tReconstruction loss: 2.80913 \tLatent loss: 2.00677e-06\n",
      "3087 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 7.16129e-07\n",
      "3088 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.2443e-06\n",
      "3089 Train total loss: 2.85858 \tReconstruction loss: 2.85857 \tLatent loss: 3.94972e-06\n",
      "3090 Train total loss: 2.98602 \tReconstruction loss: 2.98601 \tLatent loss: 1.79255e-06\n",
      "3091 Train total loss: 2.94649 \tReconstruction loss: 2.94649 \tLatent loss: 3.00419e-06\n",
      "3092 Train total loss: 2.92841 \tReconstruction loss: 2.9284 \tLatent loss: 6.34713e-06\n",
      "3093 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.98346e-06\n",
      "3094 Train total loss: 3.02178 \tReconstruction loss: 3.02178 \tLatent loss: 1.15187e-06\n",
      "3095 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.18468e-06\n",
      "3096 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.11898e-06\n",
      "3097 Train total loss: 2.79713 \tReconstruction loss: 2.79713 \tLatent loss: 7.31455e-07\n",
      "3098 Train total loss: 2.77433 \tReconstruction loss: 2.77433 \tLatent loss: 1.945e-06\n",
      "3099 Train total loss: 2.80519 \tReconstruction loss: 2.80519 \tLatent loss: 2.08094e-06\n",
      "3100 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 2.1773e-06\n",
      "3101 Train total loss: 2.80589 \tReconstruction loss: 2.80589 \tLatent loss: 5.36452e-07\n",
      "3102 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 2.46181e-06\n",
      "3103 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.75368e-06\n",
      "3104 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 5.09834e-07\n",
      "3105 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.17562e-06\n",
      "3106 Train total loss: 2.80905 \tReconstruction loss: 2.80905 \tLatent loss: 1.64463e-06\n",
      "3107 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 5.50279e-07\n",
      "3108 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.41e-06\n",
      "3109 Train total loss: 2.85857 \tReconstruction loss: 2.85857 \tLatent loss: 4.70497e-06\n",
      "3110 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 1.5609e-06\n",
      "3111 Train total loss: 2.94651 \tReconstruction loss: 2.9465 \tLatent loss: 1.52123e-06\n",
      "3112 Train total loss: 2.92842 \tReconstruction loss: 2.92842 \tLatent loss: 5.79502e-06\n",
      "3113 Train total loss: 2.97578 \tReconstruction loss: 2.97578 \tLatent loss: 2.70092e-06\n",
      "3114 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.74963e-07\n",
      "3115 Train total loss: 2.79367 \tReconstruction loss: 2.79366 \tLatent loss: 2.09862e-06\n",
      "3116 Train total loss: 2.77373 \tReconstruction loss: 2.77372 \tLatent loss: 3.16532e-06\n",
      "3117 Train total loss: 2.79718 \tReconstruction loss: 2.79718 \tLatent loss: 1.21149e-06\n",
      "3118 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 2.41794e-06\n",
      "3119 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 4.76753e-06\n",
      "3120 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.12137e-06\n",
      "3121 Train total loss: 2.80585 \tReconstruction loss: 2.80585 \tLatent loss: 5.81855e-07\n",
      "3122 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 2.2426e-06\n",
      "3123 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 1.68077e-06\n",
      "3124 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 1.22235e-06\n",
      "3125 Train total loss: 2.91477 \tReconstruction loss: 2.91477 \tLatent loss: 2.50463e-06\n",
      "3126 Train total loss: 2.80903 \tReconstruction loss: 2.80902 \tLatent loss: 1.73036e-06\n",
      "3127 Train total loss: 2.92519 \tReconstruction loss: 2.92519 \tLatent loss: 5.96431e-07\n",
      "3128 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 2.52204e-06\n",
      "3129 Train total loss: 2.85856 \tReconstruction loss: 2.85855 \tLatent loss: 3.90277e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3130 Train total loss: 2.98602 \tReconstruction loss: 2.98602 \tLatent loss: 1.53142e-06\n",
      "3131 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 1.02582e-06\n",
      "3132 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 5.52739e-06\n",
      "3133 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 3.053e-06\n",
      "3134 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 5.77205e-07\n",
      "3135 Train total loss: 2.7937 \tReconstruction loss: 2.7937 \tLatent loss: 1.6578e-06\n",
      "3136 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.70502e-06\n",
      "3137 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 9.98025e-07\n",
      "3138 Train total loss: 2.77428 \tReconstruction loss: 2.77427 \tLatent loss: 2.29865e-06\n",
      "3139 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 3.56387e-06\n",
      "3140 Train total loss: 3.03971 \tReconstruction loss: 3.03971 \tLatent loss: 2.57008e-06\n",
      "3141 Train total loss: 2.80582 \tReconstruction loss: 2.80582 \tLatent loss: 5.9117e-07\n",
      "3142 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 1.70614e-06\n",
      "3143 Train total loss: 2.86599 \tReconstruction loss: 2.86599 \tLatent loss: 1.76852e-06\n",
      "3144 Train total loss: 3.09075 \tReconstruction loss: 3.09074 \tLatent loss: 6.87835e-07\n",
      "3145 Train total loss: 2.91476 \tReconstruction loss: 2.91475 \tLatent loss: 2.5106e-06\n",
      "3146 Train total loss: 2.80911 \tReconstruction loss: 2.80911 \tLatent loss: 2.54468e-06\n",
      "3147 Train total loss: 2.92518 \tReconstruction loss: 2.92518 \tLatent loss: 3.51464e-07\n",
      "3148 Train total loss: 2.85417 \tReconstruction loss: 2.85417 \tLatent loss: 2.90819e-06\n",
      "3149 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 5.14804e-06\n",
      "3150 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 2.1259e-06\n",
      "3151 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 1.92022e-06\n",
      "3152 Train total loss: 2.92845 \tReconstruction loss: 2.92845 \tLatent loss: 5.07823e-06\n",
      "3153 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 4.4406e-06\n",
      "3154 Train total loss: 3.02171 \tReconstruction loss: 3.02171 \tLatent loss: 6.8629e-07\n",
      "3155 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.23074e-06\n",
      "3156 Train total loss: 2.77375 \tReconstruction loss: 2.77374 \tLatent loss: 1.90922e-06\n",
      "3157 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 6.63265e-07\n",
      "3158 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 1.23524e-06\n",
      "3159 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 2.45997e-06\n",
      "3160 Train total loss: 3.03977 \tReconstruction loss: 3.03976 \tLatent loss: 2.67597e-06\n",
      "3161 Train total loss: 2.80584 \tReconstruction loss: 2.80584 \tLatent loss: 8.76107e-07\n",
      "3162 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 1.95324e-06\n",
      "3163 Train total loss: 2.86599 \tReconstruction loss: 2.86598 \tLatent loss: 2.2136e-06\n",
      "3164 Train total loss: 3.09083 \tReconstruction loss: 3.09083 \tLatent loss: 1.43659e-06\n",
      "3165 Train total loss: 2.9148 \tReconstruction loss: 2.91479 \tLatent loss: 1.89162e-06\n",
      "3166 Train total loss: 2.80906 \tReconstruction loss: 2.80905 \tLatent loss: 2.08936e-06\n",
      "3167 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 1.00494e-06\n",
      "3168 Train total loss: 2.85427 \tReconstruction loss: 2.85427 \tLatent loss: 2.85027e-06\n",
      "3169 Train total loss: 2.85858 \tReconstruction loss: 2.85857 \tLatent loss: 4.31992e-06\n",
      "3170 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 1.29624e-06\n",
      "3171 Train total loss: 2.94641 \tReconstruction loss: 2.94641 \tLatent loss: 8.86258e-07\n",
      "3172 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 4.26392e-06\n",
      "3173 Train total loss: 2.9758 \tReconstruction loss: 2.97579 \tLatent loss: 3.61536e-06\n",
      "3174 Train total loss: 3.02186 \tReconstruction loss: 3.02186 \tLatent loss: 5.87747e-07\n",
      "3175 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.52199e-06\n",
      "3176 Train total loss: 2.77375 \tReconstruction loss: 2.77374 \tLatent loss: 2.55756e-06\n",
      "3177 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 8.3888e-07\n",
      "3178 Train total loss: 2.77427 \tReconstruction loss: 2.77427 \tLatent loss: 1.52335e-06\n",
      "3179 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.88153e-06\n",
      "3180 Train total loss: 3.03971 \tReconstruction loss: 3.03971 \tLatent loss: 2.11845e-06\n",
      "3181 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 6.00793e-07\n",
      "3182 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 1.63439e-06\n",
      "3183 Train total loss: 2.86595 \tReconstruction loss: 2.86594 \tLatent loss: 3.78875e-06\n",
      "3184 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.46361e-06\n",
      "3185 Train total loss: 2.91477 \tReconstruction loss: 2.91477 \tLatent loss: 1.9481e-06\n",
      "3186 Train total loss: 2.8091 \tReconstruction loss: 2.8091 \tLatent loss: 2.38018e-06\n",
      "3187 Train total loss: 2.92512 \tReconstruction loss: 2.92512 \tLatent loss: 5.99796e-07\n",
      "3188 Train total loss: 2.85417 \tReconstruction loss: 2.85417 \tLatent loss: 2.13258e-06\n",
      "3189 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 5.09117e-06\n",
      "3190 Train total loss: 2.98604 \tReconstruction loss: 2.98604 \tLatent loss: 3.31533e-06\n",
      "3191 Train total loss: 2.9465 \tReconstruction loss: 2.9465 \tLatent loss: 1.07451e-06\n",
      "3192 Train total loss: 2.92837 \tReconstruction loss: 2.92837 \tLatent loss: 3.9655e-06\n",
      "3193 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 4.33441e-06\n",
      "3194 Train total loss: 3.02173 \tReconstruction loss: 3.02172 \tLatent loss: 1.34087e-06\n",
      "3195 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 2.26646e-06\n",
      "3196 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.71042e-06\n",
      "3197 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 6.24108e-07\n",
      "3198 Train total loss: 2.77431 \tReconstruction loss: 2.77431 \tLatent loss: 1.76919e-06\n",
      "3199 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 3.62248e-06\n",
      "3200 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 3.46265e-06\n",
      "3201 Train total loss: 2.80588 \tReconstruction loss: 2.80588 \tLatent loss: 1.20791e-06\n",
      "3202 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.33324e-06\n",
      "3203 Train total loss: 2.86596 \tReconstruction loss: 2.86595 \tLatent loss: 3.13183e-06\n",
      "3204 Train total loss: 3.09081 \tReconstruction loss: 3.09081 \tLatent loss: 9.51839e-07\n",
      "3205 Train total loss: 2.91473 \tReconstruction loss: 2.91473 \tLatent loss: 1.7264e-06\n",
      "3206 Train total loss: 2.8091 \tReconstruction loss: 2.8091 \tLatent loss: 2.13494e-06\n",
      "3207 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.29275e-07\n",
      "3208 Train total loss: 2.85423 \tReconstruction loss: 2.85422 \tLatent loss: 1.97087e-06\n",
      "3209 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 4.47457e-06\n",
      "3210 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 2.51745e-06\n",
      "3211 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 9.75686e-07\n",
      "3212 Train total loss: 2.92842 \tReconstruction loss: 2.92842 \tLatent loss: 5.9766e-06\n",
      "3213 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 4.11535e-06\n",
      "3214 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 9.8201e-07\n",
      "3215 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 2.44813e-06\n",
      "3216 Train total loss: 2.77375 \tReconstruction loss: 2.77374 \tLatent loss: 2.79413e-06\n",
      "3217 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 1.25608e-06\n",
      "3218 Train total loss: 2.77426 \tReconstruction loss: 2.77426 \tLatent loss: 1.65699e-06\n",
      "3219 Train total loss: 2.80523 \tReconstruction loss: 2.80523 \tLatent loss: 4.65476e-06\n",
      "3220 Train total loss: 3.03984 \tReconstruction loss: 3.03984 \tLatent loss: 2.06242e-06\n",
      "3221 Train total loss: 2.80585 \tReconstruction loss: 2.80585 \tLatent loss: 8.90422e-07\n",
      "3222 Train total loss: 2.83696 \tReconstruction loss: 2.83696 \tLatent loss: 1.8299e-06\n",
      "3223 Train total loss: 2.86598 \tReconstruction loss: 2.86598 \tLatent loss: 2.01344e-06\n",
      "3224 Train total loss: 3.0908 \tReconstruction loss: 3.0908 \tLatent loss: 6.09155e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3225 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.06444e-06\n",
      "3226 Train total loss: 2.80907 \tReconstruction loss: 2.80906 \tLatent loss: 1.29288e-06\n",
      "3227 Train total loss: 2.92522 \tReconstruction loss: 2.92521 \tLatent loss: 8.33109e-07\n",
      "3228 Train total loss: 2.85419 \tReconstruction loss: 2.85419 \tLatent loss: 2.03955e-06\n",
      "3229 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 4.59293e-06\n",
      "3230 Train total loss: 2.98602 \tReconstruction loss: 2.98601 \tLatent loss: 2.61958e-06\n",
      "3231 Train total loss: 2.94649 \tReconstruction loss: 2.94649 \tLatent loss: 9.08566e-07\n",
      "3232 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.73149e-06\n",
      "3233 Train total loss: 2.97578 \tReconstruction loss: 2.97578 \tLatent loss: 3.49259e-06\n",
      "3234 Train total loss: 3.02176 \tReconstruction loss: 3.02176 \tLatent loss: 6.94359e-07\n",
      "3235 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 1.74302e-06\n",
      "3236 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.95198e-06\n",
      "3237 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.16755e-06\n",
      "3238 Train total loss: 2.77431 \tReconstruction loss: 2.77431 \tLatent loss: 1.67443e-06\n",
      "3239 Train total loss: 2.80519 \tReconstruction loss: 2.80519 \tLatent loss: 3.39192e-06\n",
      "3240 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 1.42104e-06\n",
      "3241 Train total loss: 2.80585 \tReconstruction loss: 2.80585 \tLatent loss: 5.89181e-07\n",
      "3242 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.91261e-06\n",
      "3243 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.54622e-06\n",
      "3244 Train total loss: 3.09081 \tReconstruction loss: 3.09081 \tLatent loss: 7.59339e-07\n",
      "3245 Train total loss: 2.91465 \tReconstruction loss: 2.91465 \tLatent loss: 9.58535e-07\n",
      "3246 Train total loss: 2.80903 \tReconstruction loss: 2.80903 \tLatent loss: 2.98035e-06\n",
      "3247 Train total loss: 2.92514 \tReconstruction loss: 2.92513 \tLatent loss: 6.4268e-07\n",
      "3248 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.96759e-06\n",
      "3249 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.1997e-06\n",
      "3250 Train total loss: 2.98605 \tReconstruction loss: 2.98605 \tLatent loss: 2.4255e-06\n",
      "3251 Train total loss: 2.94651 \tReconstruction loss: 2.94651 \tLatent loss: 6.27121e-07\n",
      "3252 Train total loss: 2.92843 \tReconstruction loss: 2.92843 \tLatent loss: 4.27974e-06\n",
      "3253 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.22214e-06\n",
      "3254 Train total loss: 3.02184 \tReconstruction loss: 3.02184 \tLatent loss: 5.84307e-07\n",
      "3255 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.82445e-06\n",
      "3256 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.92841e-06\n",
      "3257 Train total loss: 2.79717 \tReconstruction loss: 2.79717 \tLatent loss: 1.73627e-06\n",
      "3258 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.00904e-06\n",
      "3259 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 3.48046e-06\n",
      "3260 Train total loss: 3.03977 \tReconstruction loss: 3.03977 \tLatent loss: 1.85824e-06\n",
      "3261 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.71018e-07\n",
      "3262 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 2.43077e-06\n",
      "3263 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.67658e-06\n",
      "3264 Train total loss: 3.0908 \tReconstruction loss: 3.0908 \tLatent loss: 4.5294e-07\n",
      "3265 Train total loss: 2.91481 \tReconstruction loss: 2.91481 \tLatent loss: 1.41654e-06\n",
      "3266 Train total loss: 2.8091 \tReconstruction loss: 2.8091 \tLatent loss: 2.32785e-06\n",
      "3267 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 8.57137e-07\n",
      "3268 Train total loss: 2.8542 \tReconstruction loss: 2.8542 \tLatent loss: 1.25018e-06\n",
      "3269 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 3.85992e-06\n",
      "3270 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.5805e-06\n",
      "3271 Train total loss: 2.94644 \tReconstruction loss: 2.94643 \tLatent loss: 5.63668e-07\n",
      "3272 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.32954e-06\n",
      "3273 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 4.13933e-06\n",
      "3274 Train total loss: 3.02173 \tReconstruction loss: 3.02173 \tLatent loss: 9.05551e-07\n",
      "3275 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 1.57367e-06\n",
      "3276 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.88978e-06\n",
      "3277 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.48746e-06\n",
      "3278 Train total loss: 2.7743 \tReconstruction loss: 2.77429 \tLatent loss: 1.10078e-06\n",
      "3279 Train total loss: 2.80518 \tReconstruction loss: 2.80518 \tLatent loss: 3.17142e-06\n",
      "3280 Train total loss: 3.03975 \tReconstruction loss: 3.03975 \tLatent loss: 2.44663e-06\n",
      "3281 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 5.14473e-07\n",
      "3282 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.88688e-06\n",
      "3283 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.97056e-06\n",
      "3284 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 9.73087e-07\n",
      "3285 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 1.28239e-06\n",
      "3286 Train total loss: 2.80905 \tReconstruction loss: 2.80905 \tLatent loss: 2.89963e-06\n",
      "3287 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 1.09e-06\n",
      "3288 Train total loss: 2.85424 \tReconstruction loss: 2.85424 \tLatent loss: 2.38934e-06\n",
      "3289 Train total loss: 2.85857 \tReconstruction loss: 2.85856 \tLatent loss: 4.75104e-06\n",
      "3290 Train total loss: 2.98602 \tReconstruction loss: 2.98602 \tLatent loss: 2.62582e-06\n",
      "3291 Train total loss: 2.9465 \tReconstruction loss: 2.9465 \tLatent loss: 5.0315e-07\n",
      "3292 Train total loss: 2.92844 \tReconstruction loss: 2.92843 \tLatent loss: 3.36902e-06\n",
      "3293 Train total loss: 2.97581 \tReconstruction loss: 2.97581 \tLatent loss: 3.32248e-06\n",
      "3294 Train total loss: 3.02186 \tReconstruction loss: 3.02186 \tLatent loss: 1.51446e-06\n",
      "3295 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 1.13236e-06\n",
      "3296 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.77588e-06\n",
      "3297 Train total loss: 2.79714 \tReconstruction loss: 2.79713 \tLatent loss: 1.31467e-06\n",
      "3298 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.95195e-07\n",
      "3299 Train total loss: 2.80518 \tReconstruction loss: 2.80518 \tLatent loss: 2.97612e-06\n",
      "3300 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 3.59453e-06\n",
      "3301 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 4.0233e-07\n",
      "3302 Train total loss: 2.83692 \tReconstruction loss: 2.83692 \tLatent loss: 1.78047e-06\n",
      "3303 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 2.91187e-06\n",
      "3304 Train total loss: 3.09075 \tReconstruction loss: 3.09075 \tLatent loss: 1.69055e-06\n",
      "3305 Train total loss: 2.91477 \tReconstruction loss: 2.91477 \tLatent loss: 9.99504e-07\n",
      "3306 Train total loss: 2.80904 \tReconstruction loss: 2.80904 \tLatent loss: 2.69325e-06\n",
      "3307 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 1.42985e-06\n",
      "3308 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.35288e-06\n",
      "3309 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.04766e-06\n",
      "3310 Train total loss: 2.98604 \tReconstruction loss: 2.98604 \tLatent loss: 2.33429e-06\n",
      "3311 Train total loss: 2.94638 \tReconstruction loss: 2.94638 \tLatent loss: 4.38714e-07\n",
      "3312 Train total loss: 2.92842 \tReconstruction loss: 2.92842 \tLatent loss: 2.63926e-06\n",
      "3313 Train total loss: 2.97586 \tReconstruction loss: 2.97585 \tLatent loss: 3.21441e-06\n",
      "3314 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 1.29058e-06\n",
      "3315 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 9.01625e-07\n",
      "3316 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.59085e-06\n",
      "3317 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 1.6842e-06\n",
      "3318 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.30632e-07\n",
      "3319 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 3.65584e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3320 Train total loss: 3.03978 \tReconstruction loss: 3.03977 \tLatent loss: 3.09733e-06\n",
      "3321 Train total loss: 2.80585 \tReconstruction loss: 2.80585 \tLatent loss: 2.33047e-07\n",
      "3322 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 1.60946e-06\n",
      "3323 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 2.39986e-06\n",
      "3324 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 9.38948e-07\n",
      "3325 Train total loss: 2.91472 \tReconstruction loss: 2.91472 \tLatent loss: 8.29154e-07\n",
      "3326 Train total loss: 2.80903 \tReconstruction loss: 2.80903 \tLatent loss: 2.62172e-06\n",
      "3327 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 2.27107e-06\n",
      "3328 Train total loss: 2.85424 \tReconstruction loss: 2.85424 \tLatent loss: 8.68829e-07\n",
      "3329 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 3.99479e-06\n",
      "3330 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.21821e-06\n",
      "3331 Train total loss: 2.94644 \tReconstruction loss: 2.94644 \tLatent loss: 4.89264e-07\n",
      "3332 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.64198e-06\n",
      "3333 Train total loss: 2.97591 \tReconstruction loss: 2.9759 \tLatent loss: 3.96632e-06\n",
      "3334 Train total loss: 3.02176 \tReconstruction loss: 3.02176 \tLatent loss: 1.41748e-06\n",
      "3335 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.06613e-06\n",
      "3336 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.5076e-06\n",
      "3337 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 1.85313e-06\n",
      "3338 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 7.41224e-07\n",
      "3339 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.82893e-06\n",
      "3340 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 2.57685e-06\n",
      "3341 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 6.39423e-07\n",
      "3342 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.4036e-06\n",
      "3343 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.15831e-06\n",
      "3344 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 1.2649e-06\n",
      "3345 Train total loss: 2.91472 \tReconstruction loss: 2.91472 \tLatent loss: 4.71123e-07\n",
      "3346 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.344e-06\n",
      "3347 Train total loss: 2.92512 \tReconstruction loss: 2.92512 \tLatent loss: 1.75318e-06\n",
      "3348 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.8666e-07\n",
      "3349 Train total loss: 2.85856 \tReconstruction loss: 2.85855 \tLatent loss: 3.80827e-06\n",
      "3350 Train total loss: 2.98602 \tReconstruction loss: 2.98601 \tLatent loss: 3.12994e-06\n",
      "3351 Train total loss: 2.94648 \tReconstruction loss: 2.94648 \tLatent loss: 6.72054e-07\n",
      "3352 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.87773e-06\n",
      "3353 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 4.8829e-06\n",
      "3354 Train total loss: 3.0218 \tReconstruction loss: 3.02179 \tLatent loss: 1.63475e-06\n",
      "3355 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 8.3186e-07\n",
      "3356 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.36222e-06\n",
      "3357 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 1.37793e-06\n",
      "3358 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 9.01236e-07\n",
      "3359 Train total loss: 2.80519 \tReconstruction loss: 2.80519 \tLatent loss: 2.59806e-06\n",
      "3360 Train total loss: 3.03976 \tReconstruction loss: 3.03976 \tLatent loss: 2.24826e-06\n",
      "3361 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 4.90241e-07\n",
      "3362 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 9.61567e-07\n",
      "3363 Train total loss: 2.86598 \tReconstruction loss: 2.86598 \tLatent loss: 2.60356e-06\n",
      "3364 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 1.41689e-06\n",
      "3365 Train total loss: 2.9147 \tReconstruction loss: 2.9147 \tLatent loss: 4.32874e-07\n",
      "3366 Train total loss: 2.80909 \tReconstruction loss: 2.80909 \tLatent loss: 2.51487e-06\n",
      "3367 Train total loss: 2.92515 \tReconstruction loss: 2.92514 \tLatent loss: 2.26244e-06\n",
      "3368 Train total loss: 2.8542 \tReconstruction loss: 2.8542 \tLatent loss: 9.38105e-07\n",
      "3369 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 3.24785e-06\n",
      "3370 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.62122e-06\n",
      "3371 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.14362e-07\n",
      "3372 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.67566e-06\n",
      "3373 Train total loss: 2.97582 \tReconstruction loss: 2.97581 \tLatent loss: 4.23406e-06\n",
      "3374 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 3.15571e-06\n",
      "3375 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.67042e-07\n",
      "3376 Train total loss: 2.77374 \tReconstruction loss: 2.77373 \tLatent loss: 2.28865e-06\n",
      "3377 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 2.22428e-06\n",
      "3378 Train total loss: 2.7743 \tReconstruction loss: 2.77429 \tLatent loss: 8.43899e-07\n",
      "3379 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.45268e-06\n",
      "3380 Train total loss: 3.03978 \tReconstruction loss: 3.03977 \tLatent loss: 3.45357e-06\n",
      "3381 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.34656e-06\n",
      "3382 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 1.38349e-06\n",
      "3383 Train total loss: 2.86599 \tReconstruction loss: 2.86599 \tLatent loss: 2.98331e-06\n",
      "3384 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 1.39581e-06\n",
      "3385 Train total loss: 2.91473 \tReconstruction loss: 2.91473 \tLatent loss: 7.09679e-07\n",
      "3386 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.07073e-06\n",
      "3387 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.56898e-06\n",
      "3388 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.93431e-07\n",
      "3389 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 3.32272e-06\n",
      "3390 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 3.05412e-06\n",
      "3391 Train total loss: 2.94648 \tReconstruction loss: 2.94648 \tLatent loss: 1.0115e-06\n",
      "3392 Train total loss: 2.92837 \tReconstruction loss: 2.92837 \tLatent loss: 2.47085e-06\n",
      "3393 Train total loss: 2.97581 \tReconstruction loss: 2.9758 \tLatent loss: 4.33847e-06\n",
      "3394 Train total loss: 3.02175 \tReconstruction loss: 3.02175 \tLatent loss: 1.90831e-06\n",
      "3395 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 7.73181e-07\n",
      "3396 Train total loss: 2.77375 \tReconstruction loss: 2.77374 \tLatent loss: 3.93236e-06\n",
      "3397 Train total loss: 2.79715 \tReconstruction loss: 2.79714 \tLatent loss: 2.38094e-06\n",
      "3398 Train total loss: 2.77431 \tReconstruction loss: 2.7743 \tLatent loss: 9.91036e-07\n",
      "3399 Train total loss: 2.80519 \tReconstruction loss: 2.80518 \tLatent loss: 2.50856e-06\n",
      "3400 Train total loss: 3.03983 \tReconstruction loss: 3.03982 \tLatent loss: 3.4747e-06\n",
      "3401 Train total loss: 2.80588 \tReconstruction loss: 2.80588 \tLatent loss: 5.81727e-07\n",
      "3402 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 1.07703e-06\n",
      "3403 Train total loss: 2.86598 \tReconstruction loss: 2.86598 \tLatent loss: 2.35672e-06\n",
      "3404 Train total loss: 3.0908 \tReconstruction loss: 3.0908 \tLatent loss: 1.19308e-06\n",
      "3405 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 7.01027e-07\n",
      "3406 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.86894e-06\n",
      "3407 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.6534e-06\n",
      "3408 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 8.92328e-07\n",
      "3409 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.06476e-06\n",
      "3410 Train total loss: 2.98598 \tReconstruction loss: 2.98597 \tLatent loss: 3.23327e-06\n",
      "3411 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 9.19205e-07\n",
      "3412 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.13184e-06\n",
      "3413 Train total loss: 2.97586 \tReconstruction loss: 2.97585 \tLatent loss: 4.6936e-06\n",
      "3414 Train total loss: 3.02176 \tReconstruction loss: 3.02176 \tLatent loss: 2.74593e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3415 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 7.28128e-07\n",
      "3416 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.44642e-06\n",
      "3417 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.68111e-06\n",
      "3418 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 1.28737e-06\n",
      "3419 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 1.97192e-06\n",
      "3420 Train total loss: 3.03978 \tReconstruction loss: 3.03977 \tLatent loss: 2.01123e-06\n",
      "3421 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 4.49762e-07\n",
      "3422 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 5.63724e-07\n",
      "3423 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.11044e-06\n",
      "3424 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 1.45942e-06\n",
      "3425 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.27978e-07\n",
      "3426 Train total loss: 2.80909 \tReconstruction loss: 2.80909 \tLatent loss: 1.36253e-06\n",
      "3427 Train total loss: 2.92517 \tReconstruction loss: 2.92517 \tLatent loss: 1.44089e-06\n",
      "3428 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.12226e-07\n",
      "3429 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.48961e-06\n",
      "3430 Train total loss: 2.98599 \tReconstruction loss: 2.98598 \tLatent loss: 3.03754e-06\n",
      "3431 Train total loss: 2.9464 \tReconstruction loss: 2.9464 \tLatent loss: 9.62504e-07\n",
      "3432 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.48873e-06\n",
      "3433 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 2.59502e-06\n",
      "3434 Train total loss: 3.02177 \tReconstruction loss: 3.02176 \tLatent loss: 1.78535e-06\n",
      "3435 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.92048e-07\n",
      "3436 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.74512e-06\n",
      "3437 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.73025e-06\n",
      "3438 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.96429e-07\n",
      "3439 Train total loss: 2.80519 \tReconstruction loss: 2.80519 \tLatent loss: 1.16163e-06\n",
      "3440 Train total loss: 3.03976 \tReconstruction loss: 3.03976 \tLatent loss: 1.91194e-06\n",
      "3441 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.10656e-06\n",
      "3442 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 4.58311e-07\n",
      "3443 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.81093e-06\n",
      "3444 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.86952e-06\n",
      "3445 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.8419e-07\n",
      "3446 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 8.63132e-07\n",
      "3447 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.63711e-06\n",
      "3448 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.37126e-07\n",
      "3449 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 8.79171e-07\n",
      "3450 Train total loss: 2.98597 \tReconstruction loss: 2.98597 \tLatent loss: 1.65034e-06\n",
      "3451 Train total loss: 2.94644 \tReconstruction loss: 2.94644 \tLatent loss: 1.68688e-06\n",
      "3452 Train total loss: 2.92838 \tReconstruction loss: 2.92838 \tLatent loss: 5.70931e-07\n",
      "3453 Train total loss: 2.97581 \tReconstruction loss: 2.9758 \tLatent loss: 2.24654e-06\n",
      "3454 Train total loss: 3.02176 \tReconstruction loss: 3.02176 \tLatent loss: 2.27065e-06\n",
      "3455 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 5.52734e-07\n",
      "3456 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 7.49558e-07\n",
      "3457 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 1.74351e-06\n",
      "3458 Train total loss: 2.77431 \tReconstruction loss: 2.77431 \tLatent loss: 1.07586e-06\n",
      "3459 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 4.81984e-07\n",
      "3460 Train total loss: 3.03977 \tReconstruction loss: 3.03977 \tLatent loss: 1.32514e-06\n",
      "3461 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.32156e-06\n",
      "3462 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 5.143e-07\n",
      "3463 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 9.11849e-07\n",
      "3464 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.71453e-06\n",
      "3465 Train total loss: 2.91471 \tReconstruction loss: 2.91471 \tLatent loss: 9.61486e-07\n",
      "3466 Train total loss: 2.8091 \tReconstruction loss: 2.8091 \tLatent loss: 4.4702e-07\n",
      "3467 Train total loss: 2.92512 \tReconstruction loss: 2.92511 \tLatent loss: 1.11051e-06\n",
      "3468 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 7.40241e-07\n",
      "3469 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.96352e-07\n",
      "3470 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 9.69676e-07\n",
      "3471 Train total loss: 2.94642 \tReconstruction loss: 2.94642 \tLatent loss: 6.02278e-07\n",
      "3472 Train total loss: 2.92838 \tReconstruction loss: 2.92838 \tLatent loss: 4.49154e-07\n",
      "3473 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 1.24136e-06\n",
      "3474 Train total loss: 3.02176 \tReconstruction loss: 3.02176 \tLatent loss: 1.2604e-06\n",
      "3475 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 6.73978e-07\n",
      "3476 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.34652e-07\n",
      "3477 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 7.58306e-07\n",
      "3478 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 6.98076e-07\n",
      "3479 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.11335e-07\n",
      "3480 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 1.24545e-06\n",
      "3481 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 7.48367e-07\n",
      "3482 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.42287e-07\n",
      "3483 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 7.1348e-07\n",
      "3484 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 7.2128e-07\n",
      "3485 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.02406e-06\n",
      "3486 Train total loss: 2.80909 \tReconstruction loss: 2.80909 \tLatent loss: 7.12322e-07\n",
      "3487 Train total loss: 2.92517 \tReconstruction loss: 2.92517 \tLatent loss: 5.66986e-07\n",
      "3488 Train total loss: 2.85421 \tReconstruction loss: 2.8542 \tLatent loss: 6.59321e-07\n",
      "3489 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 5.76919e-07\n",
      "3490 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 5.82714e-07\n",
      "3491 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 7.14642e-07\n",
      "3492 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 3.55538e-07\n",
      "3493 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 8.93393e-07\n",
      "3494 Train total loss: 3.02183 \tReconstruction loss: 3.02183 \tLatent loss: 1.13366e-06\n",
      "3495 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 5.74933e-07\n",
      "3496 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.99735e-07\n",
      "3497 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 5.16247e-07\n",
      "3498 Train total loss: 2.77427 \tReconstruction loss: 2.77427 \tLatent loss: 6.21878e-07\n",
      "3499 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.60139e-07\n",
      "3500 Train total loss: 3.03977 \tReconstruction loss: 3.03977 \tLatent loss: 9.25981e-07\n",
      "3501 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 7.18642e-07\n",
      "3502 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 4.47349e-07\n",
      "3503 Train total loss: 2.86598 \tReconstruction loss: 2.86598 \tLatent loss: 6.07093e-07\n",
      "3504 Train total loss: 3.09075 \tReconstruction loss: 3.09075 \tLatent loss: 3.72523e-07\n",
      "3505 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 1.26175e-06\n",
      "3506 Train total loss: 2.80909 \tReconstruction loss: 2.80909 \tLatent loss: 4.80381e-07\n",
      "3507 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 4.54763e-07\n",
      "3508 Train total loss: 2.85424 \tReconstruction loss: 2.85423 \tLatent loss: 1.19744e-06\n",
      "3509 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 5.03661e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3510 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 7.29581e-07\n",
      "3511 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 9.96265e-07\n",
      "3512 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 7.39731e-07\n",
      "3513 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 7.26484e-07\n",
      "3514 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.26379e-07\n",
      "3515 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.62161e-07\n",
      "3516 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.47566e-07\n",
      "3517 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.39652e-07\n",
      "3518 Train total loss: 2.77427 \tReconstruction loss: 2.77427 \tLatent loss: 6.49638e-07\n",
      "3519 Train total loss: 2.80519 \tReconstruction loss: 2.80519 \tLatent loss: 3.5191e-07\n",
      "3520 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 6.38681e-07\n",
      "3521 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 7.04064e-07\n",
      "3522 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 4.90606e-07\n",
      "3523 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.05491e-06\n",
      "3524 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 4.21889e-07\n",
      "3525 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 8.86346e-07\n",
      "3526 Train total loss: 2.80909 \tReconstruction loss: 2.80909 \tLatent loss: 5.07393e-07\n",
      "3527 Train total loss: 2.92517 \tReconstruction loss: 2.92517 \tLatent loss: 3.95541e-07\n",
      "3528 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 8.55824e-07\n",
      "3529 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 3.98309e-07\n",
      "3530 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 5.12642e-07\n",
      "3531 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.07251e-06\n",
      "3532 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.4945e-07\n",
      "3533 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.02871e-07\n",
      "3534 Train total loss: 3.02178 \tReconstruction loss: 3.02178 \tLatent loss: 9.51433e-07\n",
      "3535 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.63331e-07\n",
      "3536 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 3.1737e-07\n",
      "3537 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.51542e-07\n",
      "3538 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.719e-07\n",
      "3539 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 4.75757e-07\n",
      "3540 Train total loss: 3.03982 \tReconstruction loss: 3.03982 \tLatent loss: 7.13381e-07\n",
      "3541 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 4.9901e-07\n",
      "3542 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 6.99419e-07\n",
      "3543 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 8.26208e-07\n",
      "3544 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 5.76595e-07\n",
      "3545 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 5.7095e-07\n",
      "3546 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 9.18022e-07\n",
      "3547 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 3.42776e-07\n",
      "3548 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 4.41773e-07\n",
      "3549 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.43409e-07\n",
      "3550 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.81276e-07\n",
      "3551 Train total loss: 2.94644 \tReconstruction loss: 2.94644 \tLatent loss: 7.54057e-07\n",
      "3552 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 7.65573e-07\n",
      "3553 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 8.52543e-07\n",
      "3554 Train total loss: 3.0218 \tReconstruction loss: 3.02179 \tLatent loss: 8.20024e-07\n",
      "3555 Train total loss: 2.79366 \tReconstruction loss: 2.79366 \tLatent loss: 4.36627e-07\n",
      "3556 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.92736e-07\n",
      "3557 Train total loss: 2.79716 \tReconstruction loss: 2.79715 \tLatent loss: 4.52189e-07\n",
      "3558 Train total loss: 2.77431 \tReconstruction loss: 2.77431 \tLatent loss: 6.74895e-07\n",
      "3559 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 3.13167e-07\n",
      "3560 Train total loss: 3.03977 \tReconstruction loss: 3.03977 \tLatent loss: 1.19707e-06\n",
      "3561 Train total loss: 2.80588 \tReconstruction loss: 2.80588 \tLatent loss: 3.21593e-07\n",
      "3562 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.18549e-07\n",
      "3563 Train total loss: 2.86598 \tReconstruction loss: 2.86598 \tLatent loss: 3.25692e-07\n",
      "3564 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 5.48917e-07\n",
      "3565 Train total loss: 2.9147 \tReconstruction loss: 2.9147 \tLatent loss: 6.63058e-07\n",
      "3566 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 4.13827e-07\n",
      "3567 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.5716e-07\n",
      "3568 Train total loss: 2.8542 \tReconstruction loss: 2.8542 \tLatent loss: 7.11846e-07\n",
      "3569 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.54888e-07\n",
      "3570 Train total loss: 2.98598 \tReconstruction loss: 2.98598 \tLatent loss: 4.23696e-07\n",
      "3571 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 5.59712e-07\n",
      "3572 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.24329e-07\n",
      "3573 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 4.19482e-07\n",
      "3574 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 6.46865e-07\n",
      "3575 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 1.12903e-06\n",
      "3576 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 6.10226e-07\n",
      "3577 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 4.98557e-07\n",
      "3578 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 1.34281e-06\n",
      "3579 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 4.74874e-07\n",
      "3580 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 5.89929e-07\n",
      "3581 Train total loss: 2.80588 \tReconstruction loss: 2.80588 \tLatent loss: 2.59389e-07\n",
      "3582 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 4.53102e-07\n",
      "3583 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.6325e-07\n",
      "3584 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 5.37693e-07\n",
      "3585 Train total loss: 2.91469 \tReconstruction loss: 2.91469 \tLatent loss: 4.45436e-07\n",
      "3586 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 5.26171e-07\n",
      "3587 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.45046e-07\n",
      "3588 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 7.20393e-07\n",
      "3589 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 5.27529e-07\n",
      "3590 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 4.82434e-07\n",
      "3591 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.999e-07\n",
      "3592 Train total loss: 2.92838 \tReconstruction loss: 2.92838 \tLatent loss: 3.90012e-07\n",
      "3593 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.07596e-07\n",
      "3594 Train total loss: 3.02188 \tReconstruction loss: 3.02188 \tLatent loss: 4.0279e-07\n",
      "3595 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.21533e-07\n",
      "3596 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 7.56703e-07\n",
      "3597 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 4.24348e-07\n",
      "3598 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 9.98846e-07\n",
      "3599 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.69057e-07\n",
      "3600 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 9.39335e-07\n",
      "3601 Train total loss: 2.80585 \tReconstruction loss: 2.80585 \tLatent loss: 2.15103e-07\n",
      "3602 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 3.75364e-07\n",
      "3603 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.79593e-07\n",
      "3604 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.0904e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3605 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 2.83847e-07\n",
      "3606 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 4.41266e-07\n",
      "3607 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 3.55116e-07\n",
      "3608 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.43612e-07\n",
      "3609 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 6.04818e-07\n",
      "3610 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 3.19707e-07\n",
      "3611 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 8.05769e-07\n",
      "3612 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.89258e-07\n",
      "3613 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 4.17305e-07\n",
      "3614 Train total loss: 3.02184 \tReconstruction loss: 3.02184 \tLatent loss: 3.48598e-07\n",
      "3615 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.70727e-07\n",
      "3616 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 7.86749e-07\n",
      "3617 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 3.29259e-07\n",
      "3618 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 7.33577e-07\n",
      "3619 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.46057e-07\n",
      "3620 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 7.20643e-07\n",
      "3621 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.04541e-07\n",
      "3622 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 4.0078e-07\n",
      "3623 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.55053e-07\n",
      "3624 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 4.42629e-07\n",
      "3625 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.86377e-07\n",
      "3626 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.34537e-07\n",
      "3627 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.4663e-07\n",
      "3628 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.81441e-07\n",
      "3629 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 6.89519e-07\n",
      "3630 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 4.52835e-07\n",
      "3631 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.57815e-06\n",
      "3632 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.93996e-07\n",
      "3633 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 8.63874e-07\n",
      "3634 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 6.36361e-07\n",
      "3635 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.30344e-07\n",
      "3636 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 6.29878e-07\n",
      "3637 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.95727e-07\n",
      "3638 Train total loss: 2.77428 \tReconstruction loss: 2.77427 \tLatent loss: 1.09065e-06\n",
      "3639 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 5.99147e-07\n",
      "3640 Train total loss: 3.03983 \tReconstruction loss: 3.03983 \tLatent loss: 1.25701e-06\n",
      "3641 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 4.02181e-07\n",
      "3642 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.84381e-07\n",
      "3643 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 6.09002e-07\n",
      "3644 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 7.74239e-07\n",
      "3645 Train total loss: 2.91473 \tReconstruction loss: 2.91473 \tLatent loss: 6.58388e-07\n",
      "3646 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 6.11632e-07\n",
      "3647 Train total loss: 2.92518 \tReconstruction loss: 2.92518 \tLatent loss: 2.8233e-07\n",
      "3648 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.34399e-07\n",
      "3649 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 5.52268e-07\n",
      "3650 Train total loss: 2.98597 \tReconstruction loss: 2.98597 \tLatent loss: 3.89878e-07\n",
      "3651 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 8.83991e-07\n",
      "3652 Train total loss: 2.92838 \tReconstruction loss: 2.92838 \tLatent loss: 4.25952e-07\n",
      "3653 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 6.81408e-07\n",
      "3654 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 8.17441e-07\n",
      "3655 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.62911e-07\n",
      "3656 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 3.74336e-07\n",
      "3657 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.60345e-07\n",
      "3658 Train total loss: 2.7743 \tReconstruction loss: 2.77429 \tLatent loss: 8.76424e-07\n",
      "3659 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.50097e-07\n",
      "3660 Train total loss: 3.03977 \tReconstruction loss: 3.03977 \tLatent loss: 7.66071e-07\n",
      "3661 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.48292e-07\n",
      "3662 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 4.83268e-07\n",
      "3663 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 5.96362e-07\n",
      "3664 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 4.28043e-07\n",
      "3665 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 4.69571e-07\n",
      "3666 Train total loss: 2.80905 \tReconstruction loss: 2.80905 \tLatent loss: 2.74264e-07\n",
      "3667 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.20995e-07\n",
      "3668 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 8.85582e-07\n",
      "3669 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 9.26404e-07\n",
      "3670 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 4.23154e-07\n",
      "3671 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.2148e-06\n",
      "3672 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.14412e-07\n",
      "3673 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 6.09242e-07\n",
      "3674 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 5.92847e-07\n",
      "3675 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.13743e-07\n",
      "3676 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.26118e-07\n",
      "3677 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.36175e-07\n",
      "3678 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.44277e-07\n",
      "3679 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.16612e-07\n",
      "3680 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 6.66062e-07\n",
      "3681 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.8043e-07\n",
      "3682 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.32183e-07\n",
      "3683 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 3.13002e-07\n",
      "3684 Train total loss: 3.09077 \tReconstruction loss: 3.09076 \tLatent loss: 6.20689e-07\n",
      "3685 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.76462e-07\n",
      "3686 Train total loss: 2.80909 \tReconstruction loss: 2.80909 \tLatent loss: 4.48329e-07\n",
      "3687 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.54329e-07\n",
      "3688 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.98213e-07\n",
      "3689 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 4.3211e-07\n",
      "3690 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.50134e-07\n",
      "3691 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 8.93511e-07\n",
      "3692 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.37876e-07\n",
      "3693 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 8.05417e-07\n",
      "3694 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.85586e-07\n",
      "3695 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.62932e-07\n",
      "3696 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 7.4938e-07\n",
      "3697 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.0255e-07\n",
      "3698 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.53118e-07\n",
      "3699 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 2.27e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 9.95667e-07\n",
      "3701 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.1449e-07\n",
      "3702 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.16435e-07\n",
      "3703 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.5392e-07\n",
      "3704 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 5.74783e-07\n",
      "3705 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.35462e-07\n",
      "3706 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 8.33991e-07\n",
      "3707 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 4.33775e-07\n",
      "3708 Train total loss: 2.85424 \tReconstruction loss: 2.85424 \tLatent loss: 3.99628e-07\n",
      "3709 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.50167e-07\n",
      "3710 Train total loss: 2.98598 \tReconstruction loss: 2.98598 \tLatent loss: 5.74169e-07\n",
      "3711 Train total loss: 2.94644 \tReconstruction loss: 2.94644 \tLatent loss: 5.92071e-07\n",
      "3712 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.45511e-07\n",
      "3713 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.21682e-07\n",
      "3714 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.93674e-07\n",
      "3715 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 6.7525e-07\n",
      "3716 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 6.57301e-07\n",
      "3717 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 4.98198e-07\n",
      "3718 Train total loss: 2.77427 \tReconstruction loss: 2.77427 \tLatent loss: 5.7139e-07\n",
      "3719 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.84094e-07\n",
      "3720 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 4.80523e-07\n",
      "3721 Train total loss: 2.80585 \tReconstruction loss: 2.80585 \tLatent loss: 2.26463e-07\n",
      "3722 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 4.30539e-07\n",
      "3723 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 7.414e-07\n",
      "3724 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 3.98382e-07\n",
      "3725 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.39724e-07\n",
      "3726 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.58272e-07\n",
      "3727 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.88721e-07\n",
      "3728 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.58456e-07\n",
      "3729 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 5.09102e-07\n",
      "3730 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.08335e-07\n",
      "3731 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.01918e-06\n",
      "3732 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.12102e-07\n",
      "3733 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 8.77655e-07\n",
      "3734 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 3.05451e-07\n",
      "3735 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.12038e-07\n",
      "3736 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.9758e-07\n",
      "3737 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.58634e-07\n",
      "3738 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.48676e-06\n",
      "3739 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.89032e-07\n",
      "3740 Train total loss: 3.0398 \tReconstruction loss: 3.03979 \tLatent loss: 1.36479e-06\n",
      "3741 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.78986e-07\n",
      "3742 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 2.41247e-07\n",
      "3743 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.54395e-07\n",
      "3744 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.75607e-07\n",
      "3745 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.02227e-07\n",
      "3746 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.32098e-07\n",
      "3747 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 2.67629e-07\n",
      "3748 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.67926e-07\n",
      "3749 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.97748e-07\n",
      "3750 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.25265e-07\n",
      "3751 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 7.2589e-07\n",
      "3752 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.96001e-07\n",
      "3753 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.66257e-07\n",
      "3754 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 2.26081e-07\n",
      "3755 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.41351e-07\n",
      "3756 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 6.00726e-07\n",
      "3757 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.21322e-07\n",
      "3758 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.62269e-07\n",
      "3759 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.42281e-07\n",
      "3760 Train total loss: 3.03984 \tReconstruction loss: 3.03984 \tLatent loss: 3.40254e-07\n",
      "3761 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.99833e-07\n",
      "3762 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.88872e-07\n",
      "3763 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.77083e-07\n",
      "3764 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 6.03098e-07\n",
      "3765 Train total loss: 2.91473 \tReconstruction loss: 2.91473 \tLatent loss: 3.22849e-07\n",
      "3766 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.13544e-07\n",
      "3767 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.81095e-07\n",
      "3768 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.85015e-07\n",
      "3769 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.23239e-07\n",
      "3770 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 8.67419e-07\n",
      "3771 Train total loss: 2.94646 \tReconstruction loss: 2.94645 \tLatent loss: 9.07521e-07\n",
      "3772 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 3.48622e-07\n",
      "3773 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.12598e-06\n",
      "3774 Train total loss: 3.02179 \tReconstruction loss: 3.02178 \tLatent loss: 4.99422e-07\n",
      "3775 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.86478e-07\n",
      "3776 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.82707e-07\n",
      "3777 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 1.82385e-07\n",
      "3778 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 9.65514e-07\n",
      "3779 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 2.34268e-07\n",
      "3780 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.7028e-07\n",
      "3781 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.56713e-07\n",
      "3782 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.57813e-07\n",
      "3783 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.84251e-07\n",
      "3784 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.03279e-07\n",
      "3785 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.01365e-07\n",
      "3786 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.38053e-07\n",
      "3787 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 4.00271e-07\n",
      "3788 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 6.75261e-07\n",
      "3789 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.38184e-07\n",
      "3790 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 3.79913e-07\n",
      "3791 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 6.42968e-07\n",
      "3792 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.3964e-07\n",
      "3793 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 4.86473e-07\n",
      "3794 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 8.24618e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.06305e-07\n",
      "3796 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 6.03122e-07\n",
      "3797 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.70103e-07\n",
      "3798 Train total loss: 2.77427 \tReconstruction loss: 2.77427 \tLatent loss: 8.27367e-07\n",
      "3799 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.98519e-07\n",
      "3800 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.45934e-07\n",
      "3801 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 4.27749e-07\n",
      "3802 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 2.81214e-07\n",
      "3803 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 4.0017e-07\n",
      "3804 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.15473e-07\n",
      "3805 Train total loss: 2.91473 \tReconstruction loss: 2.91473 \tLatent loss: 3.34287e-07\n",
      "3806 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 2.3121e-07\n",
      "3807 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.52126e-07\n",
      "3808 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.52493e-07\n",
      "3809 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.94255e-07\n",
      "3810 Train total loss: 2.98601 \tReconstruction loss: 2.986 \tLatent loss: 4.0701e-07\n",
      "3811 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.40056e-07\n",
      "3812 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.51232e-07\n",
      "3813 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 5.53089e-07\n",
      "3814 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 3.08374e-07\n",
      "3815 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 4.73401e-07\n",
      "3816 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.45028e-07\n",
      "3817 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 3.76481e-07\n",
      "3818 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 6.26029e-07\n",
      "3819 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.1139e-07\n",
      "3820 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 6.89973e-07\n",
      "3821 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.05986e-07\n",
      "3822 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.22304e-07\n",
      "3823 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 4.31173e-07\n",
      "3824 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.70329e-07\n",
      "3825 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.09514e-07\n",
      "3826 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.89112e-07\n",
      "3827 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.83172e-07\n",
      "3828 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 4.17068e-07\n",
      "3829 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.41789e-07\n",
      "3830 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 5.48418e-07\n",
      "3831 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 6.32121e-07\n",
      "3832 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.98839e-07\n",
      "3833 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.04611e-07\n",
      "3834 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 3.83414e-07\n",
      "3835 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.03546e-07\n",
      "3836 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 5.05314e-07\n",
      "3837 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.37313e-07\n",
      "3838 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.77336e-07\n",
      "3839 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.46261e-07\n",
      "3840 Train total loss: 3.03979 \tReconstruction loss: 3.03978 \tLatent loss: 9.89027e-07\n",
      "3841 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.23148e-07\n",
      "3842 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.01161e-07\n",
      "3843 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.05254e-07\n",
      "3844 Train total loss: 3.09075 \tReconstruction loss: 3.09075 \tLatent loss: 3.90771e-07\n",
      "3845 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.84797e-07\n",
      "3846 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.43566e-07\n",
      "3847 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.22347e-07\n",
      "3848 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.50829e-07\n",
      "3849 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.41188e-07\n",
      "3850 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 7.51144e-07\n",
      "3851 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.10388e-07\n",
      "3852 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 2.18059e-07\n",
      "3853 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.26395e-07\n",
      "3854 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 3.66841e-07\n",
      "3855 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.99666e-07\n",
      "3856 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 4.70178e-07\n",
      "3857 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.26768e-07\n",
      "3858 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 5.76489e-07\n",
      "3859 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.6648e-07\n",
      "3860 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 6.65462e-07\n",
      "3861 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.55277e-07\n",
      "3862 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.30494e-07\n",
      "3863 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.65367e-07\n",
      "3864 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 4.42396e-07\n",
      "3865 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.66487e-07\n",
      "3866 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.20235e-07\n",
      "3867 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 5.22565e-07\n",
      "3868 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.8461e-07\n",
      "3869 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.16188e-07\n",
      "3870 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 5.26573e-07\n",
      "3871 Train total loss: 2.94647 \tReconstruction loss: 2.94646 \tLatent loss: 8.80717e-07\n",
      "3872 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.2576e-07\n",
      "3873 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 4.94682e-07\n",
      "3874 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 2.34583e-07\n",
      "3875 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.09842e-07\n",
      "3876 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.67504e-07\n",
      "3877 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.99214e-07\n",
      "3878 Train total loss: 2.77427 \tReconstruction loss: 2.77427 \tLatent loss: 4.16144e-07\n",
      "3879 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.03252e-07\n",
      "3880 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 6.30677e-07\n",
      "3881 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.3523e-07\n",
      "3882 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 2.84518e-07\n",
      "3883 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.72646e-07\n",
      "3884 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.65751e-07\n",
      "3885 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 2.07241e-07\n",
      "3886 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.92771e-07\n",
      "3887 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.82626e-07\n",
      "3888 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.36347e-07\n",
      "3889 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.36942e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3890 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 4.42413e-07\n",
      "3891 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 7.98888e-07\n",
      "3892 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 3.343e-07\n",
      "3893 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 1.09566e-06\n",
      "3894 Train total loss: 3.02183 \tReconstruction loss: 3.02182 \tLatent loss: 9.71096e-07\n",
      "3895 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.06003e-07\n",
      "3896 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.80624e-07\n",
      "3897 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.48765e-07\n",
      "3898 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 9.05787e-07\n",
      "3899 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.50343e-07\n",
      "3900 Train total loss: 3.03983 \tReconstruction loss: 3.03983 \tLatent loss: 5.48319e-07\n",
      "3901 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.7527e-07\n",
      "3902 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.48638e-07\n",
      "3903 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.77049e-07\n",
      "3904 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.42078e-07\n",
      "3905 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.21538e-07\n",
      "3906 Train total loss: 2.80909 \tReconstruction loss: 2.80908 \tLatent loss: 5.4932e-07\n",
      "3907 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.8768e-07\n",
      "3908 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.56926e-07\n",
      "3909 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.60088e-07\n",
      "3910 Train total loss: 2.98599 \tReconstruction loss: 2.98598 \tLatent loss: 6.28317e-07\n",
      "3911 Train total loss: 2.94644 \tReconstruction loss: 2.94644 \tLatent loss: 6.08362e-07\n",
      "3912 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.22896e-07\n",
      "3913 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.88487e-07\n",
      "3914 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.22245e-07\n",
      "3915 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.90223e-07\n",
      "3916 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 5.53817e-07\n",
      "3917 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.76868e-07\n",
      "3918 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 6.40756e-07\n",
      "3919 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.44559e-07\n",
      "3920 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 4.54884e-07\n",
      "3921 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.12876e-07\n",
      "3922 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.26576e-07\n",
      "3923 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 3.12814e-07\n",
      "3924 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 3.57645e-07\n",
      "3925 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.46615e-07\n",
      "3926 Train total loss: 2.80909 \tReconstruction loss: 2.80909 \tLatent loss: 3.89107e-07\n",
      "3927 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 4.52487e-07\n",
      "3928 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.93183e-07\n",
      "3929 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.80669e-07\n",
      "3930 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 5.12577e-07\n",
      "3931 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.24845e-07\n",
      "3932 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.17383e-07\n",
      "3933 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 4.71484e-07\n",
      "3934 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 3.97887e-07\n",
      "3935 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.41483e-07\n",
      "3936 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.32652e-07\n",
      "3937 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.35076e-07\n",
      "3938 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 5.79999e-07\n",
      "3939 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.27206e-07\n",
      "3940 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.07363e-07\n",
      "3941 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.66579e-07\n",
      "3942 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.87814e-07\n",
      "3943 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.66084e-07\n",
      "3944 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 4.06237e-07\n",
      "3945 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.21751e-07\n",
      "3946 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.06808e-07\n",
      "3947 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 2.22791e-07\n",
      "3948 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.7245e-07\n",
      "3949 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 3.51458e-07\n",
      "3950 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.86788e-07\n",
      "3951 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.61317e-07\n",
      "3952 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.5476e-07\n",
      "3953 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.92236e-07\n",
      "3954 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.53796e-07\n",
      "3955 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.50611e-07\n",
      "3956 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.06249e-07\n",
      "3957 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.614e-07\n",
      "3958 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 6.52426e-07\n",
      "3959 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.61616e-07\n",
      "3960 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 5.14853e-07\n",
      "3961 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.24647e-07\n",
      "3962 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.85385e-07\n",
      "3963 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 4.03689e-07\n",
      "3964 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 1.86004e-07\n",
      "3965 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.16119e-07\n",
      "3966 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.73784e-07\n",
      "3967 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.05645e-07\n",
      "3968 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.39957e-07\n",
      "3969 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.19459e-07\n",
      "3970 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.31015e-07\n",
      "3971 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.01848e-07\n",
      "3972 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.0994e-07\n",
      "3973 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 7.10829e-07\n",
      "3974 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.82356e-07\n",
      "3975 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.9872e-07\n",
      "3976 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 3.44712e-07\n",
      "3977 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 1.77823e-07\n",
      "3978 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.28816e-07\n",
      "3979 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.71537e-07\n",
      "3980 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.14442e-07\n",
      "3981 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.4641e-07\n",
      "3982 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.02119e-07\n",
      "3983 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.96931e-07\n",
      "3984 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.53692e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3985 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.74607e-07\n",
      "3986 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.14866e-07\n",
      "3987 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.44917e-07\n",
      "3988 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.2569e-07\n",
      "3989 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.09997e-07\n",
      "3990 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 5.29039e-07\n",
      "3991 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.88596e-07\n",
      "3992 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.61493e-07\n",
      "3993 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.72893e-07\n",
      "3994 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.11862e-07\n",
      "3995 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 6.3187e-07\n",
      "3996 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.79489e-07\n",
      "3997 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 2.77563e-07\n",
      "3998 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 8.7083e-07\n",
      "3999 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.95403e-07\n",
      "4000 Train total loss: 3.03977 \tReconstruction loss: 3.03977 \tLatent loss: 3.628e-07\n",
      "4001 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.34249e-07\n",
      "4002 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.45361e-07\n",
      "4003 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.26426e-07\n",
      "4004 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.49787e-07\n",
      "4005 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.98104e-07\n",
      "4006 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.34693e-07\n",
      "4007 Train total loss: 2.92515 \tReconstruction loss: 2.92514 \tLatent loss: 1.83951e-07\n",
      "4008 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.76538e-07\n",
      "4009 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.99268e-07\n",
      "4010 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.74286e-07\n",
      "4011 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.5833e-07\n",
      "4012 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.77239e-07\n",
      "4013 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.80008e-07\n",
      "4014 Train total loss: 3.02181 \tReconstruction loss: 3.0218 \tLatent loss: 8.14095e-07\n",
      "4015 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.51059e-07\n",
      "4016 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 7.67404e-07\n",
      "4017 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.90859e-07\n",
      "4018 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.72259e-07\n",
      "4019 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.65555e-07\n",
      "4020 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.61753e-07\n",
      "4021 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.90883e-07\n",
      "4022 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.50823e-07\n",
      "4023 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.89415e-07\n",
      "4024 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.12598e-07\n",
      "4025 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.6312e-07\n",
      "4026 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.86575e-07\n",
      "4027 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.87129e-07\n",
      "4028 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.5242e-07\n",
      "4029 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 9.43313e-08\n",
      "4030 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.03902e-07\n",
      "4031 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.78399e-07\n",
      "4032 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.82122e-07\n",
      "4033 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.03221e-07\n",
      "4034 Train total loss: 3.02183 \tReconstruction loss: 3.02183 \tLatent loss: 3.02249e-07\n",
      "4035 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.25871e-07\n",
      "4036 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 6.53381e-07\n",
      "4037 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.08075e-07\n",
      "4038 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.56601e-07\n",
      "4039 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.28115e-07\n",
      "4040 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.34704e-07\n",
      "4041 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.01829e-07\n",
      "4042 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.38012e-07\n",
      "4043 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.02831e-07\n",
      "4044 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.85467e-07\n",
      "4045 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.89566e-07\n",
      "4046 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.16022e-07\n",
      "4047 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.68871e-07\n",
      "4048 Train total loss: 2.8542 \tReconstruction loss: 2.8542 \tLatent loss: 4.29392e-07\n",
      "4049 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.68916e-07\n",
      "4050 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 4.48746e-07\n",
      "4051 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.87833e-07\n",
      "4052 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.53206e-07\n",
      "4053 Train total loss: 2.97585 \tReconstruction loss: 2.97584 \tLatent loss: 5.86614e-07\n",
      "4054 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 5.21171e-07\n",
      "4055 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.9339e-07\n",
      "4056 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.80513e-07\n",
      "4057 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.95048e-07\n",
      "4058 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.93226e-07\n",
      "4059 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.58654e-07\n",
      "4060 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 7.88126e-07\n",
      "4061 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.28903e-07\n",
      "4062 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.7488e-07\n",
      "4063 Train total loss: 2.86598 \tReconstruction loss: 2.86598 \tLatent loss: 4.41552e-07\n",
      "4064 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.02553e-07\n",
      "4065 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.17457e-07\n",
      "4066 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.9759e-07\n",
      "4067 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.42393e-07\n",
      "4068 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.16487e-07\n",
      "4069 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.66537e-07\n",
      "4070 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.48227e-07\n",
      "4071 Train total loss: 2.94647 \tReconstruction loss: 2.94646 \tLatent loss: 5.2391e-07\n",
      "4072 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.97215e-07\n",
      "4073 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 5.6482e-07\n",
      "4074 Train total loss: 3.02185 \tReconstruction loss: 3.02185 \tLatent loss: 2.28558e-07\n",
      "4075 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.52219e-07\n",
      "4076 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 5.43817e-07\n",
      "4077 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.69297e-07\n",
      "4078 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 6.14789e-07\n",
      "4079 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.17246e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 7.58062e-07\n",
      "4081 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.3306e-07\n",
      "4082 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.31355e-07\n",
      "4083 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 5.62822e-07\n",
      "4084 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 3.3086e-07\n",
      "4085 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.79385e-07\n",
      "4086 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.76211e-07\n",
      "4087 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.32591e-07\n",
      "4088 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.36775e-07\n",
      "4089 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.62592e-07\n",
      "4090 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 7.24677e-07\n",
      "4091 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 7.97986e-07\n",
      "4092 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.82107e-07\n",
      "4093 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.72785e-07\n",
      "4094 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 5.11814e-07\n",
      "4095 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 3.60118e-07\n",
      "4096 Train total loss: 2.77375 \tReconstruction loss: 2.77375 \tLatent loss: 3.81017e-07\n",
      "4097 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.7986e-07\n",
      "4098 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.37426e-07\n",
      "4099 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.1895e-07\n",
      "4100 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 6.87153e-07\n",
      "4101 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.2708e-07\n",
      "4102 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.75312e-07\n",
      "4103 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 7.53657e-07\n",
      "4104 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 2.79536e-07\n",
      "4105 Train total loss: 2.91472 \tReconstruction loss: 2.91472 \tLatent loss: 3.75208e-07\n",
      "4106 Train total loss: 2.80909 \tReconstruction loss: 2.80908 \tLatent loss: 5.3635e-07\n",
      "4107 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 2.99406e-07\n",
      "4108 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.30841e-07\n",
      "4109 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.22184e-07\n",
      "4110 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 8.17791e-07\n",
      "4111 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.89438e-07\n",
      "4112 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.36493e-07\n",
      "4113 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 8.16176e-07\n",
      "4114 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.8189e-07\n",
      "4115 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.61425e-07\n",
      "4116 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 6.60543e-07\n",
      "4117 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.79521e-07\n",
      "4118 Train total loss: 2.7743 \tReconstruction loss: 2.77429 \tLatent loss: 9.62528e-07\n",
      "4119 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.89977e-07\n",
      "4120 Train total loss: 3.03977 \tReconstruction loss: 3.03977 \tLatent loss: 4.47279e-07\n",
      "4121 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.17462e-07\n",
      "4122 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 1.52196e-07\n",
      "4123 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 2.53375e-07\n",
      "4124 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.1679e-07\n",
      "4125 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.20383e-07\n",
      "4126 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.37318e-07\n",
      "4127 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 4.11515e-07\n",
      "4128 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.89723e-07\n",
      "4129 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.35966e-07\n",
      "4130 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 5.03199e-07\n",
      "4131 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.36551e-07\n",
      "4132 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.83471e-07\n",
      "4133 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.40743e-07\n",
      "4134 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 2.77019e-07\n",
      "4135 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 3.83361e-07\n",
      "4136 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.21908e-07\n",
      "4137 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.84176e-07\n",
      "4138 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 5.87665e-07\n",
      "4139 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 4.28807e-07\n",
      "4140 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 6.48662e-07\n",
      "4141 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 4.48854e-07\n",
      "4142 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.58176e-07\n",
      "4143 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.21096e-07\n",
      "4144 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.84646e-07\n",
      "4145 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.10726e-07\n",
      "4146 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.93759e-07\n",
      "4147 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.04497e-07\n",
      "4148 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.67925e-07\n",
      "4149 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.18506e-07\n",
      "4150 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 6.75398e-07\n",
      "4151 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.78442e-07\n",
      "4152 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.29703e-07\n",
      "4153 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 7.01641e-07\n",
      "4154 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 8.32266e-07\n",
      "4155 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.57079e-07\n",
      "4156 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 6.22982e-07\n",
      "4157 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.37412e-07\n",
      "4158 Train total loss: 2.77426 \tReconstruction loss: 2.77426 \tLatent loss: 4.75629e-07\n",
      "4159 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.10918e-07\n",
      "4160 Train total loss: 3.03977 \tReconstruction loss: 3.03977 \tLatent loss: 4.84347e-07\n",
      "4161 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.85291e-07\n",
      "4162 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.54313e-07\n",
      "4163 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 6.33788e-07\n",
      "4164 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.81219e-07\n",
      "4165 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.95607e-07\n",
      "4166 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.21842e-07\n",
      "4167 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.1744e-07\n",
      "4168 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 3.66339e-07\n",
      "4169 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.80482e-07\n",
      "4170 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.46125e-07\n",
      "4171 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.57995e-06\n",
      "4172 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 8.26344e-07\n",
      "4173 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.30365e-07\n",
      "4174 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.93284e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4175 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.85888e-07\n",
      "4176 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 3.89866e-07\n",
      "4177 Train total loss: 2.79713 \tReconstruction loss: 2.79713 \tLatent loss: 2.64068e-07\n",
      "4178 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.0273e-07\n",
      "4179 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.35857e-07\n",
      "4180 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.3889e-07\n",
      "4181 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.15136e-07\n",
      "4182 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.07542e-07\n",
      "4183 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.74171e-07\n",
      "4184 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.77616e-07\n",
      "4185 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.48171e-07\n",
      "4186 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.26241e-07\n",
      "4187 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.5726e-07\n",
      "4188 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 7.10491e-07\n",
      "4189 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.21772e-07\n",
      "4190 Train total loss: 2.98598 \tReconstruction loss: 2.98598 \tLatent loss: 6.37278e-07\n",
      "4191 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.80787e-07\n",
      "4192 Train total loss: 2.92838 \tReconstruction loss: 2.92838 \tLatent loss: 3.1276e-07\n",
      "4193 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 6.57303e-07\n",
      "4194 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.57807e-07\n",
      "4195 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.06266e-07\n",
      "4196 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.94767e-07\n",
      "4197 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.77627e-07\n",
      "4198 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 7.02767e-07\n",
      "4199 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.53648e-07\n",
      "4200 Train total loss: 3.03979 \tReconstruction loss: 3.03978 \tLatent loss: 1.04602e-06\n",
      "4201 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.00653e-07\n",
      "4202 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.94115e-07\n",
      "4203 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.89705e-07\n",
      "4204 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.70877e-07\n",
      "4205 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.41839e-07\n",
      "4206 Train total loss: 2.80905 \tReconstruction loss: 2.80905 \tLatent loss: 1.2541e-07\n",
      "4207 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.58836e-07\n",
      "4208 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 6.49181e-07\n",
      "4209 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.28034e-07\n",
      "4210 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.10795e-06\n",
      "4211 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 5.06648e-07\n",
      "4212 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.03415e-07\n",
      "4213 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.93704e-07\n",
      "4214 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.17411e-07\n",
      "4215 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 6.32435e-07\n",
      "4216 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 5.59143e-07\n",
      "4217 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.40851e-06\n",
      "4218 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 6.09766e-07\n",
      "4219 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.56864e-07\n",
      "4220 Train total loss: 3.03982 \tReconstruction loss: 3.03982 \tLatent loss: 2.22333e-07\n",
      "4221 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.00689e-07\n",
      "4222 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 8.10993e-08\n",
      "4223 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.08239e-07\n",
      "4224 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.65598e-07\n",
      "4225 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.7202e-07\n",
      "4226 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.50458e-07\n",
      "4227 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.39133e-07\n",
      "4228 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.8063e-07\n",
      "4229 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.94316e-07\n",
      "4230 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.49567e-07\n",
      "4231 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.87078e-07\n",
      "4232 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.97735e-07\n",
      "4233 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 5.16433e-07\n",
      "4234 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 1.3953e-07\n",
      "4235 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.57369e-07\n",
      "4236 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.32967e-07\n",
      "4237 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.92957e-07\n",
      "4238 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.19739e-06\n",
      "4239 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.11333e-07\n",
      "4240 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 6.73173e-07\n",
      "4241 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.4215e-07\n",
      "4242 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.8725e-07\n",
      "4243 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.37706e-07\n",
      "4244 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.0444e-07\n",
      "4245 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.59293e-07\n",
      "4246 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.5996e-07\n",
      "4247 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.81949e-07\n",
      "4248 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.54298e-07\n",
      "4249 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.90242e-07\n",
      "4250 Train total loss: 2.98598 \tReconstruction loss: 2.98598 \tLatent loss: 3.01334e-07\n",
      "4251 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.6234e-07\n",
      "4252 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.33925e-07\n",
      "4253 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 5.27536e-07\n",
      "4254 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 9.52979e-07\n",
      "4255 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.39098e-07\n",
      "4256 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.85558e-07\n",
      "4257 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.10731e-07\n",
      "4258 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.31862e-07\n",
      "4259 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.33953e-07\n",
      "4260 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.00158e-06\n",
      "4261 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.59827e-07\n",
      "4262 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.37054e-07\n",
      "4263 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.82326e-07\n",
      "4264 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.39362e-07\n",
      "4265 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.36114e-07\n",
      "4266 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.12852e-07\n",
      "4267 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.24316e-07\n",
      "4268 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.04128e-07\n",
      "4269 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.99013e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4270 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 1.86732e-07\n",
      "4271 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.07988e-07\n",
      "4272 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.92285e-07\n",
      "4273 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.2545e-07\n",
      "4274 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.88784e-07\n",
      "4275 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.67029e-07\n",
      "4276 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.07461e-07\n",
      "4277 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.16655e-07\n",
      "4278 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.96406e-07\n",
      "4279 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.53724e-07\n",
      "4280 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.61264e-07\n",
      "4281 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.67344e-07\n",
      "4282 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.1506e-07\n",
      "4283 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.78567e-07\n",
      "4284 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.93778e-07\n",
      "4285 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.98257e-07\n",
      "4286 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 3.51953e-07\n",
      "4287 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.50133e-07\n",
      "4288 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.29602e-07\n",
      "4289 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.66348e-07\n",
      "4290 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.43511e-07\n",
      "4291 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 5.09637e-07\n",
      "4292 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.98778e-07\n",
      "4293 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 7.98845e-07\n",
      "4294 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.81727e-07\n",
      "4295 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.03919e-07\n",
      "4296 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.45616e-07\n",
      "4297 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 2.22641e-07\n",
      "4298 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.33521e-07\n",
      "4299 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.69647e-07\n",
      "4300 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 7.19741e-07\n",
      "4301 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 4.04316e-07\n",
      "4302 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.29615e-07\n",
      "4303 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.92601e-07\n",
      "4304 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.90871e-07\n",
      "4305 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.76994e-07\n",
      "4306 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.13763e-07\n",
      "4307 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.79909e-07\n",
      "4308 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.94352e-07\n",
      "4309 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 3.22814e-07\n",
      "4310 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.59659e-07\n",
      "4311 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 4.6003e-07\n",
      "4312 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.50403e-07\n",
      "4313 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 4.08087e-07\n",
      "4314 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 5.32245e-07\n",
      "4315 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.47206e-07\n",
      "4316 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.75221e-07\n",
      "4317 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.1923e-07\n",
      "4318 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.32763e-06\n",
      "4319 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.43476e-07\n",
      "4320 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.6896e-07\n",
      "4321 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.96328e-07\n",
      "4322 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.50555e-07\n",
      "4323 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 3.24404e-07\n",
      "4324 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 4.28849e-07\n",
      "4325 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.29102e-07\n",
      "4326 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.86452e-07\n",
      "4327 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.82136e-07\n",
      "4328 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 4.20227e-07\n",
      "4329 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.84568e-07\n",
      "4330 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.93781e-07\n",
      "4331 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.50556e-07\n",
      "4332 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.3626e-07\n",
      "4333 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 4.52105e-07\n",
      "4334 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.53761e-07\n",
      "4335 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.21853e-07\n",
      "4336 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 6.52843e-07\n",
      "4337 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.58236e-07\n",
      "4338 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 6.2161e-07\n",
      "4339 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.87328e-07\n",
      "4340 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 7.56321e-07\n",
      "4341 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 5.75035e-07\n",
      "4342 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.68732e-07\n",
      "4343 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.17849e-07\n",
      "4344 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.03805e-07\n",
      "4345 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.1486e-07\n",
      "4346 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.93268e-07\n",
      "4347 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.62679e-07\n",
      "4348 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.34598e-07\n",
      "4349 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.32855e-07\n",
      "4350 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.19952e-07\n",
      "4351 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.24904e-07\n",
      "4352 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.43315e-07\n",
      "4353 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 5.01325e-07\n",
      "4354 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 5.48911e-07\n",
      "4355 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.24036e-07\n",
      "4356 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.44883e-07\n",
      "4357 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.58427e-07\n",
      "4358 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 5.04381e-07\n",
      "4359 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.26966e-07\n",
      "4360 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 4.08848e-07\n",
      "4361 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.33918e-07\n",
      "4362 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 2.40289e-07\n",
      "4363 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.06252e-07\n",
      "4364 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.35626e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4365 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.7569e-07\n",
      "4366 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.71225e-07\n",
      "4367 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.83147e-07\n",
      "4368 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 2.56725e-07\n",
      "4369 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.02672e-07\n",
      "4370 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.21571e-07\n",
      "4371 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 2.51934e-07\n",
      "4372 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.41152e-07\n",
      "4373 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 4.98648e-07\n",
      "4374 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.35387e-07\n",
      "4375 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.71211e-07\n",
      "4376 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.52645e-07\n",
      "4377 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.12312e-07\n",
      "4378 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 3.72666e-07\n",
      "4379 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.7062e-07\n",
      "4380 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.19174e-07\n",
      "4381 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.08096e-07\n",
      "4382 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.66077e-07\n",
      "4383 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.12775e-07\n",
      "4384 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.34881e-07\n",
      "4385 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.6949e-07\n",
      "4386 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.92036e-07\n",
      "4387 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.60015e-07\n",
      "4388 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.52186e-07\n",
      "4389 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.37364e-07\n",
      "4390 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 4.60711e-07\n",
      "4391 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.58652e-07\n",
      "4392 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.90562e-07\n",
      "4393 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.2375e-07\n",
      "4394 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.72211e-07\n",
      "4395 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 6.68436e-07\n",
      "4396 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 4.43235e-07\n",
      "4397 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 5.2812e-07\n",
      "4398 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 7.52331e-07\n",
      "4399 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.61345e-07\n",
      "4400 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 3.57731e-07\n",
      "4401 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.65795e-07\n",
      "4402 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.60793e-07\n",
      "4403 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.8889e-07\n",
      "4404 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.57207e-07\n",
      "4405 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.83764e-07\n",
      "4406 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 1.86132e-07\n",
      "4407 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 6.03158e-07\n",
      "4408 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.05146e-07\n",
      "4409 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.836e-07\n",
      "4410 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.94951e-07\n",
      "4411 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.20816e-07\n",
      "4412 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.25324e-07\n",
      "4413 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.12207e-07\n",
      "4414 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 6.75101e-07\n",
      "4415 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.27469e-07\n",
      "4416 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.57007e-07\n",
      "4417 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 3.9195e-07\n",
      "4418 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 6.04204e-07\n",
      "4419 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 3.65775e-07\n",
      "4420 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 5.00114e-07\n",
      "4421 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.0021e-07\n",
      "4422 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.99241e-07\n",
      "4423 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.23432e-07\n",
      "4424 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.80336e-07\n",
      "4425 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.49405e-07\n",
      "4426 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.09914e-07\n",
      "4427 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.66675e-07\n",
      "4428 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.58092e-07\n",
      "4429 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.719e-07\n",
      "4430 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 5.99337e-07\n",
      "4431 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.03958e-07\n",
      "4432 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 3.80384e-07\n",
      "4433 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.72349e-07\n",
      "4434 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 6.58171e-07\n",
      "4435 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.89011e-07\n",
      "4436 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.40685e-07\n",
      "4437 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.63478e-07\n",
      "4438 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 3.51266e-07\n",
      "4439 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.92686e-07\n",
      "4440 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.94344e-07\n",
      "4441 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.49008e-07\n",
      "4442 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.02664e-07\n",
      "4443 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.33093e-07\n",
      "4444 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.63061e-07\n",
      "4445 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.61862e-07\n",
      "4446 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.89651e-07\n",
      "4447 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.31589e-07\n",
      "4448 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.5358e-07\n",
      "4449 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.50712e-07\n",
      "4450 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.78305e-07\n",
      "4451 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.35652e-07\n",
      "4452 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.0033e-07\n",
      "4453 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.72008e-07\n",
      "4454 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.2111e-07\n",
      "4455 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.59303e-07\n",
      "4456 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.05403e-07\n",
      "4457 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.72937e-07\n",
      "4458 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.74203e-07\n",
      "4459 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.12635e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4460 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.39264e-07\n",
      "4461 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.4511e-07\n",
      "4462 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.13298e-07\n",
      "4463 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.53683e-07\n",
      "4464 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.51825e-07\n",
      "4465 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.34082e-07\n",
      "4466 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.24621e-07\n",
      "4467 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.90149e-07\n",
      "4468 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 6.41386e-07\n",
      "4469 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.20642e-07\n",
      "4470 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 4.51467e-07\n",
      "4471 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.18228e-07\n",
      "4472 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.80537e-07\n",
      "4473 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 4.29119e-07\n",
      "4474 Train total loss: 3.02184 \tReconstruction loss: 3.02184 \tLatent loss: 4.13242e-07\n",
      "4475 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.37884e-07\n",
      "4476 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.42662e-07\n",
      "4477 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.12028e-07\n",
      "4478 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 7.99695e-07\n",
      "4479 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.35854e-07\n",
      "4480 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 6.34085e-07\n",
      "4481 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.65173e-07\n",
      "4482 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.01563e-07\n",
      "4483 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.20291e-07\n",
      "4484 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.8646e-07\n",
      "4485 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.6548e-07\n",
      "4486 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.1516e-07\n",
      "4487 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.98167e-07\n",
      "4488 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.27109e-07\n",
      "4489 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.57832e-07\n",
      "4490 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.80226e-07\n",
      "4491 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.62419e-07\n",
      "4492 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.15233e-07\n",
      "4493 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.03054e-06\n",
      "4494 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.69857e-07\n",
      "4495 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.72317e-07\n",
      "4496 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.43545e-07\n",
      "4497 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 2.44939e-07\n",
      "4498 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.18481e-07\n",
      "4499 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.21698e-07\n",
      "4500 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 3.67436e-07\n",
      "4501 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.75154e-07\n",
      "4502 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.3664e-07\n",
      "4503 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.29434e-07\n",
      "4504 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.38476e-07\n",
      "4505 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.59804e-07\n",
      "4506 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.59403e-07\n",
      "4507 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.3263e-07\n",
      "4508 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.85107e-07\n",
      "4509 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.21092e-07\n",
      "4510 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.74729e-07\n",
      "4511 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.26966e-07\n",
      "4512 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 8.70434e-07\n",
      "4513 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 6.66411e-07\n",
      "4514 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 2.76481e-07\n",
      "4515 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.94229e-07\n",
      "4516 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 3.22501e-07\n",
      "4517 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 6.42713e-07\n",
      "4518 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.57503e-07\n",
      "4519 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.26947e-07\n",
      "4520 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.99652e-07\n",
      "4521 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.74534e-07\n",
      "4522 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 4.83112e-07\n",
      "4523 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.00454e-07\n",
      "4524 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.45451e-07\n",
      "4525 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.05395e-07\n",
      "4526 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.2481e-07\n",
      "4527 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.37771e-07\n",
      "4528 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.12089e-07\n",
      "4529 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.77784e-07\n",
      "4530 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.26454e-07\n",
      "4531 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.05306e-07\n",
      "4532 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.25068e-07\n",
      "4533 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.26963e-07\n",
      "4534 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.19781e-07\n",
      "4535 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.06324e-07\n",
      "4536 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.80218e-07\n",
      "4537 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 2.36752e-07\n",
      "4538 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 3.86233e-07\n",
      "4539 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.09543e-07\n",
      "4540 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.98447e-07\n",
      "4541 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 6.29079e-08\n",
      "4542 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.84365e-07\n",
      "4543 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.60631e-07\n",
      "4544 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 3.13429e-07\n",
      "4545 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.89748e-07\n",
      "4546 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.49474e-07\n",
      "4547 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.62825e-07\n",
      "4548 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.00023e-07\n",
      "4549 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 5.04623e-07\n",
      "4550 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 7.05346e-07\n",
      "4551 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.91865e-07\n",
      "4552 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 5.76473e-07\n",
      "4553 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.35665e-07\n",
      "4554 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 2.71715e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4555 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.67859e-07\n",
      "4556 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.92123e-07\n",
      "4557 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.60022e-07\n",
      "4558 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.74575e-07\n",
      "4559 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.96615e-07\n",
      "4560 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.08867e-07\n",
      "4561 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.52001e-07\n",
      "4562 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.77392e-07\n",
      "4563 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.99395e-07\n",
      "4564 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.55533e-07\n",
      "4565 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.2151e-07\n",
      "4566 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 2.07969e-07\n",
      "4567 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.44416e-07\n",
      "4568 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.81837e-07\n",
      "4569 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.06787e-07\n",
      "4570 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.06044e-07\n",
      "4571 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 3.25663e-07\n",
      "4572 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 6.74379e-07\n",
      "4573 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.52731e-07\n",
      "4574 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 6.83579e-07\n",
      "4575 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.67865e-07\n",
      "4576 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.53083e-07\n",
      "4577 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 3.35302e-07\n",
      "4578 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.90856e-07\n",
      "4579 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.65008e-07\n",
      "4580 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 7.05189e-07\n",
      "4581 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.30847e-07\n",
      "4582 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.03258e-07\n",
      "4583 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 3.00219e-07\n",
      "4584 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.33241e-07\n",
      "4585 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 5.42376e-07\n",
      "4586 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.75878e-07\n",
      "4587 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.61235e-07\n",
      "4588 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.60561e-07\n",
      "4589 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.54369e-07\n",
      "4590 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.79182e-07\n",
      "4591 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 2.95258e-07\n",
      "4592 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.23939e-07\n",
      "4593 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.02712e-07\n",
      "4594 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.23184e-07\n",
      "4595 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.4377e-07\n",
      "4596 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.42952e-07\n",
      "4597 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 5.34984e-07\n",
      "4598 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.0257e-07\n",
      "4599 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.58166e-07\n",
      "4600 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 6.50878e-07\n",
      "4601 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.38231e-07\n",
      "4602 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.57754e-07\n",
      "4603 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.18266e-07\n",
      "4604 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.78738e-07\n",
      "4605 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.96017e-07\n",
      "4606 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.75016e-07\n",
      "4607 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.33664e-07\n",
      "4608 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.26782e-07\n",
      "4609 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 5.01674e-07\n",
      "4610 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.07943e-07\n",
      "4611 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.36361e-07\n",
      "4612 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.89604e-07\n",
      "4613 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.76497e-07\n",
      "4614 Train total loss: 3.02183 \tReconstruction loss: 3.02183 \tLatent loss: 4.54398e-07\n",
      "4615 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.04262e-07\n",
      "4616 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.39075e-07\n",
      "4617 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.72463e-07\n",
      "4618 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 4.66828e-07\n",
      "4619 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 4.49078e-07\n",
      "4620 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.09363e-07\n",
      "4621 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 8.59727e-08\n",
      "4622 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.22866e-07\n",
      "4623 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.78004e-07\n",
      "4624 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.60548e-07\n",
      "4625 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.87102e-07\n",
      "4626 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.60572e-07\n",
      "4627 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.41974e-07\n",
      "4628 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.86527e-07\n",
      "4629 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.75065e-07\n",
      "4630 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.67453e-07\n",
      "4631 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.27098e-07\n",
      "4632 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.05992e-07\n",
      "4633 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 5.27807e-07\n",
      "4634 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.99366e-07\n",
      "4635 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.85583e-07\n",
      "4636 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.14669e-07\n",
      "4637 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.67054e-07\n",
      "4638 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.36781e-07\n",
      "4639 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.56239e-07\n",
      "4640 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 6.63608e-07\n",
      "4641 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.16896e-07\n",
      "4642 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.51068e-07\n",
      "4643 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.06583e-07\n",
      "4644 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.27544e-07\n",
      "4645 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.60885e-07\n",
      "4646 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.17681e-07\n",
      "4647 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.72474e-07\n",
      "4648 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.05976e-07\n",
      "4649 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.90023e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 5.92749e-07\n",
      "4651 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.64484e-07\n",
      "4652 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 5.74582e-07\n",
      "4653 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.97352e-07\n",
      "4654 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.31042e-07\n",
      "4655 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.43969e-07\n",
      "4656 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.47471e-07\n",
      "4657 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.89206e-07\n",
      "4658 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.34703e-07\n",
      "4659 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.57342e-07\n",
      "4660 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.72912e-07\n",
      "4661 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.35374e-07\n",
      "4662 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.07796e-07\n",
      "4663 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.91523e-07\n",
      "4664 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.88407e-07\n",
      "4665 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.08867e-07\n",
      "4666 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.77024e-07\n",
      "4667 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.51252e-07\n",
      "4668 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.57697e-07\n",
      "4669 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.56729e-07\n",
      "4670 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.90244e-07\n",
      "4671 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.75386e-07\n",
      "4672 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.90964e-07\n",
      "4673 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.91145e-07\n",
      "4674 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.67603e-07\n",
      "4675 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.55874e-07\n",
      "4676 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.45487e-07\n",
      "4677 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.54002e-07\n",
      "4678 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.62367e-07\n",
      "4679 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.73625e-07\n",
      "4680 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 3.08426e-07\n",
      "4681 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.46778e-07\n",
      "4682 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.61037e-07\n",
      "4683 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.50415e-07\n",
      "4684 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.27024e-07\n",
      "4685 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.84016e-07\n",
      "4686 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.15911e-07\n",
      "4687 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.60182e-07\n",
      "4688 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.67565e-07\n",
      "4689 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.14725e-07\n",
      "4690 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.93842e-07\n",
      "4691 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.47773e-07\n",
      "4692 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 4.10677e-07\n",
      "4693 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.79566e-07\n",
      "4694 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.32125e-07\n",
      "4695 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 2.51392e-07\n",
      "4696 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.30729e-07\n",
      "4697 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.09324e-07\n",
      "4698 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.57838e-07\n",
      "4699 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.93485e-07\n",
      "4700 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.31593e-07\n",
      "4701 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 6.9871e-08\n",
      "4702 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.44435e-07\n",
      "4703 Train total loss: 2.86598 \tReconstruction loss: 2.86598 \tLatent loss: 3.55252e-07\n",
      "4704 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.47914e-07\n",
      "4705 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.42813e-07\n",
      "4706 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.31499e-07\n",
      "4707 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 4.5738e-07\n",
      "4708 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 2.45978e-07\n",
      "4709 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.22587e-07\n",
      "4710 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.15751e-07\n",
      "4711 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.80656e-07\n",
      "4712 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 6.77783e-07\n",
      "4713 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.2183e-07\n",
      "4714 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 9.73715e-07\n",
      "4715 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.89884e-07\n",
      "4716 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.43751e-07\n",
      "4717 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.72596e-07\n",
      "4718 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.11425e-07\n",
      "4719 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 4.33518e-07\n",
      "4720 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.74398e-07\n",
      "4721 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.47523e-07\n",
      "4722 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.0283e-07\n",
      "4723 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.33309e-07\n",
      "4724 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.30353e-07\n",
      "4725 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 6.16224e-07\n",
      "4726 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.85475e-07\n",
      "4727 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.42642e-07\n",
      "4728 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.24052e-07\n",
      "4729 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.84137e-07\n",
      "4730 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.93801e-07\n",
      "4731 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 2.72943e-07\n",
      "4732 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.87638e-07\n",
      "4733 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 8.00121e-07\n",
      "4734 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.9619e-07\n",
      "4735 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.08818e-07\n",
      "4736 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.0757e-07\n",
      "4737 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.35765e-07\n",
      "4738 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.19193e-07\n",
      "4739 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 4.71639e-07\n",
      "4740 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 3.25155e-07\n",
      "4741 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.8095e-07\n",
      "4742 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.6898e-07\n",
      "4743 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.12723e-07\n",
      "4744 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.58045e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4745 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.47475e-07\n",
      "4746 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.36189e-07\n",
      "4747 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.4037e-07\n",
      "4748 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.50939e-07\n",
      "4749 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.07339e-07\n",
      "4750 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.1364e-07\n",
      "4751 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.59135e-07\n",
      "4752 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.85093e-07\n",
      "4753 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.41874e-07\n",
      "4754 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.7647e-07\n",
      "4755 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.64808e-07\n",
      "4756 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.12597e-07\n",
      "4757 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.50152e-07\n",
      "4758 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.10206e-07\n",
      "4759 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.27414e-07\n",
      "4760 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 3.52133e-07\n",
      "4761 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.10667e-07\n",
      "4762 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.36266e-07\n",
      "4763 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.05209e-07\n",
      "4764 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.50748e-07\n",
      "4765 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.44494e-07\n",
      "4766 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.92994e-07\n",
      "4767 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.73046e-07\n",
      "4768 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.32196e-07\n",
      "4769 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.12527e-07\n",
      "4770 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.91091e-07\n",
      "4771 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.29011e-07\n",
      "4772 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.82408e-07\n",
      "4773 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.96746e-07\n",
      "4774 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.71784e-07\n",
      "4775 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.38785e-07\n",
      "4776 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.5089e-07\n",
      "4777 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.42719e-07\n",
      "4778 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.31309e-07\n",
      "4779 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.43096e-07\n",
      "4780 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.38387e-07\n",
      "4781 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.70332e-07\n",
      "4782 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 2.74755e-07\n",
      "4783 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.59708e-07\n",
      "4784 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.31339e-07\n",
      "4785 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.28838e-07\n",
      "4786 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.95277e-07\n",
      "4787 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.24836e-07\n",
      "4788 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.96292e-07\n",
      "4789 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.19712e-07\n",
      "4790 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.39998e-07\n",
      "4791 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.2707e-07\n",
      "4792 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.81022e-07\n",
      "4793 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.70146e-07\n",
      "4794 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 5.88839e-07\n",
      "4795 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.62461e-07\n",
      "4796 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.1388e-07\n",
      "4797 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.49499e-07\n",
      "4798 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.83693e-07\n",
      "4799 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.24883e-07\n",
      "4800 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.33318e-07\n",
      "4801 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.75002e-07\n",
      "4802 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.69143e-07\n",
      "4803 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.59816e-07\n",
      "4804 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.25768e-07\n",
      "4805 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.20595e-07\n",
      "4806 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.79914e-07\n",
      "4807 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.1102e-07\n",
      "4808 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.49638e-07\n",
      "4809 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 1.57578e-07\n",
      "4810 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.93947e-07\n",
      "4811 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.11131e-07\n",
      "4812 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.13966e-07\n",
      "4813 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.26867e-07\n",
      "4814 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.47029e-07\n",
      "4815 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.03557e-07\n",
      "4816 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.48277e-07\n",
      "4817 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.92193e-07\n",
      "4818 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.03306e-07\n",
      "4819 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 2.82074e-07\n",
      "4820 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.91392e-07\n",
      "4821 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.70023e-07\n",
      "4822 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.94693e-07\n",
      "4823 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 5.41144e-07\n",
      "4824 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.64691e-07\n",
      "4825 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.20319e-07\n",
      "4826 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.73549e-07\n",
      "4827 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 5.02211e-07\n",
      "4828 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.23037e-07\n",
      "4829 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.05233e-07\n",
      "4830 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.01969e-07\n",
      "4831 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.16816e-07\n",
      "4832 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.13509e-07\n",
      "4833 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.32154e-07\n",
      "4834 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 3.76503e-07\n",
      "4835 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.66647e-07\n",
      "4836 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.00146e-07\n",
      "4837 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.11378e-07\n",
      "4838 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.20274e-07\n",
      "4839 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 1.63929e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4840 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 7.606e-07\n",
      "4841 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.29564e-07\n",
      "4842 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 1.32051e-07\n",
      "4843 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.41346e-07\n",
      "4844 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.99534e-07\n",
      "4845 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.44772e-07\n",
      "4846 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.67405e-07\n",
      "4847 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.9609e-07\n",
      "4848 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.39073e-07\n",
      "4849 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.95936e-07\n",
      "4850 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.94686e-07\n",
      "4851 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 1.76324e-07\n",
      "4852 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.31965e-07\n",
      "4853 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.77621e-07\n",
      "4854 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 3.17417e-07\n",
      "4855 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.19468e-07\n",
      "4856 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.39912e-07\n",
      "4857 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.73291e-07\n",
      "4858 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.70169e-07\n",
      "4859 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 7.26389e-07\n",
      "4860 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.63536e-07\n",
      "4861 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.82045e-07\n",
      "4862 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.32815e-07\n",
      "4863 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.94179e-07\n",
      "4864 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.74806e-07\n",
      "4865 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.85064e-07\n",
      "4866 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.91668e-07\n",
      "4867 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.66379e-07\n",
      "4868 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.16508e-07\n",
      "4869 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.42874e-07\n",
      "4870 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.12406e-07\n",
      "4871 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.15917e-07\n",
      "4872 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.66817e-07\n",
      "4873 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.09256e-06\n",
      "4874 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.06264e-07\n",
      "4875 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.94983e-07\n",
      "4876 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.08194e-07\n",
      "4877 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.33498e-07\n",
      "4878 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.1443e-07\n",
      "4879 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 8.70213e-07\n",
      "4880 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.58109e-07\n",
      "4881 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.57829e-07\n",
      "4882 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.25644e-07\n",
      "4883 Train total loss: 2.86596 \tReconstruction loss: 2.86595 \tLatent loss: 3.14246e-07\n",
      "4884 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.0066e-07\n",
      "4885 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.49632e-07\n",
      "4886 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.87464e-07\n",
      "4887 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.55872e-07\n",
      "4888 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.94987e-07\n",
      "4889 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.35389e-07\n",
      "4890 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.7461e-07\n",
      "4891 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.99224e-07\n",
      "4892 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.65205e-07\n",
      "4893 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 5.65279e-07\n",
      "4894 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.45708e-07\n",
      "4895 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.40668e-07\n",
      "4896 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.44731e-07\n",
      "4897 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.83047e-07\n",
      "4898 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.48748e-07\n",
      "4899 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.13796e-07\n",
      "4900 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 3.86522e-07\n",
      "4901 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.07571e-07\n",
      "4902 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.33359e-07\n",
      "4903 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.51924e-07\n",
      "4904 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.74705e-07\n",
      "4905 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.63111e-07\n",
      "4906 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.80381e-07\n",
      "4907 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.85914e-07\n",
      "4908 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.67431e-07\n",
      "4909 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.63e-07\n",
      "4910 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.87853e-07\n",
      "4911 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.69499e-07\n",
      "4912 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.74633e-07\n",
      "4913 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.541e-07\n",
      "4914 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 8.10731e-07\n",
      "4915 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 3.48342e-07\n",
      "4916 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.1095e-07\n",
      "4917 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 8.59596e-08\n",
      "4918 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.56964e-07\n",
      "4919 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.78448e-07\n",
      "4920 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.65562e-07\n",
      "4921 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.83479e-07\n",
      "4922 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.07895e-07\n",
      "4923 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.73553e-07\n",
      "4924 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.26789e-07\n",
      "4925 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.99086e-07\n",
      "4926 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.26534e-07\n",
      "4927 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.89819e-07\n",
      "4928 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.50238e-07\n",
      "4929 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.62311e-07\n",
      "4930 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.38483e-07\n",
      "4931 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.16299e-07\n",
      "4932 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.32347e-07\n",
      "4933 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.8396e-07\n",
      "4934 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 7.26152e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4935 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 1.47005e-07\n",
      "4936 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.04674e-07\n",
      "4937 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.90474e-07\n",
      "4938 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.5807e-07\n",
      "4939 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.42618e-07\n",
      "4940 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.25774e-07\n",
      "4941 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.16882e-07\n",
      "4942 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.32557e-07\n",
      "4943 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.08621e-07\n",
      "4944 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.55112e-07\n",
      "4945 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.45125e-07\n",
      "4946 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.94569e-07\n",
      "4947 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.33054e-07\n",
      "4948 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.22425e-07\n",
      "4949 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.00421e-07\n",
      "4950 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.94719e-07\n",
      "4951 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.80245e-07\n",
      "4952 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.80719e-07\n",
      "4953 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.56723e-07\n",
      "4954 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.16405e-07\n",
      "4955 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.10261e-07\n",
      "4956 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.9584e-07\n",
      "4957 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.04705e-07\n",
      "4958 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 4.89851e-07\n",
      "4959 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 5.42379e-07\n",
      "4960 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.84276e-07\n",
      "4961 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.43296e-07\n",
      "4962 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.5783e-07\n",
      "4963 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.00874e-07\n",
      "4964 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.34987e-07\n",
      "4965 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.59343e-07\n",
      "4966 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 4.88613e-07\n",
      "4967 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.76255e-07\n",
      "4968 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.59006e-07\n",
      "4969 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.21058e-07\n",
      "4970 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.29397e-07\n",
      "4971 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.31891e-07\n",
      "4972 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.57586e-07\n",
      "4973 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 8.51604e-07\n",
      "4974 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.89241e-07\n",
      "4975 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.21322e-07\n",
      "4976 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.77231e-07\n",
      "4977 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.27697e-07\n",
      "4978 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.33273e-07\n",
      "4979 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.51983e-07\n",
      "4980 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.1194e-07\n",
      "4981 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.38659e-07\n",
      "4982 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.15994e-07\n",
      "4983 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.13473e-07\n",
      "4984 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.12924e-07\n",
      "4985 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.12034e-07\n",
      "4986 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.57161e-07\n",
      "4987 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.21973e-07\n",
      "4988 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.75813e-07\n",
      "4989 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.14261e-07\n",
      "4990 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 1.81695e-07\n",
      "4991 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 3.12952e-07\n",
      "4992 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.81056e-07\n",
      "4993 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.34809e-07\n",
      "4994 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.80654e-07\n",
      "4995 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.9852e-07\n",
      "4996 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.82011e-07\n",
      "4997 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.31653e-07\n",
      "4998 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.91911e-07\n",
      "4999 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 4.50209e-07\n",
      "5000 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.95156e-07\n",
      "5001 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.0724e-07\n",
      "5002 Train total loss: 2.83694 \tReconstruction loss: 2.83693 \tLatent loss: 1.7555e-07\n",
      "5003 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 2.50417e-07\n",
      "5004 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.61672e-07\n",
      "5005 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.13567e-07\n",
      "5006 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 4.40973e-07\n",
      "5007 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.87143e-07\n",
      "5008 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.62975e-07\n",
      "5009 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.76675e-07\n",
      "5010 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.36466e-07\n",
      "5011 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.53102e-07\n",
      "5012 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.33861e-07\n",
      "5013 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.84481e-07\n",
      "5014 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 6.01505e-07\n",
      "5015 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.37612e-07\n",
      "5016 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.02268e-07\n",
      "5017 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.21576e-07\n",
      "5018 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.92273e-07\n",
      "5019 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.97563e-07\n",
      "5020 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.33635e-07\n",
      "5021 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.79624e-07\n",
      "5022 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.32421e-07\n",
      "5023 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 2.77668e-07\n",
      "5024 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 2.55022e-07\n",
      "5025 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.67084e-07\n",
      "5026 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.2922e-07\n",
      "5027 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.03954e-07\n",
      "5028 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.73533e-07\n",
      "5029 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 3.21907e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5030 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.38998e-07\n",
      "5031 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.92211e-07\n",
      "5032 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.9395e-07\n",
      "5033 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.02319e-07\n",
      "5034 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.87451e-07\n",
      "5035 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.12976e-07\n",
      "5036 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.84744e-07\n",
      "5037 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.3865e-07\n",
      "5038 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.23324e-07\n",
      "5039 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.82279e-07\n",
      "5040 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 1.23717e-07\n",
      "5041 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.14867e-07\n",
      "5042 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.68264e-07\n",
      "5043 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.08903e-07\n",
      "5044 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.14314e-07\n",
      "5045 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.4967e-07\n",
      "5046 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.9175e-07\n",
      "5047 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.96398e-07\n",
      "5048 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.297e-07\n",
      "5049 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.23292e-07\n",
      "5050 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.60192e-07\n",
      "5051 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.21169e-07\n",
      "5052 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.84751e-07\n",
      "5053 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 4.37761e-07\n",
      "5054 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.09457e-07\n",
      "5055 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.8221e-07\n",
      "5056 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.36127e-07\n",
      "5057 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.22331e-07\n",
      "5058 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.49674e-07\n",
      "5059 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.33179e-07\n",
      "5060 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 1.77574e-07\n",
      "5061 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.53865e-07\n",
      "5062 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.58674e-07\n",
      "5063 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.31543e-07\n",
      "5064 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.31652e-07\n",
      "5065 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.93801e-07\n",
      "5066 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.3614e-07\n",
      "5067 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.1617e-07\n",
      "5068 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.08043e-07\n",
      "5069 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.40352e-07\n",
      "5070 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.94903e-07\n",
      "5071 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.01615e-07\n",
      "5072 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.89701e-07\n",
      "5073 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 5.17358e-07\n",
      "5074 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.00109e-07\n",
      "5075 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.01444e-07\n",
      "5076 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.07225e-07\n",
      "5077 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.89757e-07\n",
      "5078 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.63735e-07\n",
      "5079 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 8.05501e-07\n",
      "5080 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.25233e-07\n",
      "5081 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.25782e-07\n",
      "5082 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 1.09214e-07\n",
      "5083 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.60196e-07\n",
      "5084 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.21006e-07\n",
      "5085 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.91274e-07\n",
      "5086 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 5.39538e-07\n",
      "5087 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 4.47941e-07\n",
      "5088 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.83225e-07\n",
      "5089 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.07106e-07\n",
      "5090 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.49281e-07\n",
      "5091 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.17414e-07\n",
      "5092 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.51293e-07\n",
      "5093 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.35183e-07\n",
      "5094 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 5.48306e-07\n",
      "5095 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.96404e-07\n",
      "5096 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.35961e-07\n",
      "5097 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.97392e-07\n",
      "5098 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.80162e-07\n",
      "5099 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.3326e-07\n",
      "5100 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.47362e-07\n",
      "5101 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.2158e-07\n",
      "5102 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.3255e-07\n",
      "5103 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.85967e-07\n",
      "5104 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.98261e-08\n",
      "5105 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.36786e-07\n",
      "5106 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.81495e-07\n",
      "5107 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.75868e-07\n",
      "5108 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.67761e-07\n",
      "5109 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.86668e-07\n",
      "5110 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.29665e-07\n",
      "5111 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 5.10013e-07\n",
      "5112 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.15611e-06\n",
      "5113 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.39867e-07\n",
      "5114 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.30012e-07\n",
      "5115 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.63287e-07\n",
      "5116 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.90281e-07\n",
      "5117 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.61538e-07\n",
      "5118 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.73967e-07\n",
      "5119 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.95795e-07\n",
      "5120 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 2.9917e-07\n",
      "5121 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.30442e-07\n",
      "5122 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.0683e-07\n",
      "5123 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.19165e-07\n",
      "5124 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.57426e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5125 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.62605e-07\n",
      "5126 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.09581e-07\n",
      "5127 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.04504e-07\n",
      "5128 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.35523e-07\n",
      "5129 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.71788e-07\n",
      "5130 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.66701e-07\n",
      "5131 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.48687e-07\n",
      "5132 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 5.94661e-07\n",
      "5133 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.23541e-07\n",
      "5134 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.11134e-07\n",
      "5135 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.30376e-07\n",
      "5136 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.81409e-07\n",
      "5137 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.32137e-07\n",
      "5138 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.17708e-07\n",
      "5139 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 8.32675e-07\n",
      "5140 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.50988e-07\n",
      "5141 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.85888e-07\n",
      "5142 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.06216e-07\n",
      "5143 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.00245e-07\n",
      "5144 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.20163e-07\n",
      "5145 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.11568e-07\n",
      "5146 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 4.92333e-07\n",
      "5147 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.22281e-07\n",
      "5148 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.21592e-07\n",
      "5149 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.11359e-07\n",
      "5150 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.04858e-07\n",
      "5151 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.5707e-07\n",
      "5152 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.7269e-07\n",
      "5153 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.6863e-07\n",
      "5154 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.86684e-07\n",
      "5155 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.27316e-07\n",
      "5156 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.33459e-07\n",
      "5157 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.36351e-07\n",
      "5158 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.31072e-07\n",
      "5159 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 7.09075e-07\n",
      "5160 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.66912e-07\n",
      "5161 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.37691e-07\n",
      "5162 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.00739e-07\n",
      "5163 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.88065e-07\n",
      "5164 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 3.50388e-07\n",
      "5165 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.77526e-07\n",
      "5166 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.45551e-07\n",
      "5167 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.35578e-07\n",
      "5168 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.59616e-07\n",
      "5169 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.76406e-07\n",
      "5170 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.71526e-07\n",
      "5171 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.16175e-07\n",
      "5172 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.64582e-07\n",
      "5173 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.9111e-07\n",
      "5174 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.86281e-07\n",
      "5175 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.08408e-07\n",
      "5176 Train total loss: 2.77375 \tReconstruction loss: 2.77375 \tLatent loss: 5.9388e-07\n",
      "5177 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 8.6426e-07\n",
      "5178 Train total loss: 2.77427 \tReconstruction loss: 2.77427 \tLatent loss: 4.40452e-07\n",
      "5179 Train total loss: 2.80522 \tReconstruction loss: 2.80522 \tLatent loss: 2.42379e-06\n",
      "5180 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 9.58281e-07\n",
      "5181 Train total loss: 2.80588 \tReconstruction loss: 2.80587 \tLatent loss: 2.08119e-06\n",
      "5182 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 3.74625e-06\n",
      "5183 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 1.20627e-06\n",
      "5184 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 1.05213e-06\n",
      "5185 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.04607e-06\n",
      "5186 Train total loss: 2.80905 \tReconstruction loss: 2.80904 \tLatent loss: 8.99966e-07\n",
      "5187 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.11455e-06\n",
      "5188 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 7.66564e-07\n",
      "5189 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.99052e-06\n",
      "5190 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.05003e-06\n",
      "5191 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 4.97527e-07\n",
      "5192 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.32273e-07\n",
      "5193 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 5.08281e-07\n",
      "5194 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.03248e-07\n",
      "5195 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.14362e-07\n",
      "5196 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.97696e-07\n",
      "5197 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.28926e-07\n",
      "5198 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.89514e-07\n",
      "5199 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.77088e-07\n",
      "5200 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 5.01066e-07\n",
      "5201 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 5.13323e-07\n",
      "5202 Train total loss: 2.83695 \tReconstruction loss: 2.83695 \tLatent loss: 3.29956e-07\n",
      "5203 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.72768e-07\n",
      "5204 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 2.08944e-07\n",
      "5205 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.24812e-07\n",
      "5206 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.39685e-07\n",
      "5207 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.74785e-07\n",
      "5208 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.68712e-07\n",
      "5209 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 3.44294e-07\n",
      "5210 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 4.85641e-07\n",
      "5211 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.08285e-07\n",
      "5212 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.19719e-07\n",
      "5213 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 2.87898e-07\n",
      "5214 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 1.18035e-06\n",
      "5215 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.21287e-07\n",
      "5216 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.46117e-07\n",
      "5217 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 7.89721e-07\n",
      "5218 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.65212e-07\n",
      "5219 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.65807e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5220 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 5.5354e-07\n",
      "5221 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.84433e-07\n",
      "5222 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.24765e-07\n",
      "5223 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.00204e-07\n",
      "5224 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.04869e-07\n",
      "5225 Train total loss: 2.91473 \tReconstruction loss: 2.91473 \tLatent loss: 6.16917e-07\n",
      "5226 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 5.16696e-07\n",
      "5227 Train total loss: 2.92515 \tReconstruction loss: 2.92514 \tLatent loss: 2.16071e-07\n",
      "5228 Train total loss: 2.8542 \tReconstruction loss: 2.8542 \tLatent loss: 5.56474e-07\n",
      "5229 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.62921e-07\n",
      "5230 Train total loss: 2.98598 \tReconstruction loss: 2.98598 \tLatent loss: 6.25214e-07\n",
      "5231 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 7.2877e-07\n",
      "5232 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 5.12339e-07\n",
      "5233 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.63398e-07\n",
      "5234 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 7.18892e-07\n",
      "5235 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.41233e-07\n",
      "5236 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.93552e-07\n",
      "5237 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 3.60177e-07\n",
      "5238 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.49699e-06\n",
      "5239 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 4.32577e-07\n",
      "5240 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 7.11594e-07\n",
      "5241 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 6.28263e-07\n",
      "5242 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.46886e-07\n",
      "5243 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.88315e-07\n",
      "5244 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.36009e-07\n",
      "5245 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.73179e-07\n",
      "5246 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.89317e-07\n",
      "5247 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.55658e-07\n",
      "5248 Train total loss: 2.85421 \tReconstruction loss: 2.8542 \tLatent loss: 1.35973e-06\n",
      "5249 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.99469e-07\n",
      "5250 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 8.54972e-07\n",
      "5251 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 5.66611e-07\n",
      "5252 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.31167e-07\n",
      "5253 Train total loss: 2.97585 \tReconstruction loss: 2.97584 \tLatent loss: 9.03969e-06\n",
      "5254 Train total loss: 3.02183 \tReconstruction loss: 3.02183 \tLatent loss: 1.00177e-06\n",
      "5255 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.97962e-07\n",
      "5256 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.12637e-07\n",
      "5257 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.05726e-07\n",
      "5258 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.2253e-07\n",
      "5259 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.06414e-07\n",
      "5260 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.40632e-07\n",
      "5261 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 6.59275e-08\n",
      "5262 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 9.9426e-08\n",
      "5263 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 1.2549e-07\n",
      "5264 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.60628e-07\n",
      "5265 Train total loss: 2.91473 \tReconstruction loss: 2.91473 \tLatent loss: 1.32783e-07\n",
      "5266 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.03921e-07\n",
      "5267 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.57661e-07\n",
      "5268 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.99628e-07\n",
      "5269 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.50343e-07\n",
      "5270 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.56284e-07\n",
      "5271 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.79518e-07\n",
      "5272 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.75704e-07\n",
      "5273 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.05253e-07\n",
      "5274 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 8.0782e-08\n",
      "5275 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.17008e-07\n",
      "5276 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.69094e-07\n",
      "5277 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.45588e-07\n",
      "5278 Train total loss: 2.7743 \tReconstruction loss: 2.7743 \tLatent loss: 3.41573e-07\n",
      "5279 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.66253e-07\n",
      "5280 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.6298e-07\n",
      "5281 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.03879e-07\n",
      "5282 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.84005e-07\n",
      "5283 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.66072e-07\n",
      "5284 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.49071e-07\n",
      "5285 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.50247e-07\n",
      "5286 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.25424e-07\n",
      "5287 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.51891e-07\n",
      "5288 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.16065e-07\n",
      "5289 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.74197e-07\n",
      "5290 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 5.89558e-07\n",
      "5291 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.55329e-07\n",
      "5292 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 3.06932e-07\n",
      "5293 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.44006e-07\n",
      "5294 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.22557e-07\n",
      "5295 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.63353e-07\n",
      "5296 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.52902e-08\n",
      "5297 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.59862e-07\n",
      "5298 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 5.73823e-07\n",
      "5299 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 8.02596e-08\n",
      "5300 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 3.21098e-07\n",
      "5301 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.37973e-08\n",
      "5302 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.77341e-07\n",
      "5303 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.37739e-07\n",
      "5304 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.0236e-07\n",
      "5305 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.46134e-07\n",
      "5306 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.0471e-07\n",
      "5307 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.73018e-07\n",
      "5308 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.038e-07\n",
      "5309 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.21657e-07\n",
      "5310 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.01618e-07\n",
      "5311 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 4.54026e-07\n",
      "5312 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 6.13341e-07\n",
      "5313 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.8963e-07\n",
      "5314 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.06302e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5315 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.63589e-07\n",
      "5316 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.22247e-07\n",
      "5317 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.83386e-07\n",
      "5318 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.681e-07\n",
      "5319 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.08166e-07\n",
      "5320 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 4.36953e-07\n",
      "5321 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.19489e-07\n",
      "5322 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.47936e-07\n",
      "5323 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 1.38732e-07\n",
      "5324 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.98105e-06\n",
      "5325 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.49051e-07\n",
      "5326 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.25412e-07\n",
      "5327 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.55676e-07\n",
      "5328 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.62111e-07\n",
      "5329 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.02037e-07\n",
      "5330 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.2021e-07\n",
      "5331 Train total loss: 2.94647 \tReconstruction loss: 2.94646 \tLatent loss: 2.51475e-07\n",
      "5332 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 8.62201e-07\n",
      "5333 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.49851e-07\n",
      "5334 Train total loss: 3.02178 \tReconstruction loss: 3.02178 \tLatent loss: 1.41248e-07\n",
      "5335 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.90064e-07\n",
      "5336 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.53043e-07\n",
      "5337 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.42573e-07\n",
      "5338 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.87433e-07\n",
      "5339 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.52946e-07\n",
      "5340 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.72137e-07\n",
      "5341 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.84053e-07\n",
      "5342 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.37563e-07\n",
      "5343 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.31998e-07\n",
      "5344 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 3.11795e-07\n",
      "5345 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.72675e-07\n",
      "5346 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.33488e-07\n",
      "5347 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.52003e-07\n",
      "5348 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.88269e-07\n",
      "5349 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.49578e-07\n",
      "5350 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 7.90271e-07\n",
      "5351 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.30571e-07\n",
      "5352 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.80634e-07\n",
      "5353 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 5.57996e-07\n",
      "5354 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.19938e-07\n",
      "5355 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.64445e-07\n",
      "5356 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.78144e-07\n",
      "5357 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.89905e-07\n",
      "5358 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 5.92472e-07\n",
      "5359 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.19556e-07\n",
      "5360 Train total loss: 3.03979 \tReconstruction loss: 3.03978 \tLatent loss: 5.24495e-07\n",
      "5361 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.69501e-07\n",
      "5362 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.95434e-07\n",
      "5363 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 2.44094e-07\n",
      "5364 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 5.54349e-07\n",
      "5365 Train total loss: 2.91474 \tReconstruction loss: 2.91473 \tLatent loss: 6.45076e-07\n",
      "5366 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.92925e-07\n",
      "5367 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.27517e-07\n",
      "5368 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.42249e-07\n",
      "5369 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.82224e-07\n",
      "5370 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 1.25752e-06\n",
      "5371 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 3.09918e-07\n",
      "5372 Train total loss: 2.92838 \tReconstruction loss: 2.92838 \tLatent loss: 5.46365e-07\n",
      "5373 Train total loss: 2.97585 \tReconstruction loss: 2.97585 \tLatent loss: 5.56363e-07\n",
      "5374 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 9.24626e-07\n",
      "5375 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 6.01655e-07\n",
      "5376 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.5886e-07\n",
      "5377 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.55941e-07\n",
      "5378 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.46539e-07\n",
      "5379 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.254e-07\n",
      "5380 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 5.00034e-07\n",
      "5381 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.89476e-07\n",
      "5382 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.44001e-07\n",
      "5383 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.38283e-07\n",
      "5384 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 2.18896e-07\n",
      "5385 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.96619e-07\n",
      "5386 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.4149e-07\n",
      "5387 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.84408e-07\n",
      "5388 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 6.87072e-07\n",
      "5389 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.32071e-06\n",
      "5390 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.80269e-07\n",
      "5391 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.24358e-07\n",
      "5392 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 1.4891e-07\n",
      "5393 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 4.32876e-07\n",
      "5394 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.46575e-07\n",
      "5395 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 4.21726e-07\n",
      "5396 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.21498e-07\n",
      "5397 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.52281e-07\n",
      "5398 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 5.30839e-07\n",
      "5399 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.54116e-07\n",
      "5400 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 5.85036e-07\n",
      "5401 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.98543e-07\n",
      "5402 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.39138e-07\n",
      "5403 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.04769e-07\n",
      "5404 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.16698e-07\n",
      "5405 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 5.22147e-07\n",
      "5406 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.27473e-07\n",
      "5407 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 5.49195e-07\n",
      "5408 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.17997e-07\n",
      "5409 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.28324e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5410 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 6.77997e-07\n",
      "5411 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 2.28251e-07\n",
      "5412 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.28469e-07\n",
      "5413 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.50551e-06\n",
      "5414 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 9.99466e-07\n",
      "5415 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 6.25831e-07\n",
      "5416 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.72168e-07\n",
      "5417 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.29583e-07\n",
      "5418 Train total loss: 2.77426 \tReconstruction loss: 2.77426 \tLatent loss: 4.34979e-07\n",
      "5419 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 8.80988e-08\n",
      "5420 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.13086e-06\n",
      "5421 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.98906e-07\n",
      "5422 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.92921e-07\n",
      "5423 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 4.30194e-07\n",
      "5424 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.78909e-07\n",
      "5425 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.12494e-07\n",
      "5426 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.96035e-07\n",
      "5427 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.34206e-07\n",
      "5428 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.19149e-07\n",
      "5429 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.98575e-07\n",
      "5430 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 7.81656e-07\n",
      "5431 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.45507e-07\n",
      "5432 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.91892e-07\n",
      "5433 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.78476e-07\n",
      "5434 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 6.1364e-07\n",
      "5435 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.04809e-07\n",
      "5436 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 3.16434e-07\n",
      "5437 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.90565e-07\n",
      "5438 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.55555e-07\n",
      "5439 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.21641e-07\n",
      "5440 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.9524e-07\n",
      "5441 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.69298e-07\n",
      "5442 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.73174e-07\n",
      "5443 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.28718e-07\n",
      "5444 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.18554e-07\n",
      "5445 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.21931e-07\n",
      "5446 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.82614e-07\n",
      "5447 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 5.74453e-07\n",
      "5448 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.0743e-07\n",
      "5449 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 4.23573e-07\n",
      "5450 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.13703e-06\n",
      "5451 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.5508e-07\n",
      "5452 Train total loss: 2.92838 \tReconstruction loss: 2.92838 \tLatent loss: 3.88687e-07\n",
      "5453 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 4.63217e-07\n",
      "5454 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 3.71592e-07\n",
      "5455 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 3.98533e-07\n",
      "5456 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.54236e-07\n",
      "5457 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.16074e-07\n",
      "5458 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.34258e-07\n",
      "5459 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 4.68515e-07\n",
      "5460 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.94413e-07\n",
      "5461 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.83638e-07\n",
      "5462 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.20316e-07\n",
      "5463 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.2275e-07\n",
      "5464 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.50108e-07\n",
      "5465 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.3491e-07\n",
      "5466 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.3799e-07\n",
      "5467 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 2.16157e-07\n",
      "5468 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.2946e-07\n",
      "5469 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.51891e-07\n",
      "5470 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.15453e-07\n",
      "5471 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 1.54809e-07\n",
      "5472 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.76869e-07\n",
      "5473 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.53855e-07\n",
      "5474 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.67452e-07\n",
      "5475 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.19453e-07\n",
      "5476 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.46223e-07\n",
      "5477 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.2623e-07\n",
      "5478 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.67036e-07\n",
      "5479 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.906e-07\n",
      "5480 Train total loss: 3.0398 \tReconstruction loss: 3.03979 \tLatent loss: 4.95304e-07\n",
      "5481 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.49575e-07\n",
      "5482 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.31228e-07\n",
      "5483 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.70062e-07\n",
      "5484 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.99007e-07\n",
      "5485 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.06626e-07\n",
      "5486 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.73354e-07\n",
      "5487 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.41489e-07\n",
      "5488 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.31981e-07\n",
      "5489 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.30311e-07\n",
      "5490 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.32829e-07\n",
      "5491 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.13895e-07\n",
      "5492 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 7.56574e-07\n",
      "5493 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 3.47405e-07\n",
      "5494 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.36598e-07\n",
      "5495 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.19318e-07\n",
      "5496 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.95391e-07\n",
      "5497 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.15333e-07\n",
      "5498 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.52412e-07\n",
      "5499 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.39692e-07\n",
      "5500 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 6.31629e-07\n",
      "5501 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.11591e-07\n",
      "5502 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 2.18076e-07\n",
      "5503 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.58152e-07\n",
      "5504 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 4.07059e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5505 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.187e-07\n",
      "5506 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 5.49541e-07\n",
      "5507 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 5.12574e-07\n",
      "5508 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.79346e-07\n",
      "5509 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.41933e-07\n",
      "5510 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.15331e-07\n",
      "5511 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.37899e-07\n",
      "5512 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.29854e-07\n",
      "5513 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.74427e-07\n",
      "5514 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 4.74486e-07\n",
      "5515 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.51102e-07\n",
      "5516 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.53017e-07\n",
      "5517 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.51154e-07\n",
      "5518 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.18114e-06\n",
      "5519 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.7116e-07\n",
      "5520 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 4.30064e-07\n",
      "5521 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.36957e-07\n",
      "5522 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 2.49382e-07\n",
      "5523 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.87439e-07\n",
      "5524 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.682e-07\n",
      "5525 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.89702e-07\n",
      "5526 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.62877e-07\n",
      "5527 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 4.01319e-07\n",
      "5528 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.30178e-07\n",
      "5529 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.05733e-07\n",
      "5530 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 6.13878e-07\n",
      "5531 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.56589e-07\n",
      "5532 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.347e-07\n",
      "5533 Train total loss: 2.97583 \tReconstruction loss: 2.97582 \tLatent loss: 8.59544e-07\n",
      "5534 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.07593e-07\n",
      "5535 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.3476e-07\n",
      "5536 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.75113e-07\n",
      "5537 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.53475e-07\n",
      "5538 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.47834e-07\n",
      "5539 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.55262e-07\n",
      "5540 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.89733e-07\n",
      "5541 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 6.92948e-08\n",
      "5542 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.52632e-07\n",
      "5543 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.92054e-07\n",
      "5544 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.56638e-07\n",
      "5545 Train total loss: 2.91472 \tReconstruction loss: 2.91472 \tLatent loss: 2.30866e-07\n",
      "5546 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.74159e-07\n",
      "5547 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.77856e-07\n",
      "5548 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.09824e-07\n",
      "5549 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.67945e-07\n",
      "5550 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.91135e-07\n",
      "5551 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.63406e-07\n",
      "5552 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 6.20956e-07\n",
      "5553 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.78011e-07\n",
      "5554 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 5.01335e-07\n",
      "5555 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.57124e-07\n",
      "5556 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.35074e-07\n",
      "5557 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.99437e-07\n",
      "5558 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.23099e-07\n",
      "5559 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.54281e-07\n",
      "5560 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.60871e-07\n",
      "5561 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.3368e-07\n",
      "5562 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.85437e-07\n",
      "5563 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 9.38555e-08\n",
      "5564 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 3.41746e-07\n",
      "5565 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.60835e-07\n",
      "5566 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.74393e-07\n",
      "5567 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.74285e-07\n",
      "5568 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.0684e-07\n",
      "5569 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.14741e-07\n",
      "5570 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.17035e-07\n",
      "5571 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.2797e-07\n",
      "5572 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.28847e-07\n",
      "5573 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.82805e-07\n",
      "5574 Train total loss: 3.02183 \tReconstruction loss: 3.02183 \tLatent loss: 6.21335e-07\n",
      "5575 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 4.4269e-07\n",
      "5576 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.54071e-07\n",
      "5577 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.67255e-07\n",
      "5578 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.8951e-07\n",
      "5579 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.99053e-07\n",
      "5580 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.77317e-07\n",
      "5581 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.77381e-07\n",
      "5582 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.18425e-07\n",
      "5583 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.32118e-07\n",
      "5584 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.66214e-07\n",
      "5585 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.80506e-07\n",
      "5586 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.92047e-07\n",
      "5587 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.45586e-07\n",
      "5588 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.51023e-07\n",
      "5589 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.31155e-07\n",
      "5590 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.73388e-07\n",
      "5591 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.39622e-07\n",
      "5592 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.83896e-07\n",
      "5593 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 4.79142e-07\n",
      "5594 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.84385e-07\n",
      "5595 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.60518e-07\n",
      "5596 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.42238e-07\n",
      "5597 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.99332e-07\n",
      "5598 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.69461e-07\n",
      "5599 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.27177e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.65811e-07\n",
      "5601 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.2752e-07\n",
      "5602 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.5635e-07\n",
      "5603 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.85883e-07\n",
      "5604 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.44234e-07\n",
      "5605 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.80021e-07\n",
      "5606 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.9864e-07\n",
      "5607 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.89662e-07\n",
      "5608 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.76211e-07\n",
      "5609 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.62976e-07\n",
      "5610 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.91444e-07\n",
      "5611 Train total loss: 2.94646 \tReconstruction loss: 2.94645 \tLatent loss: 1.98925e-07\n",
      "5612 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.72202e-07\n",
      "5613 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 4.10457e-07\n",
      "5614 Train total loss: 3.02181 \tReconstruction loss: 3.0218 \tLatent loss: 3.83263e-07\n",
      "5615 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.39204e-07\n",
      "5616 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.70231e-07\n",
      "5617 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.62297e-07\n",
      "5618 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.30908e-07\n",
      "5619 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 2.55418e-07\n",
      "5620 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 2.19425e-07\n",
      "5621 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.31482e-07\n",
      "5622 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.17573e-07\n",
      "5623 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.54896e-07\n",
      "5624 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.02917e-07\n",
      "5625 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 5.12786e-07\n",
      "5626 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.6859e-07\n",
      "5627 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.89012e-07\n",
      "5628 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.99447e-07\n",
      "5629 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.74782e-07\n",
      "5630 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.94874e-07\n",
      "5631 Train total loss: 2.94644 \tReconstruction loss: 2.94644 \tLatent loss: 6.57447e-07\n",
      "5632 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.50764e-07\n",
      "5633 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 4.91132e-07\n",
      "5634 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 5.2664e-07\n",
      "5635 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.30869e-07\n",
      "5636 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.39479e-07\n",
      "5637 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.58314e-07\n",
      "5638 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.93907e-07\n",
      "5639 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.96518e-07\n",
      "5640 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 3.01457e-07\n",
      "5641 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.932e-07\n",
      "5642 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.76718e-07\n",
      "5643 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.24447e-07\n",
      "5644 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.32835e-07\n",
      "5645 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.95309e-07\n",
      "5646 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.22718e-07\n",
      "5647 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.49257e-07\n",
      "5648 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.10345e-07\n",
      "5649 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.90396e-07\n",
      "5650 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.0391e-07\n",
      "5651 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.42089e-07\n",
      "5652 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.16589e-07\n",
      "5653 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.66142e-07\n",
      "5654 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.36564e-07\n",
      "5655 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.60667e-07\n",
      "5656 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.28029e-07\n",
      "5657 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.06336e-07\n",
      "5658 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.2002e-07\n",
      "5659 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.10042e-07\n",
      "5660 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.78279e-07\n",
      "5661 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.99424e-07\n",
      "5662 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 2.41793e-07\n",
      "5663 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.44619e-07\n",
      "5664 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.98037e-07\n",
      "5665 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.04286e-07\n",
      "5666 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.09687e-07\n",
      "5667 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.85263e-07\n",
      "5668 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.83344e-07\n",
      "5669 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.87172e-07\n",
      "5670 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.52009e-07\n",
      "5671 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.87819e-07\n",
      "5672 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.27665e-07\n",
      "5673 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.16505e-07\n",
      "5674 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 4.97187e-07\n",
      "5675 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.38238e-07\n",
      "5676 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.94304e-07\n",
      "5677 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.64247e-07\n",
      "5678 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.42395e-07\n",
      "5679 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.50008e-07\n",
      "5680 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 6.00456e-07\n",
      "5681 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.24206e-07\n",
      "5682 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.30566e-07\n",
      "5683 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.2934e-07\n",
      "5684 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 4.07122e-07\n",
      "5685 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.37266e-07\n",
      "5686 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.24909e-07\n",
      "5687 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.39035e-07\n",
      "5688 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.84515e-07\n",
      "5689 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.89505e-07\n",
      "5690 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.47394e-07\n",
      "5691 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.03729e-07\n",
      "5692 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.43913e-07\n",
      "5693 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.61177e-07\n",
      "5694 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 3.65888e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5695 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.65044e-07\n",
      "5696 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.72886e-07\n",
      "5697 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.60684e-07\n",
      "5698 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.24741e-07\n",
      "5699 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.60611e-07\n",
      "5700 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 2.00819e-07\n",
      "5701 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.92838e-07\n",
      "5702 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.77236e-07\n",
      "5703 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.21952e-07\n",
      "5704 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 8.69724e-08\n",
      "5705 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.13113e-07\n",
      "5706 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.22427e-07\n",
      "5707 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.24747e-07\n",
      "5708 Train total loss: 2.85423 \tReconstruction loss: 2.85423 \tLatent loss: 1.25376e-07\n",
      "5709 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.59477e-07\n",
      "5710 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.09208e-07\n",
      "5711 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.16879e-07\n",
      "5712 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.65273e-07\n",
      "5713 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.14339e-07\n",
      "5714 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.29136e-07\n",
      "5715 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.07746e-07\n",
      "5716 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.35537e-07\n",
      "5717 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.46208e-07\n",
      "5718 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.19881e-07\n",
      "5719 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.95458e-07\n",
      "5720 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.231e-07\n",
      "5721 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 7.64805e-08\n",
      "5722 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.486e-07\n",
      "5723 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.51087e-07\n",
      "5724 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 9.42207e-08\n",
      "5725 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.88196e-07\n",
      "5726 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.84848e-07\n",
      "5727 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.65443e-07\n",
      "5728 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 3.0731e-07\n",
      "5729 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.80579e-07\n",
      "5730 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 1.53987e-07\n",
      "5731 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.86615e-07\n",
      "5732 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.40067e-07\n",
      "5733 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.52781e-07\n",
      "5734 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 9.03773e-07\n",
      "5735 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.69833e-07\n",
      "5736 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.23123e-07\n",
      "5737 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.37156e-07\n",
      "5738 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.63041e-07\n",
      "5739 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.28421e-07\n",
      "5740 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 1.89524e-07\n",
      "5741 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.08331e-08\n",
      "5742 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.01504e-08\n",
      "5743 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.23018e-07\n",
      "5744 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.79904e-07\n",
      "5745 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.93558e-07\n",
      "5746 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.75891e-07\n",
      "5747 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.07769e-07\n",
      "5748 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.82145e-07\n",
      "5749 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.10771e-07\n",
      "5750 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.59265e-07\n",
      "5751 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.62746e-07\n",
      "5752 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.49229e-07\n",
      "5753 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.12844e-07\n",
      "5754 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.56458e-07\n",
      "5755 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 2.12991e-07\n",
      "5756 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.98906e-07\n",
      "5757 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.56638e-07\n",
      "5758 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.93155e-07\n",
      "5759 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.65307e-07\n",
      "5760 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.08618e-07\n",
      "5761 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.0277e-07\n",
      "5762 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.96391e-07\n",
      "5763 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.78599e-07\n",
      "5764 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.39289e-07\n",
      "5765 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.15122e-07\n",
      "5766 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.69109e-07\n",
      "5767 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.14245e-07\n",
      "5768 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.27438e-07\n",
      "5769 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.02058e-07\n",
      "5770 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.36345e-07\n",
      "5771 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.8771e-07\n",
      "5772 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 2.66683e-07\n",
      "5773 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.72321e-07\n",
      "5774 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.72893e-07\n",
      "5775 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.12218e-07\n",
      "5776 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.27609e-07\n",
      "5777 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.79655e-07\n",
      "5778 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.33582e-07\n",
      "5779 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.80364e-07\n",
      "5780 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.55069e-07\n",
      "5781 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.32914e-07\n",
      "5782 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.62311e-07\n",
      "5783 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.93336e-07\n",
      "5784 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.94761e-07\n",
      "5785 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.72401e-07\n",
      "5786 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.11211e-07\n",
      "5787 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.799e-07\n",
      "5788 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.86953e-07\n",
      "5789 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.42517e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5790 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.96292e-07\n",
      "5791 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.13661e-07\n",
      "5792 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.83741e-07\n",
      "5793 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.50829e-07\n",
      "5794 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.1134e-07\n",
      "5795 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.20253e-07\n",
      "5796 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.13618e-07\n",
      "5797 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.441e-07\n",
      "5798 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.58507e-07\n",
      "5799 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.54848e-07\n",
      "5800 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.08722e-07\n",
      "5801 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.00772e-07\n",
      "5802 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.50155e-07\n",
      "5803 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.74681e-07\n",
      "5804 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.91484e-07\n",
      "5805 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.91004e-07\n",
      "5806 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.25334e-07\n",
      "5807 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.24892e-07\n",
      "5808 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.20794e-07\n",
      "5809 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.16246e-07\n",
      "5810 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.66226e-07\n",
      "5811 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.8543e-07\n",
      "5812 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.37377e-07\n",
      "5813 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.31012e-07\n",
      "5814 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.48929e-07\n",
      "5815 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.89946e-07\n",
      "5816 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.34848e-07\n",
      "5817 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.12213e-07\n",
      "5818 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.6062e-07\n",
      "5819 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.59332e-07\n",
      "5820 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.75559e-07\n",
      "5821 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.58777e-07\n",
      "5822 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.45119e-07\n",
      "5823 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.45751e-07\n",
      "5824 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.74202e-07\n",
      "5825 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.38745e-07\n",
      "5826 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.14615e-07\n",
      "5827 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.6609e-07\n",
      "5828 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.42891e-08\n",
      "5829 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.25718e-07\n",
      "5830 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.48836e-07\n",
      "5831 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.84392e-07\n",
      "5832 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.02825e-07\n",
      "5833 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.95739e-07\n",
      "5834 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.57366e-07\n",
      "5835 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.07036e-07\n",
      "5836 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.75338e-07\n",
      "5837 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.31966e-07\n",
      "5838 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.04689e-06\n",
      "5839 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 5.82605e-07\n",
      "5840 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 4.33144e-07\n",
      "5841 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.7658e-07\n",
      "5842 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.15241e-07\n",
      "5843 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.30018e-07\n",
      "5844 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.83559e-07\n",
      "5845 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.22705e-07\n",
      "5846 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.84911e-07\n",
      "5847 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.66933e-07\n",
      "5848 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.32339e-07\n",
      "5849 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.70492e-07\n",
      "5850 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.33484e-07\n",
      "5851 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.79496e-07\n",
      "5852 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.52928e-07\n",
      "5853 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.05616e-07\n",
      "5854 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.59449e-07\n",
      "5855 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.36544e-07\n",
      "5856 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.77121e-07\n",
      "5857 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.59217e-07\n",
      "5858 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.40742e-07\n",
      "5859 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.14511e-07\n",
      "5860 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.3126e-07\n",
      "5861 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.66509e-08\n",
      "5862 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.78204e-07\n",
      "5863 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.39611e-07\n",
      "5864 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.6148e-07\n",
      "5865 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.41894e-07\n",
      "5866 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.33932e-07\n",
      "5867 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.69668e-07\n",
      "5868 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.48596e-08\n",
      "5869 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.60464e-07\n",
      "5870 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.05618e-07\n",
      "5871 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.34027e-07\n",
      "5872 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 4.18202e-07\n",
      "5873 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.27025e-07\n",
      "5874 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.13586e-07\n",
      "5875 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 7.56248e-08\n",
      "5876 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.90531e-07\n",
      "5877 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.94323e-07\n",
      "5878 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.30595e-07\n",
      "5879 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.07438e-07\n",
      "5880 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.51408e-07\n",
      "5881 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.47431e-07\n",
      "5882 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 9.31803e-08\n",
      "5883 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.3777e-07\n",
      "5884 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.96425e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5885 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.61595e-07\n",
      "5886 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.02921e-07\n",
      "5887 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.5074e-07\n",
      "5888 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.13059e-07\n",
      "5889 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.20408e-07\n",
      "5890 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.39235e-07\n",
      "5891 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.30593e-07\n",
      "5892 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.55465e-07\n",
      "5893 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.39576e-07\n",
      "5894 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.18918e-07\n",
      "5895 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.94719e-07\n",
      "5896 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.79896e-07\n",
      "5897 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.04906e-07\n",
      "5898 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.82162e-07\n",
      "5899 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.66029e-07\n",
      "5900 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.83807e-07\n",
      "5901 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.07039e-07\n",
      "5902 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.62388e-07\n",
      "5903 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.61961e-07\n",
      "5904 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.07096e-07\n",
      "5905 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.80042e-07\n",
      "5906 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.39233e-07\n",
      "5907 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.37719e-07\n",
      "5908 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.93685e-07\n",
      "5909 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 2.73855e-07\n",
      "5910 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.0195e-07\n",
      "5911 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.75028e-07\n",
      "5912 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.04132e-07\n",
      "5913 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.35462e-07\n",
      "5914 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.33186e-07\n",
      "5915 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.38905e-07\n",
      "5916 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.32177e-07\n",
      "5917 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.60481e-07\n",
      "5918 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 4.02183e-07\n",
      "5919 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.16562e-07\n",
      "5920 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.59268e-07\n",
      "5921 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.29517e-07\n",
      "5922 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.17031e-08\n",
      "5923 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.0263e-07\n",
      "5924 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.84779e-07\n",
      "5925 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.87298e-07\n",
      "5926 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.41143e-07\n",
      "5927 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.73551e-07\n",
      "5928 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.56569e-07\n",
      "5929 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.16565e-07\n",
      "5930 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 5.37036e-07\n",
      "5931 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.11113e-07\n",
      "5932 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.78428e-07\n",
      "5933 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 1.67696e-07\n",
      "5934 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.8249e-07\n",
      "5935 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.29327e-07\n",
      "5936 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.91695e-07\n",
      "5937 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.7344e-07\n",
      "5938 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.8771e-07\n",
      "5939 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 4.75306e-07\n",
      "5940 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.93934e-07\n",
      "5941 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.42162e-07\n",
      "5942 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.71856e-07\n",
      "5943 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.3447e-07\n",
      "5944 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.31597e-07\n",
      "5945 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.2154e-07\n",
      "5946 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.45107e-07\n",
      "5947 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.06412e-07\n",
      "5948 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.72871e-07\n",
      "5949 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.00174e-07\n",
      "5950 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.99367e-07\n",
      "5951 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.89061e-07\n",
      "5952 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.53382e-07\n",
      "5953 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 2.20491e-07\n",
      "5954 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.77346e-07\n",
      "5955 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.55783e-07\n",
      "5956 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.40984e-07\n",
      "5957 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.73506e-07\n",
      "5958 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.3665e-07\n",
      "5959 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 6.2569e-07\n",
      "5960 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.93954e-07\n",
      "5961 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.8834e-07\n",
      "5962 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.5051e-07\n",
      "5963 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.27759e-07\n",
      "5964 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.23667e-07\n",
      "5965 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.27107e-07\n",
      "5966 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.35142e-07\n",
      "5967 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.21024e-07\n",
      "5968 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 3.42033e-07\n",
      "5969 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.0867e-07\n",
      "5970 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.41745e-07\n",
      "5971 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.75796e-07\n",
      "5972 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.69818e-07\n",
      "5973 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.30972e-07\n",
      "5974 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.15432e-07\n",
      "5975 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.45737e-07\n",
      "5976 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.38488e-07\n",
      "5977 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.35447e-07\n",
      "5978 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.34653e-07\n",
      "5979 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 3.49389e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5980 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.94495e-07\n",
      "5981 Train total loss: 2.80587 \tReconstruction loss: 2.80586 \tLatent loss: 4.33534e-07\n",
      "5982 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.23896e-07\n",
      "5983 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.58403e-07\n",
      "5984 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.405e-07\n",
      "5985 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.59675e-07\n",
      "5986 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 3.33227e-07\n",
      "5987 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.69019e-07\n",
      "5988 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.35202e-07\n",
      "5989 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.22287e-07\n",
      "5990 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.3484e-07\n",
      "5991 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.56481e-07\n",
      "5992 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 7.94046e-07\n",
      "5993 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.27005e-06\n",
      "5994 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.90907e-07\n",
      "5995 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.56221e-07\n",
      "5996 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.32965e-07\n",
      "5997 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.23077e-07\n",
      "5998 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 9.24469e-08\n",
      "5999 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.88898e-07\n",
      "6000 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 1.80992e-07\n",
      "6001 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.89503e-07\n",
      "6002 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.30492e-07\n",
      "6003 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.49547e-07\n",
      "6004 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.36238e-07\n",
      "6005 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.62569e-07\n",
      "6006 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.59904e-07\n",
      "6007 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.36892e-07\n",
      "6008 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.49475e-07\n",
      "6009 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.98479e-07\n",
      "6010 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.27938e-07\n",
      "6011 Train total loss: 2.94646 \tReconstruction loss: 2.94645 \tLatent loss: 4.80597e-07\n",
      "6012 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.86161e-07\n",
      "6013 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.62049e-07\n",
      "6014 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 4.79307e-07\n",
      "6015 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.10179e-08\n",
      "6016 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.6745e-07\n",
      "6017 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.34241e-07\n",
      "6018 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.80544e-07\n",
      "6019 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.32495e-07\n",
      "6020 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.83762e-07\n",
      "6021 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.18525e-07\n",
      "6022 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.18012e-07\n",
      "6023 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.59024e-07\n",
      "6024 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.28045e-07\n",
      "6025 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.79944e-07\n",
      "6026 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 3.64817e-07\n",
      "6027 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.84783e-07\n",
      "6028 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.23599e-07\n",
      "6029 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.77414e-07\n",
      "6030 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.7377e-07\n",
      "6031 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.86927e-07\n",
      "6032 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.97218e-07\n",
      "6033 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.91914e-07\n",
      "6034 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 2.70447e-07\n",
      "6035 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.28421e-07\n",
      "6036 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.38098e-07\n",
      "6037 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.02486e-07\n",
      "6038 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.21255e-07\n",
      "6039 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 3.65553e-07\n",
      "6040 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.93177e-07\n",
      "6041 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 5.68281e-08\n",
      "6042 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.92724e-07\n",
      "6043 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.72762e-07\n",
      "6044 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 7.23361e-08\n",
      "6045 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 5.84089e-07\n",
      "6046 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.83407e-07\n",
      "6047 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.66181e-07\n",
      "6048 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.6405e-08\n",
      "6049 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.78101e-07\n",
      "6050 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.02921e-07\n",
      "6051 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.74958e-07\n",
      "6052 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.03442e-07\n",
      "6053 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.06498e-07\n",
      "6054 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.19051e-07\n",
      "6055 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.01989e-07\n",
      "6056 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.3892e-07\n",
      "6057 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.97303e-07\n",
      "6058 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.28629e-07\n",
      "6059 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.71789e-07\n",
      "6060 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.40165e-07\n",
      "6061 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.83985e-07\n",
      "6062 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.07023e-07\n",
      "6063 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.4392e-07\n",
      "6064 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.9641e-07\n",
      "6065 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.85601e-07\n",
      "6066 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.70085e-07\n",
      "6067 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.24735e-07\n",
      "6068 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.62471e-07\n",
      "6069 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.83602e-07\n",
      "6070 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.97854e-07\n",
      "6071 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.75498e-07\n",
      "6072 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.76312e-07\n",
      "6073 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.40255e-07\n",
      "6074 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.81967e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6075 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.39426e-07\n",
      "6076 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.43014e-07\n",
      "6077 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.44252e-07\n",
      "6078 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.5368e-07\n",
      "6079 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.71619e-07\n",
      "6080 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 1.7409e-07\n",
      "6081 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.32487e-08\n",
      "6082 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.45297e-07\n",
      "6083 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.4924e-07\n",
      "6084 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.73596e-07\n",
      "6085 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.42106e-07\n",
      "6086 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.21035e-07\n",
      "6087 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.34883e-07\n",
      "6088 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.14434e-07\n",
      "6089 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.5813e-07\n",
      "6090 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.05119e-07\n",
      "6091 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.88427e-07\n",
      "6092 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.82849e-07\n",
      "6093 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.98593e-07\n",
      "6094 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.09855e-07\n",
      "6095 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.5634e-07\n",
      "6096 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.8379e-07\n",
      "6097 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.56551e-07\n",
      "6098 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.90943e-07\n",
      "6099 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 4.51675e-07\n",
      "6100 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 3.12561e-07\n",
      "6101 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.0513e-07\n",
      "6102 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.58422e-07\n",
      "6103 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.95829e-06\n",
      "6104 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 3.047e-07\n",
      "6105 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.23792e-07\n",
      "6106 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.72015e-07\n",
      "6107 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.03925e-06\n",
      "6108 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.26861e-07\n",
      "6109 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.4473e-07\n",
      "6110 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.22497e-07\n",
      "6111 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.11017e-07\n",
      "6112 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.34954e-07\n",
      "6113 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.63056e-07\n",
      "6114 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.79074e-07\n",
      "6115 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.49687e-07\n",
      "6116 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.70413e-07\n",
      "6117 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.09753e-07\n",
      "6118 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.64933e-07\n",
      "6119 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.92474e-07\n",
      "6120 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.8747e-07\n",
      "6121 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.97296e-07\n",
      "6122 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 3.08089e-07\n",
      "6123 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.02572e-07\n",
      "6124 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.12779e-07\n",
      "6125 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.49003e-07\n",
      "6126 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.31772e-07\n",
      "6127 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.15675e-07\n",
      "6128 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.66218e-07\n",
      "6129 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.83348e-07\n",
      "6130 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.31945e-07\n",
      "6131 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.95249e-07\n",
      "6132 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.38892e-07\n",
      "6133 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.1934e-07\n",
      "6134 Train total loss: 3.02181 \tReconstruction loss: 3.0218 \tLatent loss: 4.42992e-07\n",
      "6135 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.72389e-07\n",
      "6136 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.33201e-07\n",
      "6137 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.51587e-07\n",
      "6138 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.20087e-07\n",
      "6139 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.71538e-07\n",
      "6140 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.54237e-07\n",
      "6141 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.33817e-07\n",
      "6142 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.79769e-07\n",
      "6143 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.48083e-07\n",
      "6144 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 3.69248e-07\n",
      "6145 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.23282e-07\n",
      "6146 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.20261e-07\n",
      "6147 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.25161e-07\n",
      "6148 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 8.65926e-08\n",
      "6149 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.08256e-07\n",
      "6150 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.58249e-07\n",
      "6151 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.63663e-07\n",
      "6152 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.85268e-07\n",
      "6153 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.17788e-07\n",
      "6154 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.18052e-07\n",
      "6155 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.90225e-07\n",
      "6156 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.56496e-07\n",
      "6157 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.43727e-07\n",
      "6158 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.39103e-07\n",
      "6159 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.99057e-07\n",
      "6160 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.59639e-06\n",
      "6161 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 4.70968e-07\n",
      "6162 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.66763e-07\n",
      "6163 Train total loss: 2.86595 \tReconstruction loss: 2.86595 \tLatent loss: 1.02297e-07\n",
      "6164 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.62754e-07\n",
      "6165 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 6.00892e-08\n",
      "6166 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.52754e-07\n",
      "6167 Train total loss: 2.92513 \tReconstruction loss: 2.92513 \tLatent loss: 1.97007e-07\n",
      "6168 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.10209e-07\n",
      "6169 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 9.79355e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6170 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.57474e-07\n",
      "6171 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.32005e-07\n",
      "6172 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.06007e-07\n",
      "6173 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.51199e-07\n",
      "6174 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 7.40815e-07\n",
      "6175 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 5.68962e-07\n",
      "6176 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.59552e-07\n",
      "6177 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.98424e-07\n",
      "6178 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.49661e-07\n",
      "6179 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.01399e-07\n",
      "6180 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.96812e-07\n",
      "6181 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.6021e-07\n",
      "6182 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.12158e-07\n",
      "6183 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.71391e-07\n",
      "6184 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 2.70762e-07\n",
      "6185 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 1.31993e-07\n",
      "6186 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.91233e-07\n",
      "6187 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.67599e-07\n",
      "6188 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.92344e-07\n",
      "6189 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.23956e-07\n",
      "6190 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 8.56761e-07\n",
      "6191 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.68006e-06\n",
      "6192 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.16273e-07\n",
      "6193 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.84811e-07\n",
      "6194 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 1.28284e-07\n",
      "6195 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.34658e-07\n",
      "6196 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.28411e-08\n",
      "6197 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.16263e-07\n",
      "6198 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 6.09507e-08\n",
      "6199 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.30333e-07\n",
      "6200 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.60065e-07\n",
      "6201 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.01549e-07\n",
      "6202 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 9.08592e-08\n",
      "6203 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.64626e-07\n",
      "6204 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.65292e-07\n",
      "6205 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.0815e-07\n",
      "6206 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.36736e-07\n",
      "6207 Train total loss: 2.92515 \tReconstruction loss: 2.92514 \tLatent loss: 2.28893e-06\n",
      "6208 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.49397e-07\n",
      "6209 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.11524e-07\n",
      "6210 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 6.74496e-08\n",
      "6211 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.58055e-07\n",
      "6212 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 8.69535e-08\n",
      "6213 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.73901e-08\n",
      "6214 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 1.1909e-07\n",
      "6215 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.05398e-07\n",
      "6216 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.0701e-07\n",
      "6217 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.13598e-08\n",
      "6218 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.93046e-07\n",
      "6219 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.24001e-07\n",
      "6220 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.32729e-07\n",
      "6221 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 9.95307e-08\n",
      "6222 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.23643e-07\n",
      "6223 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.26628e-07\n",
      "6224 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.08969e-07\n",
      "6225 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.99904e-07\n",
      "6226 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.22152e-07\n",
      "6227 Train total loss: 2.92515 \tReconstruction loss: 2.92514 \tLatent loss: 1.5198e-07\n",
      "6228 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.15976e-07\n",
      "6229 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.04676e-07\n",
      "6230 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.28124e-07\n",
      "6231 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.83027e-07\n",
      "6232 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.33846e-07\n",
      "6233 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.19661e-07\n",
      "6234 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.61012e-07\n",
      "6235 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.08522e-07\n",
      "6236 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.91815e-07\n",
      "6237 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.03266e-07\n",
      "6238 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.71903e-07\n",
      "6239 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.40617e-07\n",
      "6240 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.74964e-08\n",
      "6241 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.43773e-07\n",
      "6242 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.1618e-07\n",
      "6243 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.84681e-07\n",
      "6244 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.50493e-07\n",
      "6245 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 2.92314e-07\n",
      "6246 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.92945e-07\n",
      "6247 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.1133e-07\n",
      "6248 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.28023e-07\n",
      "6249 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.59671e-07\n",
      "6250 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.6597e-07\n",
      "6251 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.87226e-07\n",
      "6252 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.09382e-07\n",
      "6253 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 2.29132e-07\n",
      "6254 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.08313e-07\n",
      "6255 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.1731e-07\n",
      "6256 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.21296e-07\n",
      "6257 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.00158e-07\n",
      "6258 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 3.49242e-07\n",
      "6259 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.93325e-07\n",
      "6260 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.39765e-07\n",
      "6261 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.34124e-07\n",
      "6262 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.33679e-07\n",
      "6263 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.13293e-07\n",
      "6264 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.94053e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6265 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.86347e-07\n",
      "6266 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.73268e-07\n",
      "6267 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.07478e-07\n",
      "6268 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.38351e-08\n",
      "6269 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.54492e-07\n",
      "6270 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.80098e-07\n",
      "6271 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.73933e-07\n",
      "6272 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.21454e-07\n",
      "6273 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.5773e-07\n",
      "6274 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.72781e-07\n",
      "6275 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.2208e-07\n",
      "6276 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.53878e-07\n",
      "6277 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.93182e-07\n",
      "6278 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.87018e-07\n",
      "6279 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.05905e-07\n",
      "6280 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.02272e-07\n",
      "6281 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.59204e-07\n",
      "6282 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.3129e-07\n",
      "6283 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.18109e-07\n",
      "6284 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.16898e-07\n",
      "6285 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.87288e-07\n",
      "6286 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.98242e-07\n",
      "6287 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.4225e-07\n",
      "6288 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.16464e-07\n",
      "6289 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.66334e-07\n",
      "6290 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.27984e-07\n",
      "6291 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.92216e-07\n",
      "6292 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.77383e-07\n",
      "6293 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.35523e-07\n",
      "6294 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.2282e-07\n",
      "6295 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.36101e-07\n",
      "6296 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.44543e-07\n",
      "6297 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.09139e-07\n",
      "6298 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.02085e-07\n",
      "6299 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.36943e-07\n",
      "6300 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.35342e-07\n",
      "6301 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.11172e-07\n",
      "6302 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.97664e-07\n",
      "6303 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.87487e-07\n",
      "6304 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.21927e-07\n",
      "6305 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.59266e-07\n",
      "6306 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.11484e-07\n",
      "6307 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.63249e-08\n",
      "6308 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 7.82587e-08\n",
      "6309 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.14639e-07\n",
      "6310 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.57412e-07\n",
      "6311 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.28835e-07\n",
      "6312 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.75963e-07\n",
      "6313 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.68153e-07\n",
      "6314 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.22898e-07\n",
      "6315 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 2.8408e-07\n",
      "6316 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 7.38728e-08\n",
      "6317 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.75191e-07\n",
      "6318 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 2.54219e-07\n",
      "6319 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.99942e-07\n",
      "6320 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.59996e-07\n",
      "6321 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.50051e-07\n",
      "6322 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.27726e-07\n",
      "6323 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 7.14186e-07\n",
      "6324 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 5.89679e-07\n",
      "6325 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.92499e-08\n",
      "6326 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.72967e-07\n",
      "6327 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.43865e-07\n",
      "6328 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.8967e-07\n",
      "6329 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.04158e-07\n",
      "6330 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.76593e-07\n",
      "6331 Train total loss: 2.94646 \tReconstruction loss: 2.94645 \tLatent loss: 2.22863e-07\n",
      "6332 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.11565e-07\n",
      "6333 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 8.86314e-08\n",
      "6334 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.48553e-07\n",
      "6335 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.40895e-07\n",
      "6336 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.07883e-07\n",
      "6337 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.76393e-07\n",
      "6338 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.16097e-07\n",
      "6339 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.9863e-07\n",
      "6340 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.9797e-07\n",
      "6341 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.47875e-07\n",
      "6342 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.96673e-07\n",
      "6343 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 5.48104e-07\n",
      "6344 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.24731e-07\n",
      "6345 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.60907e-08\n",
      "6346 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.2739e-07\n",
      "6347 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 8.83563e-08\n",
      "6348 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.30479e-08\n",
      "6349 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.27172e-07\n",
      "6350 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.56498e-07\n",
      "6351 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 3.522e-07\n",
      "6352 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.27676e-07\n",
      "6353 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.22286e-07\n",
      "6354 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.6533e-08\n",
      "6355 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.75367e-07\n",
      "6356 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.17267e-07\n",
      "6357 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.73732e-07\n",
      "6358 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 6.17182e-07\n",
      "6359 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.26434e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6360 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 1.67202e-07\n",
      "6361 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.49412e-07\n",
      "6362 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.26404e-07\n",
      "6363 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.46317e-07\n",
      "6364 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.50054e-07\n",
      "6365 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.51288e-07\n",
      "6366 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.17149e-07\n",
      "6367 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.20913e-07\n",
      "6368 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.61785e-07\n",
      "6369 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.42494e-07\n",
      "6370 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.07103e-07\n",
      "6371 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.53769e-07\n",
      "6372 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.64896e-07\n",
      "6373 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.69296e-07\n",
      "6374 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 8.55944e-08\n",
      "6375 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.56605e-07\n",
      "6376 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.89088e-07\n",
      "6377 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.94427e-07\n",
      "6378 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.04171e-07\n",
      "6379 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 2.78855e-07\n",
      "6380 Train total loss: 3.03979 \tReconstruction loss: 3.03978 \tLatent loss: 1.68273e-07\n",
      "6381 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.57135e-07\n",
      "6382 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.72012e-07\n",
      "6383 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.11671e-07\n",
      "6384 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.84146e-07\n",
      "6385 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.48781e-07\n",
      "6386 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.17035e-07\n",
      "6387 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.02697e-07\n",
      "6388 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.21354e-07\n",
      "6389 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.44805e-07\n",
      "6390 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.39433e-07\n",
      "6391 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.60927e-07\n",
      "6392 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 4.36668e-07\n",
      "6393 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.54007e-07\n",
      "6394 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.76961e-07\n",
      "6395 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.3392e-07\n",
      "6396 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.7595e-07\n",
      "6397 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.00418e-07\n",
      "6398 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.16523e-07\n",
      "6399 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.71314e-07\n",
      "6400 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.18811e-07\n",
      "6401 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.63664e-07\n",
      "6402 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.62454e-07\n",
      "6403 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.00263e-07\n",
      "6404 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.6368e-07\n",
      "6405 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.81791e-07\n",
      "6406 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.20269e-07\n",
      "6407 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.91212e-07\n",
      "6408 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 9.13016e-08\n",
      "6409 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.18554e-07\n",
      "6410 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 5.00129e-07\n",
      "6411 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.83574e-07\n",
      "6412 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.26696e-07\n",
      "6413 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.78646e-07\n",
      "6414 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.96517e-07\n",
      "6415 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.69562e-07\n",
      "6416 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.58398e-07\n",
      "6417 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.87528e-07\n",
      "6418 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.54617e-07\n",
      "6419 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.67086e-07\n",
      "6420 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.43971e-07\n",
      "6421 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.30445e-07\n",
      "6422 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.10184e-07\n",
      "6423 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.81782e-07\n",
      "6424 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.19938e-07\n",
      "6425 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.5721e-07\n",
      "6426 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.85607e-07\n",
      "6427 Train total loss: 2.92515 \tReconstruction loss: 2.92514 \tLatent loss: 1.98108e-07\n",
      "6428 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.89699e-07\n",
      "6429 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.84123e-07\n",
      "6430 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.24752e-07\n",
      "6431 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.89651e-07\n",
      "6432 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.50471e-07\n",
      "6433 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.70801e-07\n",
      "6434 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.04086e-07\n",
      "6435 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.48053e-07\n",
      "6436 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 9.39093e-08\n",
      "6437 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.84834e-07\n",
      "6438 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.75701e-07\n",
      "6439 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.5332e-07\n",
      "6440 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.36133e-07\n",
      "6441 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.36821e-07\n",
      "6442 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.68064e-07\n",
      "6443 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.88975e-07\n",
      "6444 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.31319e-07\n",
      "6445 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.52898e-07\n",
      "6446 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.35555e-07\n",
      "6447 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.84512e-07\n",
      "6448 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.12276e-07\n",
      "6449 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.62865e-07\n",
      "6450 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.67308e-07\n",
      "6451 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.79526e-07\n",
      "6452 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.94655e-07\n",
      "6453 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.21173e-07\n",
      "6454 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 3.52605e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6455 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.60584e-07\n",
      "6456 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.28971e-07\n",
      "6457 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.5739e-07\n",
      "6458 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.42433e-07\n",
      "6459 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.19633e-07\n",
      "6460 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.50646e-07\n",
      "6461 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.67999e-07\n",
      "6462 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.16524e-07\n",
      "6463 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.07538e-07\n",
      "6464 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.58743e-07\n",
      "6465 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.02307e-07\n",
      "6466 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.27761e-07\n",
      "6467 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.56724e-07\n",
      "6468 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.68076e-07\n",
      "6469 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.70952e-07\n",
      "6470 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.94351e-07\n",
      "6471 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.59076e-07\n",
      "6472 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.20438e-07\n",
      "6473 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.45607e-07\n",
      "6474 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 3.23288e-07\n",
      "6475 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.6504e-07\n",
      "6476 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.55022e-07\n",
      "6477 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.9115e-07\n",
      "6478 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.17351e-07\n",
      "6479 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.61279e-07\n",
      "6480 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.88037e-07\n",
      "6481 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.26059e-07\n",
      "6482 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.18938e-07\n",
      "6483 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.04107e-07\n",
      "6484 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.62479e-07\n",
      "6485 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.25867e-07\n",
      "6486 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.22274e-07\n",
      "6487 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.21564e-07\n",
      "6488 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.24963e-07\n",
      "6489 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.8773e-07\n",
      "6490 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.34204e-07\n",
      "6491 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.22979e-07\n",
      "6492 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.0151e-07\n",
      "6493 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.97221e-07\n",
      "6494 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.37047e-07\n",
      "6495 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.44246e-07\n",
      "6496 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.50099e-07\n",
      "6497 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.06395e-07\n",
      "6498 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.48724e-07\n",
      "6499 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 7.01907e-07\n",
      "6500 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.88845e-07\n",
      "6501 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.32676e-07\n",
      "6502 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.69686e-07\n",
      "6503 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.64649e-07\n",
      "6504 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.76107e-07\n",
      "6505 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.34832e-07\n",
      "6506 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.38123e-07\n",
      "6507 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.949e-07\n",
      "6508 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.20339e-07\n",
      "6509 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.85307e-07\n",
      "6510 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.79748e-07\n",
      "6511 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.96067e-07\n",
      "6512 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 2.1757e-07\n",
      "6513 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.16891e-07\n",
      "6514 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.32318e-07\n",
      "6515 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.90971e-07\n",
      "6516 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.81585e-07\n",
      "6517 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.93702e-07\n",
      "6518 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.65473e-07\n",
      "6519 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.818e-07\n",
      "6520 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 3.55765e-07\n",
      "6521 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.00985e-07\n",
      "6522 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.88648e-07\n",
      "6523 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.32818e-07\n",
      "6524 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.17692e-07\n",
      "6525 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.8795e-07\n",
      "6526 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 4.11461e-07\n",
      "6527 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.29989e-07\n",
      "6528 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 6.49845e-07\n",
      "6529 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.7166e-07\n",
      "6530 Train total loss: 2.98598 \tReconstruction loss: 2.98598 \tLatent loss: 1.73546e-07\n",
      "6531 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.39409e-07\n",
      "6532 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.65271e-07\n",
      "6533 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 3.20603e-07\n",
      "6534 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.69712e-07\n",
      "6535 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.02364e-08\n",
      "6536 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.08936e-07\n",
      "6537 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.37606e-07\n",
      "6538 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.99468e-08\n",
      "6539 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.65425e-07\n",
      "6540 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.6517e-07\n",
      "6541 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.80782e-07\n",
      "6542 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.19367e-07\n",
      "6543 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.28822e-07\n",
      "6544 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.45134e-07\n",
      "6545 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.79817e-07\n",
      "6546 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.32625e-07\n",
      "6547 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 6.81431e-07\n",
      "6548 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.36846e-07\n",
      "6549 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.06602e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6550 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.8304e-07\n",
      "6551 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.65434e-07\n",
      "6552 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.69624e-07\n",
      "6553 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.63783e-07\n",
      "6554 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 2.41509e-07\n",
      "6555 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.12211e-07\n",
      "6556 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.41259e-07\n",
      "6557 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.09643e-07\n",
      "6558 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.851e-07\n",
      "6559 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.73072e-07\n",
      "6560 Train total loss: 3.0398 \tReconstruction loss: 3.03979 \tLatent loss: 3.60121e-07\n",
      "6561 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.42091e-07\n",
      "6562 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.52246e-07\n",
      "6563 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 7.51097e-08\n",
      "6564 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.51256e-07\n",
      "6565 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.00999e-07\n",
      "6566 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 5.28136e-07\n",
      "6567 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.06484e-07\n",
      "6568 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.06241e-07\n",
      "6569 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.20076e-07\n",
      "6570 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.19589e-07\n",
      "6571 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 1.77302e-07\n",
      "6572 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.57171e-07\n",
      "6573 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.51571e-07\n",
      "6574 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.31621e-07\n",
      "6575 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.58446e-07\n",
      "6576 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 9.81105e-08\n",
      "6577 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.19417e-07\n",
      "6578 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.7456e-07\n",
      "6579 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.36171e-07\n",
      "6580 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.76545e-07\n",
      "6581 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.29624e-07\n",
      "6582 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.71582e-07\n",
      "6583 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.68043e-07\n",
      "6584 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.46528e-07\n",
      "6585 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.46913e-07\n",
      "6586 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.23986e-07\n",
      "6587 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.16549e-07\n",
      "6588 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.80245e-07\n",
      "6589 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 5.97953e-07\n",
      "6590 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.68024e-07\n",
      "6591 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.56239e-07\n",
      "6592 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.34417e-07\n",
      "6593 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.65567e-07\n",
      "6594 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.32464e-07\n",
      "6595 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.61502e-07\n",
      "6596 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.61481e-07\n",
      "6597 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.44474e-07\n",
      "6598 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.51616e-07\n",
      "6599 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.22717e-07\n",
      "6600 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 4.75995e-07\n",
      "6601 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.46205e-07\n",
      "6602 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.50244e-07\n",
      "6603 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.18757e-07\n",
      "6604 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.78761e-08\n",
      "6605 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.175e-07\n",
      "6606 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 4.49593e-07\n",
      "6607 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.71812e-07\n",
      "6608 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.33715e-07\n",
      "6609 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.67081e-07\n",
      "6610 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.07582e-07\n",
      "6611 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.47856e-07\n",
      "6612 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.60401e-07\n",
      "6613 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.11986e-07\n",
      "6614 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.85847e-07\n",
      "6615 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.99653e-07\n",
      "6616 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.41427e-07\n",
      "6617 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.15555e-07\n",
      "6618 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.10114e-07\n",
      "6619 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.74448e-07\n",
      "6620 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 2.24976e-07\n",
      "6621 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.33889e-07\n",
      "6622 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.0664e-07\n",
      "6623 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.15967e-07\n",
      "6624 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 1.44774e-07\n",
      "6625 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.21112e-07\n",
      "6626 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.85515e-07\n",
      "6627 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.2801e-07\n",
      "6628 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.22742e-07\n",
      "6629 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.3636e-07\n",
      "6630 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.88164e-07\n",
      "6631 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.78581e-07\n",
      "6632 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.73949e-07\n",
      "6633 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.0851e-07\n",
      "6634 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.12175e-07\n",
      "6635 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.32469e-07\n",
      "6636 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.16247e-07\n",
      "6637 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 1.96905e-07\n",
      "6638 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.60807e-07\n",
      "6639 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 2.77096e-07\n",
      "6640 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.78342e-07\n",
      "6641 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 6.58794e-08\n",
      "6642 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.71256e-07\n",
      "6643 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.30356e-07\n",
      "6644 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.47919e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6645 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.20082e-07\n",
      "6646 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.83652e-07\n",
      "6647 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.60008e-07\n",
      "6648 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.25776e-07\n",
      "6649 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.18307e-07\n",
      "6650 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.88424e-07\n",
      "6651 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.75469e-07\n",
      "6652 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.57659e-07\n",
      "6653 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.21728e-07\n",
      "6654 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.34321e-07\n",
      "6655 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.17617e-07\n",
      "6656 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.10411e-07\n",
      "6657 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.79804e-07\n",
      "6658 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.05536e-07\n",
      "6659 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.69961e-07\n",
      "6660 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.57249e-07\n",
      "6661 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.30021e-07\n",
      "6662 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.26861e-07\n",
      "6663 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.40975e-07\n",
      "6664 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.9476e-07\n",
      "6665 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.43395e-07\n",
      "6666 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.41873e-07\n",
      "6667 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.38966e-07\n",
      "6668 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.66934e-07\n",
      "6669 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.87544e-07\n",
      "6670 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.54417e-07\n",
      "6671 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.58906e-07\n",
      "6672 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.77038e-07\n",
      "6673 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.43413e-07\n",
      "6674 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.71336e-07\n",
      "6675 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.55843e-07\n",
      "6676 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.81481e-07\n",
      "6677 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.27349e-07\n",
      "6678 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.15071e-07\n",
      "6679 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.18101e-07\n",
      "6680 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.35927e-07\n",
      "6681 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.70942e-07\n",
      "6682 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.44872e-07\n",
      "6683 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.36434e-07\n",
      "6684 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.18928e-07\n",
      "6685 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.9485e-07\n",
      "6686 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.67135e-07\n",
      "6687 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.41544e-07\n",
      "6688 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 2.05358e-07\n",
      "6689 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.38546e-07\n",
      "6690 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.91833e-07\n",
      "6691 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.62337e-07\n",
      "6692 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 4.50992e-07\n",
      "6693 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.86364e-07\n",
      "6694 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.23752e-07\n",
      "6695 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.1048e-07\n",
      "6696 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.28515e-07\n",
      "6697 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.3925e-07\n",
      "6698 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.00862e-07\n",
      "6699 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 6.5904e-07\n",
      "6700 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 3.07547e-07\n",
      "6701 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 8.95336e-08\n",
      "6702 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.88873e-07\n",
      "6703 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.8854e-07\n",
      "6704 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.85869e-07\n",
      "6705 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.67911e-07\n",
      "6706 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.94859e-07\n",
      "6707 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.78487e-07\n",
      "6708 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.03563e-07\n",
      "6709 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.38496e-07\n",
      "6710 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.64726e-07\n",
      "6711 Train total loss: 2.94647 \tReconstruction loss: 2.94647 \tLatent loss: 1.84221e-07\n",
      "6712 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.30042e-07\n",
      "6713 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.16359e-07\n",
      "6714 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.00173e-07\n",
      "6715 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.36687e-07\n",
      "6716 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.4939e-07\n",
      "6717 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.75516e-07\n",
      "6718 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.38352e-07\n",
      "6719 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.41549e-07\n",
      "6720 Train total loss: 3.0398 \tReconstruction loss: 3.03979 \tLatent loss: 1.98203e-07\n",
      "6721 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.72212e-07\n",
      "6722 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.25309e-07\n",
      "6723 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.73488e-07\n",
      "6724 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.08052e-07\n",
      "6725 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.84367e-07\n",
      "6726 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 7.89233e-07\n",
      "6727 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.69444e-07\n",
      "6728 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.17777e-07\n",
      "6729 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.76503e-07\n",
      "6730 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 9.47388e-08\n",
      "6731 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.07423e-07\n",
      "6732 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.11257e-07\n",
      "6733 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.67083e-07\n",
      "6734 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.54653e-07\n",
      "6735 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.58686e-07\n",
      "6736 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.01677e-07\n",
      "6737 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.30867e-07\n",
      "6738 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 3.07708e-07\n",
      "6739 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.58706e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6740 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 8.41213e-07\n",
      "6741 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.80538e-07\n",
      "6742 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.576e-07\n",
      "6743 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 2.60635e-07\n",
      "6744 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.53937e-07\n",
      "6745 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.07161e-07\n",
      "6746 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.82226e-07\n",
      "6747 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.22176e-07\n",
      "6748 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.52076e-07\n",
      "6749 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.15205e-07\n",
      "6750 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.42287e-07\n",
      "6751 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.07718e-07\n",
      "6752 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 8.00437e-07\n",
      "6753 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 2.66399e-07\n",
      "6754 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.39109e-07\n",
      "6755 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.81024e-07\n",
      "6756 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.4036e-07\n",
      "6757 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.7919e-07\n",
      "6758 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.93153e-07\n",
      "6759 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.52948e-07\n",
      "6760 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.59393e-08\n",
      "6761 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.1771e-07\n",
      "6762 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.96322e-07\n",
      "6763 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.49531e-07\n",
      "6764 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.17339e-07\n",
      "6765 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.3875e-07\n",
      "6766 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.75745e-07\n",
      "6767 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.28665e-07\n",
      "6768 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.42891e-07\n",
      "6769 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.5013e-07\n",
      "6770 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.85281e-07\n",
      "6771 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.16473e-07\n",
      "6772 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 6.29113e-07\n",
      "6773 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 8.34919e-07\n",
      "6774 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.20159e-07\n",
      "6775 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.07107e-07\n",
      "6776 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.29328e-07\n",
      "6777 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.42568e-07\n",
      "6778 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.41144e-07\n",
      "6779 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.71718e-07\n",
      "6780 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 3.21245e-07\n",
      "6781 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.49364e-07\n",
      "6782 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.7319e-07\n",
      "6783 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.79248e-07\n",
      "6784 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 5.71854e-07\n",
      "6785 Train total loss: 2.91475 \tReconstruction loss: 2.91474 \tLatent loss: 1.52447e-06\n",
      "6786 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 4.88809e-07\n",
      "6787 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.67448e-07\n",
      "6788 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.50823e-07\n",
      "6789 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.69899e-07\n",
      "6790 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.44135e-07\n",
      "6791 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.08815e-07\n",
      "6792 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.44004e-07\n",
      "6793 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.25843e-07\n",
      "6794 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 8.00587e-06\n",
      "6795 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.45617e-07\n",
      "6796 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.32314e-07\n",
      "6797 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.33345e-08\n",
      "6798 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.1051e-08\n",
      "6799 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.06385e-07\n",
      "6800 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 6.63073e-08\n",
      "6801 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.95637e-08\n",
      "6802 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.02809e-07\n",
      "6803 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 9.62464e-08\n",
      "6804 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 6.07572e-08\n",
      "6805 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 6.37097e-08\n",
      "6806 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.56366e-07\n",
      "6807 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.65374e-08\n",
      "6808 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.1492e-08\n",
      "6809 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.06104e-07\n",
      "6810 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 7.73638e-08\n",
      "6811 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.09713e-07\n",
      "6812 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.07777e-07\n",
      "6813 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.45983e-06\n",
      "6814 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.77411e-07\n",
      "6815 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 7.38408e-08\n",
      "6816 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 9.85674e-08\n",
      "6817 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.38435e-08\n",
      "6818 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 5.43965e-08\n",
      "6819 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.41926e-07\n",
      "6820 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.7541e-07\n",
      "6821 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 8.08475e-08\n",
      "6822 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.14145e-07\n",
      "6823 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.48972e-07\n",
      "6824 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.11751e-07\n",
      "6825 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.32163e-07\n",
      "6826 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.53711e-07\n",
      "6827 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.13485e-07\n",
      "6828 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 6.66943e-08\n",
      "6829 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 6.81583e-08\n",
      "6830 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.1802e-08\n",
      "6831 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.07092e-07\n",
      "6832 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.07323e-07\n",
      "6833 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.75287e-07\n",
      "6834 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.51615e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6835 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.01884e-07\n",
      "6836 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.75167e-07\n",
      "6837 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.80515e-07\n",
      "6838 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.05449e-07\n",
      "6839 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.31183e-07\n",
      "6840 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.57315e-07\n",
      "6841 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.3593e-07\n",
      "6842 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.4906e-08\n",
      "6843 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.49659e-07\n",
      "6844 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.55839e-07\n",
      "6845 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.21185e-07\n",
      "6846 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.24461e-07\n",
      "6847 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.1753e-07\n",
      "6848 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.91711e-07\n",
      "6849 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.28541e-07\n",
      "6850 Train total loss: 2.98603 \tReconstruction loss: 2.98603 \tLatent loss: 1.14409e-07\n",
      "6851 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.25741e-07\n",
      "6852 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.38174e-07\n",
      "6853 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.44984e-07\n",
      "6854 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.48595e-07\n",
      "6855 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.139e-07\n",
      "6856 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.44046e-07\n",
      "6857 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.44729e-07\n",
      "6858 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.6329e-07\n",
      "6859 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.61965e-07\n",
      "6860 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.09551e-07\n",
      "6861 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.20372e-07\n",
      "6862 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.44282e-07\n",
      "6863 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.32294e-07\n",
      "6864 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.38279e-07\n",
      "6865 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.92698e-07\n",
      "6866 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.92948e-07\n",
      "6867 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.63694e-07\n",
      "6868 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.19186e-07\n",
      "6869 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.30342e-07\n",
      "6870 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.57306e-07\n",
      "6871 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.92169e-07\n",
      "6872 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.69499e-07\n",
      "6873 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.2074e-07\n",
      "6874 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 1.53843e-07\n",
      "6875 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.14147e-07\n",
      "6876 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.35806e-07\n",
      "6877 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.49714e-07\n",
      "6878 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.12933e-07\n",
      "6879 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.85633e-07\n",
      "6880 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.11185e-07\n",
      "6881 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.91643e-07\n",
      "6882 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.5188e-07\n",
      "6883 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 1.78663e-07\n",
      "6884 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.574e-07\n",
      "6885 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.13763e-07\n",
      "6886 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.49923e-07\n",
      "6887 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.06967e-07\n",
      "6888 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.07681e-07\n",
      "6889 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.73938e-07\n",
      "6890 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.11257e-07\n",
      "6891 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.22972e-07\n",
      "6892 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.00319e-07\n",
      "6893 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.40583e-07\n",
      "6894 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.54441e-07\n",
      "6895 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.58414e-07\n",
      "6896 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.13581e-07\n",
      "6897 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.33773e-07\n",
      "6898 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.23309e-07\n",
      "6899 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.02138e-06\n",
      "6900 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.73791e-07\n",
      "6901 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.37585e-07\n",
      "6902 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.4295e-07\n",
      "6903 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.93407e-07\n",
      "6904 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.55925e-07\n",
      "6905 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.67033e-07\n",
      "6906 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.97462e-07\n",
      "6907 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.28202e-07\n",
      "6908 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.38471e-07\n",
      "6909 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.22927e-07\n",
      "6910 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.30062e-06\n",
      "6911 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.19791e-07\n",
      "6912 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.20124e-07\n",
      "6913 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.6651e-07\n",
      "6914 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.30694e-07\n",
      "6915 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.73615e-07\n",
      "6916 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.51967e-07\n",
      "6917 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.2681e-07\n",
      "6918 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.65418e-07\n",
      "6919 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.31537e-07\n",
      "6920 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.24992e-07\n",
      "6921 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.4512e-07\n",
      "6922 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.54483e-07\n",
      "6923 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.3059e-07\n",
      "6924 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.16578e-07\n",
      "6925 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.72037e-07\n",
      "6926 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 1.83748e-07\n",
      "6927 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.59157e-07\n",
      "6928 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.66777e-07\n",
      "6929 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.82456e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6930 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 5.02958e-06\n",
      "6931 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 5.04758e-07\n",
      "6932 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.08946e-07\n",
      "6933 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.5812e-07\n",
      "6934 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 6.29015e-07\n",
      "6935 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.44475e-07\n",
      "6936 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.0959e-07\n",
      "6937 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.03863e-08\n",
      "6938 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.10178e-07\n",
      "6939 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.00005e-07\n",
      "6940 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.25146e-07\n",
      "6941 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.42719e-07\n",
      "6942 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 9.89909e-08\n",
      "6943 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.01833e-07\n",
      "6944 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.00656e-07\n",
      "6945 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.9373e-07\n",
      "6946 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 9.59772e-08\n",
      "6947 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.09433e-07\n",
      "6948 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.78337e-07\n",
      "6949 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.91033e-07\n",
      "6950 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.83197e-07\n",
      "6951 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.55458e-07\n",
      "6952 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.3629e-07\n",
      "6953 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.15927e-07\n",
      "6954 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.46584e-07\n",
      "6955 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.28692e-07\n",
      "6956 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.70409e-07\n",
      "6957 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.08491e-08\n",
      "6958 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.75768e-07\n",
      "6959 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.44816e-07\n",
      "6960 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.06346e-07\n",
      "6961 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 6.83576e-08\n",
      "6962 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.2638e-07\n",
      "6963 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.16781e-07\n",
      "6964 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.87718e-07\n",
      "6965 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.35735e-07\n",
      "6966 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.95258e-07\n",
      "6967 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 6.47255e-08\n",
      "6968 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 8.44142e-08\n",
      "6969 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 7.80216e-08\n",
      "6970 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.16579e-07\n",
      "6971 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.51073e-07\n",
      "6972 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 7.316e-07\n",
      "6973 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.56902e-07\n",
      "6974 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.62447e-07\n",
      "6975 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.22397e-07\n",
      "6976 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.05769e-07\n",
      "6977 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.99454e-07\n",
      "6978 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.62086e-07\n",
      "6979 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.06941e-07\n",
      "6980 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.17361e-07\n",
      "6981 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.07376e-07\n",
      "6982 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.08177e-07\n",
      "6983 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 2.53924e-07\n",
      "6984 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.32725e-07\n",
      "6985 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.26186e-07\n",
      "6986 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 2.44378e-07\n",
      "6987 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.70378e-07\n",
      "6988 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.52034e-07\n",
      "6989 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.11625e-07\n",
      "6990 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.10379e-07\n",
      "6991 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.27367e-07\n",
      "6992 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.65725e-07\n",
      "6993 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.56154e-07\n",
      "6994 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 3.31848e-07\n",
      "6995 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.25883e-07\n",
      "6996 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.73681e-07\n",
      "6997 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.89126e-07\n",
      "6998 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.31787e-07\n",
      "6999 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.43301e-07\n",
      "7000 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.02522e-07\n",
      "7001 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.12769e-07\n",
      "7002 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.84686e-07\n",
      "7003 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.33032e-07\n",
      "7004 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.91371e-07\n",
      "7005 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.54834e-07\n",
      "7006 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.76519e-07\n",
      "7007 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.38234e-07\n",
      "7008 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.27761e-07\n",
      "7009 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.54936e-07\n",
      "7010 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.9205e-07\n",
      "7011 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.17198e-07\n",
      "7012 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.93959e-07\n",
      "7013 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 4.1897e-07\n",
      "7014 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.02058e-07\n",
      "7015 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.66388e-07\n",
      "7016 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.19245e-07\n",
      "7017 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.6497e-07\n",
      "7018 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.12165e-07\n",
      "7019 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.16493e-07\n",
      "7020 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.04972e-07\n",
      "7021 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.82807e-08\n",
      "7022 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.33999e-07\n",
      "7023 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.57728e-07\n",
      "7024 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.75863e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7025 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.58163e-07\n",
      "7026 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.3683e-07\n",
      "7027 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.93063e-07\n",
      "7028 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.47379e-07\n",
      "7029 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.83268e-07\n",
      "7030 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.8033e-07\n",
      "7031 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.56275e-07\n",
      "7032 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.8126e-07\n",
      "7033 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.84225e-06\n",
      "7034 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.19255e-07\n",
      "7035 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 1.53806e-07\n",
      "7036 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.02149e-07\n",
      "7037 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.96957e-07\n",
      "7038 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.37113e-08\n",
      "7039 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.02395e-07\n",
      "7040 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.40639e-07\n",
      "7041 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.45743e-07\n",
      "7042 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 9.42367e-08\n",
      "7043 Train total loss: 2.86597 \tReconstruction loss: 2.86596 \tLatent loss: 1.58523e-07\n",
      "7044 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 1.67667e-07\n",
      "7045 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.48924e-07\n",
      "7046 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.34543e-07\n",
      "7047 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.25029e-07\n",
      "7048 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.79659e-07\n",
      "7049 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 2.56125e-07\n",
      "7050 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.86278e-07\n",
      "7051 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.02345e-07\n",
      "7052 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.02036e-07\n",
      "7053 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 2.04756e-07\n",
      "7054 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.12623e-07\n",
      "7055 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.0633e-07\n",
      "7056 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.66702e-07\n",
      "7057 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.13792e-07\n",
      "7058 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 3.03953e-07\n",
      "7059 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.28064e-07\n",
      "7060 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 2.38481e-07\n",
      "7061 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.443e-07\n",
      "7062 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.52197e-07\n",
      "7063 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.76748e-07\n",
      "7064 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.14514e-07\n",
      "7065 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.94492e-07\n",
      "7066 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.61837e-07\n",
      "7067 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.63566e-07\n",
      "7068 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.6202e-07\n",
      "7069 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.75009e-07\n",
      "7070 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.30473e-07\n",
      "7071 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.28594e-07\n",
      "7072 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.06341e-07\n",
      "7073 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 1.59981e-07\n",
      "7074 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.99106e-07\n",
      "7075 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.06571e-07\n",
      "7076 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.50662e-07\n",
      "7077 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.43736e-07\n",
      "7078 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.75159e-07\n",
      "7079 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.94909e-07\n",
      "7080 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.85617e-07\n",
      "7081 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.65057e-07\n",
      "7082 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 9.90549e-08\n",
      "7083 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.08823e-07\n",
      "7084 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.44981e-07\n",
      "7085 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.33039e-07\n",
      "7086 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.94459e-07\n",
      "7087 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.08378e-07\n",
      "7088 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.27192e-07\n",
      "7089 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.37213e-07\n",
      "7090 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.591e-07\n",
      "7091 Train total loss: 2.94646 \tReconstruction loss: 2.94645 \tLatent loss: 3.52194e-07\n",
      "7092 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.48663e-07\n",
      "7093 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.52815e-07\n",
      "7094 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 1.80338e-07\n",
      "7095 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.18671e-07\n",
      "7096 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.50781e-07\n",
      "7097 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.49661e-07\n",
      "7098 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.83274e-07\n",
      "7099 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.31441e-07\n",
      "7100 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.48742e-07\n",
      "7101 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.7383e-07\n",
      "7102 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.71062e-07\n",
      "7103 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.77037e-07\n",
      "7104 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.31287e-07\n",
      "7105 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.22745e-07\n",
      "7106 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.12697e-07\n",
      "7107 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.15841e-07\n",
      "7108 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.02875e-07\n",
      "7109 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.24011e-07\n",
      "7110 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.93623e-07\n",
      "7111 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.48488e-07\n",
      "7112 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.0977e-07\n",
      "7113 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.70956e-07\n",
      "7114 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 2.66684e-07\n",
      "7115 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 2.28855e-07\n",
      "7116 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.30238e-07\n",
      "7117 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.71422e-07\n",
      "7118 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.79507e-07\n",
      "7119 Train total loss: 2.8052 \tReconstruction loss: 2.80519 \tLatent loss: 4.4074e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7120 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.24313e-07\n",
      "7121 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.73616e-07\n",
      "7122 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.8947e-07\n",
      "7123 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.79664e-07\n",
      "7124 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.09727e-08\n",
      "7125 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 6.4436e-07\n",
      "7126 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.67058e-07\n",
      "7127 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.68818e-07\n",
      "7128 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.22781e-08\n",
      "7129 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.02626e-07\n",
      "7130 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.74739e-07\n",
      "7131 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.38427e-07\n",
      "7132 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.00011e-07\n",
      "7133 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.22331e-07\n",
      "7134 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.51747e-07\n",
      "7135 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.36072e-07\n",
      "7136 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.25929e-07\n",
      "7137 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.59569e-07\n",
      "7138 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.50323e-07\n",
      "7139 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.75934e-07\n",
      "7140 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.65947e-07\n",
      "7141 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.27367e-07\n",
      "7142 Train total loss: 2.83693 \tReconstruction loss: 2.83693 \tLatent loss: 2.42468e-07\n",
      "7143 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.10807e-07\n",
      "7144 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.71054e-07\n",
      "7145 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.42407e-07\n",
      "7146 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.41905e-07\n",
      "7147 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.89949e-07\n",
      "7148 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.56088e-07\n",
      "7149 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.79613e-07\n",
      "7150 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.92435e-07\n",
      "7151 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.53881e-07\n",
      "7152 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.62252e-07\n",
      "7153 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.0137e-07\n",
      "7154 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 1.85092e-07\n",
      "7155 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.20341e-07\n",
      "7156 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.08996e-07\n",
      "7157 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.61794e-07\n",
      "7158 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.63433e-07\n",
      "7159 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.32073e-07\n",
      "7160 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.69008e-07\n",
      "7161 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.85349e-07\n",
      "7162 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 1.92221e-07\n",
      "7163 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.1691e-07\n",
      "7164 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 1.91691e-07\n",
      "7165 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.01179e-07\n",
      "7166 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.22459e-07\n",
      "7167 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.17236e-07\n",
      "7168 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.11875e-07\n",
      "7169 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.05819e-07\n",
      "7170 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.70209e-07\n",
      "7171 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.51174e-07\n",
      "7172 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.11945e-07\n",
      "7173 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.76794e-07\n",
      "7174 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.73861e-07\n",
      "7175 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.73255e-07\n",
      "7176 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.42638e-07\n",
      "7177 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.66568e-07\n",
      "7178 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.01719e-07\n",
      "7179 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.28372e-07\n",
      "7180 Train total loss: 3.0398 \tReconstruction loss: 3.03979 \tLatent loss: 2.23277e-07\n",
      "7181 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.64613e-07\n",
      "7182 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.64454e-07\n",
      "7183 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.59285e-07\n",
      "7184 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.13565e-07\n",
      "7185 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.62444e-07\n",
      "7186 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.7954e-07\n",
      "7187 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.4207e-07\n",
      "7188 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.20851e-07\n",
      "7189 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.47626e-07\n",
      "7190 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.38721e-07\n",
      "7191 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.29877e-07\n",
      "7192 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.31271e-07\n",
      "7193 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.93794e-07\n",
      "7194 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.0132e-07\n",
      "7195 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.35683e-07\n",
      "7196 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.51359e-07\n",
      "7197 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.43156e-07\n",
      "7198 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.97734e-07\n",
      "7199 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.43766e-07\n",
      "7200 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.5236e-07\n",
      "7201 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.81668e-07\n",
      "7202 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.49279e-07\n",
      "7203 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.31381e-07\n",
      "7204 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.94975e-07\n",
      "7205 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.72835e-07\n",
      "7206 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.54518e-07\n",
      "7207 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.71411e-07\n",
      "7208 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.07252e-07\n",
      "7209 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.02003e-07\n",
      "7210 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 9.73491e-07\n",
      "7211 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.30617e-07\n",
      "7212 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 2.05792e-07\n",
      "7213 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.24965e-07\n",
      "7214 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.37326e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7215 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.49963e-07\n",
      "7216 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.50092e-07\n",
      "7217 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.93861e-07\n",
      "7218 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 5.81217e-07\n",
      "7219 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.03751e-07\n",
      "7220 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.7923e-07\n",
      "7221 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.304e-07\n",
      "7222 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.8428e-07\n",
      "7223 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.56842e-07\n",
      "7224 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.5154e-07\n",
      "7225 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.3955e-07\n",
      "7226 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.52801e-07\n",
      "7227 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.29591e-07\n",
      "7228 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.04828e-07\n",
      "7229 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.43897e-07\n",
      "7230 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.45231e-07\n",
      "7231 Train total loss: 2.94647 \tReconstruction loss: 2.94646 \tLatent loss: 1.41551e-07\n",
      "7232 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.26931e-07\n",
      "7233 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.13075e-07\n",
      "7234 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.5204e-07\n",
      "7235 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.00191e-07\n",
      "7236 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.65805e-07\n",
      "7237 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.70702e-07\n",
      "7238 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.99797e-07\n",
      "7239 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.94735e-07\n",
      "7240 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.19875e-07\n",
      "7241 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.02699e-07\n",
      "7242 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.49867e-07\n",
      "7243 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.03335e-07\n",
      "7244 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.55053e-07\n",
      "7245 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.95683e-07\n",
      "7246 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.81042e-07\n",
      "7247 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 8.38467e-08\n",
      "7248 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.1622e-07\n",
      "7249 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.29225e-07\n",
      "7250 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.98817e-07\n",
      "7251 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 4.06546e-07\n",
      "7252 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.68372e-07\n",
      "7253 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.80038e-07\n",
      "7254 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.31587e-07\n",
      "7255 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.66722e-07\n",
      "7256 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.24923e-07\n",
      "7257 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.76375e-07\n",
      "7258 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.87451e-07\n",
      "7259 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.14693e-06\n",
      "7260 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.86153e-07\n",
      "7261 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.37494e-07\n",
      "7262 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.47252e-07\n",
      "7263 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.4487e-07\n",
      "7264 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.75976e-07\n",
      "7265 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.85723e-07\n",
      "7266 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.95541e-07\n",
      "7267 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.05327e-07\n",
      "7268 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.37682e-07\n",
      "7269 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 5.23166e-07\n",
      "7270 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.61092e-07\n",
      "7271 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.64305e-07\n",
      "7272 Train total loss: 2.92841 \tReconstruction loss: 2.9284 \tLatent loss: 1.33456e-07\n",
      "7273 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.62162e-07\n",
      "7274 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.71051e-07\n",
      "7275 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.65272e-07\n",
      "7276 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.87914e-07\n",
      "7277 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.24161e-07\n",
      "7278 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.98573e-07\n",
      "7279 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.39904e-07\n",
      "7280 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.36941e-07\n",
      "7281 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.33466e-07\n",
      "7282 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.83332e-07\n",
      "7283 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.26901e-07\n",
      "7284 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 5.64082e-07\n",
      "7285 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.41664e-07\n",
      "7286 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.33274e-07\n",
      "7287 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.85421e-07\n",
      "7288 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.26344e-07\n",
      "7289 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.31763e-07\n",
      "7290 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.62668e-07\n",
      "7291 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.26712e-07\n",
      "7292 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.06578e-07\n",
      "7293 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.68056e-07\n",
      "7294 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.17411e-08\n",
      "7295 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.55502e-07\n",
      "7296 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.45915e-07\n",
      "7297 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.93925e-07\n",
      "7298 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.45714e-07\n",
      "7299 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.30649e-07\n",
      "7300 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.66784e-07\n",
      "7301 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.54706e-07\n",
      "7302 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.90623e-07\n",
      "7303 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.39129e-07\n",
      "7304 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.33083e-07\n",
      "7305 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.03776e-08\n",
      "7306 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.04699e-07\n",
      "7307 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.34131e-07\n",
      "7308 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.19795e-07\n",
      "7309 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.92858e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7310 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.68578e-07\n",
      "7311 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.75099e-07\n",
      "7312 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.60806e-07\n",
      "7313 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 8.70044e-08\n",
      "7314 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.07591e-07\n",
      "7315 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.31247e-07\n",
      "7316 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.38067e-07\n",
      "7317 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.76243e-07\n",
      "7318 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.85198e-07\n",
      "7319 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 6.43508e-07\n",
      "7320 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.76072e-07\n",
      "7321 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.86685e-07\n",
      "7322 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.08064e-07\n",
      "7323 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.54879e-07\n",
      "7324 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.49319e-07\n",
      "7325 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.22235e-07\n",
      "7326 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.53712e-07\n",
      "7327 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.00039e-07\n",
      "7328 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.00362e-07\n",
      "7329 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.49487e-07\n",
      "7330 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.69712e-07\n",
      "7331 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.31031e-07\n",
      "7332 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.16336e-07\n",
      "7333 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.67696e-07\n",
      "7334 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.00443e-07\n",
      "7335 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.85756e-07\n",
      "7336 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.30443e-07\n",
      "7337 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.62273e-07\n",
      "7338 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.21402e-07\n",
      "7339 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.04787e-07\n",
      "7340 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.35673e-07\n",
      "7341 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.3342e-07\n",
      "7342 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.23028e-07\n",
      "7343 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.79714e-07\n",
      "7344 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.81331e-07\n",
      "7345 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.79849e-07\n",
      "7346 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.87794e-07\n",
      "7347 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.9936e-07\n",
      "7348 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.0789e-07\n",
      "7349 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.16134e-07\n",
      "7350 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.18164e-07\n",
      "7351 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.48678e-07\n",
      "7352 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.51876e-07\n",
      "7353 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.13673e-07\n",
      "7354 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.11463e-07\n",
      "7355 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.96465e-07\n",
      "7356 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.65691e-07\n",
      "7357 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.99588e-07\n",
      "7358 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.54154e-07\n",
      "7359 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.09973e-07\n",
      "7360 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.14866e-07\n",
      "7361 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.31803e-07\n",
      "7362 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.23942e-07\n",
      "7363 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 9.93765e-08\n",
      "7364 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.52203e-07\n",
      "7365 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.57081e-07\n",
      "7366 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.34708e-07\n",
      "7367 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.78075e-07\n",
      "7368 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.62085e-07\n",
      "7369 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.33093e-07\n",
      "7370 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.32253e-07\n",
      "7371 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.74558e-07\n",
      "7372 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.35516e-07\n",
      "7373 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.56481e-07\n",
      "7374 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.75408e-07\n",
      "7375 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.09348e-07\n",
      "7376 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.04281e-07\n",
      "7377 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.78687e-07\n",
      "7378 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.60196e-07\n",
      "7379 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.04478e-07\n",
      "7380 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.08557e-07\n",
      "7381 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.51185e-07\n",
      "7382 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.798e-07\n",
      "7383 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.06858e-07\n",
      "7384 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 9.29009e-08\n",
      "7385 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.26379e-07\n",
      "7386 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.8592e-07\n",
      "7387 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.84418e-07\n",
      "7388 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.53853e-07\n",
      "7389 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.26077e-07\n",
      "7390 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.81053e-07\n",
      "7391 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.14042e-07\n",
      "7392 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.42467e-07\n",
      "7393 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.81444e-07\n",
      "7394 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.4561e-07\n",
      "7395 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.91404e-07\n",
      "7396 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.98442e-07\n",
      "7397 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.05261e-07\n",
      "7398 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.56953e-07\n",
      "7399 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.50882e-07\n",
      "7400 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.04628e-07\n",
      "7401 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.78124e-07\n",
      "7402 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.96904e-07\n",
      "7403 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.47743e-07\n",
      "7404 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.08151e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7405 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.96585e-07\n",
      "7406 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.8422e-07\n",
      "7407 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.60893e-07\n",
      "7408 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.10582e-07\n",
      "7409 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.34914e-07\n",
      "7410 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.3843e-06\n",
      "7411 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.72209e-07\n",
      "7412 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.57828e-07\n",
      "7413 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.56947e-07\n",
      "7414 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.3328e-07\n",
      "7415 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.84898e-07\n",
      "7416 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.1493e-07\n",
      "7417 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.2716e-07\n",
      "7418 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.61554e-07\n",
      "7419 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.47519e-07\n",
      "7420 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.65607e-08\n",
      "7421 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.04777e-07\n",
      "7422 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.11916e-07\n",
      "7423 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.1591e-07\n",
      "7424 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.22801e-07\n",
      "7425 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.07664e-07\n",
      "7426 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 9.15185e-08\n",
      "7427 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 7.63132e-08\n",
      "7428 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 8.14574e-07\n",
      "7429 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.79958e-07\n",
      "7430 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.48216e-07\n",
      "7431 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.72e-07\n",
      "7432 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.54439e-07\n",
      "7433 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 1.70222e-07\n",
      "7434 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.13444e-07\n",
      "7435 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.05888e-07\n",
      "7436 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.43886e-07\n",
      "7437 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.49224e-07\n",
      "7438 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.639e-07\n",
      "7439 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.35474e-07\n",
      "7440 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.27617e-07\n",
      "7441 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.15845e-07\n",
      "7442 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.62179e-07\n",
      "7443 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.42426e-07\n",
      "7444 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.7075e-07\n",
      "7445 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.30796e-07\n",
      "7446 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.41426e-07\n",
      "7447 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.80611e-07\n",
      "7448 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.44766e-07\n",
      "7449 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 5.78759e-07\n",
      "7450 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.86266e-07\n",
      "7451 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.74806e-07\n",
      "7452 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.87766e-07\n",
      "7453 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.73451e-07\n",
      "7454 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.01111e-07\n",
      "7455 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.26115e-07\n",
      "7456 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.67669e-07\n",
      "7457 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.35316e-07\n",
      "7458 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.31375e-07\n",
      "7459 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.78373e-07\n",
      "7460 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.29537e-07\n",
      "7461 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.31302e-07\n",
      "7462 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.18246e-07\n",
      "7463 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.00365e-07\n",
      "7464 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.00041e-07\n",
      "7465 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.91643e-07\n",
      "7466 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.44826e-07\n",
      "7467 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.47035e-07\n",
      "7468 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 8.98197e-07\n",
      "7469 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.18021e-07\n",
      "7470 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.60505e-07\n",
      "7471 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.71972e-07\n",
      "7472 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.04462e-07\n",
      "7473 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.85963e-08\n",
      "7474 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.18067e-07\n",
      "7475 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.05621e-07\n",
      "7476 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.01188e-07\n",
      "7477 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.61973e-07\n",
      "7478 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.57275e-06\n",
      "7479 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 8.19461e-07\n",
      "7480 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.29226e-07\n",
      "7481 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.06121e-07\n",
      "7482 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.2143e-07\n",
      "7483 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.2854e-07\n",
      "7484 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.30919e-07\n",
      "7485 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.68316e-07\n",
      "7486 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 7.57325e-08\n",
      "7487 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 4.9884e-08\n",
      "7488 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 6.63786e-08\n",
      "7489 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.29837e-07\n",
      "7490 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.64649e-07\n",
      "7491 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.87418e-07\n",
      "7492 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 8.85326e-07\n",
      "7493 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.24241e-07\n",
      "7494 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.7997e-08\n",
      "7495 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 6.45668e-08\n",
      "7496 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.67571e-08\n",
      "7497 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.33473e-08\n",
      "7498 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.66986e-07\n",
      "7499 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.26509e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.12546e-07\n",
      "7501 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.05268e-06\n",
      "7502 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.37136e-07\n",
      "7503 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.17114e-07\n",
      "7504 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 9.46151e-08\n",
      "7505 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.42076e-08\n",
      "7506 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 7.4046e-08\n",
      "7507 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 5.7889e-08\n",
      "7508 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.66244e-08\n",
      "7509 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.23914e-08\n",
      "7510 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.7144e-08\n",
      "7511 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 8.17658e-08\n",
      "7512 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.08668e-07\n",
      "7513 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.15216e-07\n",
      "7514 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.19573e-07\n",
      "7515 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.32647e-08\n",
      "7516 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.60724e-07\n",
      "7517 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.79536e-07\n",
      "7518 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.32254e-07\n",
      "7519 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.9795e-07\n",
      "7520 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.49064e-07\n",
      "7521 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 8.77306e-08\n",
      "7522 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.50131e-07\n",
      "7523 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.15346e-07\n",
      "7524 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.34972e-07\n",
      "7525 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 9.82865e-08\n",
      "7526 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.90013e-07\n",
      "7527 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 4.46696e-07\n",
      "7528 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 9.81949e-08\n",
      "7529 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.27868e-07\n",
      "7530 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 9.17731e-08\n",
      "7531 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.08135e-07\n",
      "7532 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.28007e-07\n",
      "7533 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.23004e-07\n",
      "7534 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.03399e-07\n",
      "7535 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.12192e-07\n",
      "7536 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.44858e-07\n",
      "7537 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.28727e-07\n",
      "7538 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 9.13351e-08\n",
      "7539 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.393e-07\n",
      "7540 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.75498e-07\n",
      "7541 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.38529e-07\n",
      "7542 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.797e-07\n",
      "7543 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 9.613e-08\n",
      "7544 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.70987e-07\n",
      "7545 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.93166e-07\n",
      "7546 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.30159e-07\n",
      "7547 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.32794e-07\n",
      "7548 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.01384e-07\n",
      "7549 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.94567e-07\n",
      "7550 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.84163e-07\n",
      "7551 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.96922e-07\n",
      "7552 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.89111e-07\n",
      "7553 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 1.89745e-07\n",
      "7554 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.06815e-07\n",
      "7555 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.03767e-07\n",
      "7556 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.4284e-07\n",
      "7557 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.40185e-08\n",
      "7558 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.46149e-07\n",
      "7559 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.90685e-07\n",
      "7560 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 1.21671e-07\n",
      "7561 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.44924e-07\n",
      "7562 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.24123e-07\n",
      "7563 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.70028e-07\n",
      "7564 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.44888e-07\n",
      "7565 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.5386e-07\n",
      "7566 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 9.25007e-08\n",
      "7567 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.53847e-07\n",
      "7568 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.51744e-07\n",
      "7569 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.50048e-07\n",
      "7570 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.06873e-07\n",
      "7571 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.04684e-07\n",
      "7572 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.57084e-07\n",
      "7573 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 1.1845e-07\n",
      "7574 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.01015e-07\n",
      "7575 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.28243e-07\n",
      "7576 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.52769e-07\n",
      "7577 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.73387e-07\n",
      "7578 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 3.20237e-07\n",
      "7579 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 1.62215e-07\n",
      "7580 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.20451e-07\n",
      "7581 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.94288e-07\n",
      "7582 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.15913e-07\n",
      "7583 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.39177e-07\n",
      "7584 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.92539e-07\n",
      "7585 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.6676e-07\n",
      "7586 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.91687e-07\n",
      "7587 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.1442e-07\n",
      "7588 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.86262e-07\n",
      "7589 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.18851e-07\n",
      "7590 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.04424e-07\n",
      "7591 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.16151e-07\n",
      "7592 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.70046e-07\n",
      "7593 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.56964e-07\n",
      "7594 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.05248e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7595 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.89721e-07\n",
      "7596 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.78943e-07\n",
      "7597 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.8334e-07\n",
      "7598 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 6.13625e-08\n",
      "7599 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.33212e-07\n",
      "7600 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.79971e-07\n",
      "7601 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.64479e-07\n",
      "7602 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.65354e-07\n",
      "7603 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 9.39835e-08\n",
      "7604 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.92495e-07\n",
      "7605 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.62388e-07\n",
      "7606 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.23362e-07\n",
      "7607 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.01253e-07\n",
      "7608 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.5047e-07\n",
      "7609 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.7368e-07\n",
      "7610 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.62808e-07\n",
      "7611 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.82068e-07\n",
      "7612 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.66842e-07\n",
      "7613 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.80541e-07\n",
      "7614 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.38111e-07\n",
      "7615 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.568e-07\n",
      "7616 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.50596e-07\n",
      "7617 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.67565e-07\n",
      "7618 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.19967e-07\n",
      "7619 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.35386e-07\n",
      "7620 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.5952e-07\n",
      "7621 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.03102e-07\n",
      "7622 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.43574e-07\n",
      "7623 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.00298e-07\n",
      "7624 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.75061e-07\n",
      "7625 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.73333e-07\n",
      "7626 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.25839e-07\n",
      "7627 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.09771e-07\n",
      "7628 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.43598e-07\n",
      "7629 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.23471e-07\n",
      "7630 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.75358e-07\n",
      "7631 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.20085e-07\n",
      "7632 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.03269e-07\n",
      "7633 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.40326e-07\n",
      "7634 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.30505e-07\n",
      "7635 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.56727e-07\n",
      "7636 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.03405e-07\n",
      "7637 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.82752e-07\n",
      "7638 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 2.26437e-07\n",
      "7639 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.11322e-07\n",
      "7640 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.76436e-07\n",
      "7641 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.15367e-07\n",
      "7642 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.06442e-07\n",
      "7643 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.05492e-07\n",
      "7644 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.13889e-07\n",
      "7645 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.51063e-07\n",
      "7646 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.26482e-06\n",
      "7647 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.86889e-07\n",
      "7648 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.4745e-07\n",
      "7649 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.2747e-07\n",
      "7650 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.40611e-07\n",
      "7651 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.53727e-07\n",
      "7652 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.74865e-07\n",
      "7653 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.03257e-07\n",
      "7654 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.26379e-07\n",
      "7655 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.14461e-07\n",
      "7656 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.58283e-07\n",
      "7657 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.89554e-07\n",
      "7658 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.31128e-07\n",
      "7659 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 7.93734e-08\n",
      "7660 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.16802e-07\n",
      "7661 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.84195e-07\n",
      "7662 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.72346e-07\n",
      "7663 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.86128e-07\n",
      "7664 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.44528e-07\n",
      "7665 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.91286e-07\n",
      "7666 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.79702e-07\n",
      "7667 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.05629e-07\n",
      "7668 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.92953e-07\n",
      "7669 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 2.90333e-07\n",
      "7670 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.69782e-07\n",
      "7671 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.38282e-07\n",
      "7672 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.69211e-07\n",
      "7673 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.54069e-07\n",
      "7674 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.32799e-07\n",
      "7675 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.61411e-07\n",
      "7676 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.81067e-07\n",
      "7677 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.05087e-07\n",
      "7678 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.55529e-07\n",
      "7679 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.59449e-07\n",
      "7680 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.36453e-07\n",
      "7681 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.48498e-07\n",
      "7682 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.85405e-07\n",
      "7683 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.00777e-07\n",
      "7684 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.05575e-07\n",
      "7685 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.67833e-07\n",
      "7686 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.3277e-07\n",
      "7687 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.3924e-07\n",
      "7688 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 3.85915e-07\n",
      "7689 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.93129e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7690 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.74177e-07\n",
      "7691 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.84937e-07\n",
      "7692 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.92986e-07\n",
      "7693 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.1731e-07\n",
      "7694 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.76776e-07\n",
      "7695 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.49859e-07\n",
      "7696 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.21369e-07\n",
      "7697 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.32753e-07\n",
      "7698 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.25273e-07\n",
      "7699 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.39522e-07\n",
      "7700 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.84558e-07\n",
      "7701 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.29025e-07\n",
      "7702 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.13004e-07\n",
      "7703 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.4999e-07\n",
      "7704 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.51883e-07\n",
      "7705 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 8.32486e-08\n",
      "7706 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 7.55143e-08\n",
      "7707 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.29151e-07\n",
      "7708 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.78615e-07\n",
      "7709 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.06222e-07\n",
      "7710 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.26938e-07\n",
      "7711 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.03943e-07\n",
      "7712 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.1806e-07\n",
      "7713 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.387e-07\n",
      "7714 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.5736e-07\n",
      "7715 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.11239e-07\n",
      "7716 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.18958e-07\n",
      "7717 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.64517e-07\n",
      "7718 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.13121e-07\n",
      "7719 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.86863e-07\n",
      "7720 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.50087e-07\n",
      "7721 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.64663e-07\n",
      "7722 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.26808e-07\n",
      "7723 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.55144e-07\n",
      "7724 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.37997e-07\n",
      "7725 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.00765e-07\n",
      "7726 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.68415e-07\n",
      "7727 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.95919e-08\n",
      "7728 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.48211e-07\n",
      "7729 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.63948e-07\n",
      "7730 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.58857e-07\n",
      "7731 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.07214e-07\n",
      "7732 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.29032e-07\n",
      "7733 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.30016e-07\n",
      "7734 Train total loss: 3.02181 \tReconstruction loss: 3.0218 \tLatent loss: 4.03436e-07\n",
      "7735 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.50125e-07\n",
      "7736 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.05915e-07\n",
      "7737 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.63958e-07\n",
      "7738 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.66752e-07\n",
      "7739 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.30948e-07\n",
      "7740 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.80751e-07\n",
      "7741 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.2415e-07\n",
      "7742 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.7375e-07\n",
      "7743 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.355e-07\n",
      "7744 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.77517e-07\n",
      "7745 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.89038e-07\n",
      "7746 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 5.15683e-07\n",
      "7747 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.8868e-07\n",
      "7748 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.84639e-07\n",
      "7749 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.0893e-07\n",
      "7750 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.8248e-07\n",
      "7751 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.99099e-06\n",
      "7752 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.43365e-07\n",
      "7753 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.16961e-07\n",
      "7754 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.21849e-07\n",
      "7755 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.63685e-08\n",
      "7756 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 7.68909e-08\n",
      "7757 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.02713e-07\n",
      "7758 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.04433e-07\n",
      "7759 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 8.2051e-08\n",
      "7760 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.44081e-08\n",
      "7761 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.17667e-07\n",
      "7762 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.05429e-07\n",
      "7763 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.1201e-07\n",
      "7764 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.00005e-07\n",
      "7765 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.08127e-07\n",
      "7766 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.4516e-07\n",
      "7767 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.11218e-07\n",
      "7768 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.75066e-08\n",
      "7769 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.34586e-07\n",
      "7770 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.86044e-07\n",
      "7771 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.23031e-07\n",
      "7772 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.08862e-07\n",
      "7773 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.73508e-07\n",
      "7774 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.69723e-07\n",
      "7775 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.40752e-07\n",
      "7776 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.54808e-07\n",
      "7777 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.66772e-07\n",
      "7778 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.27501e-07\n",
      "7779 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 2.59186e-07\n",
      "7780 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.82632e-07\n",
      "7781 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.20684e-07\n",
      "7782 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.0906e-07\n",
      "7783 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.41389e-07\n",
      "7784 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.07668e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7785 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.4846e-07\n",
      "7786 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.36224e-07\n",
      "7787 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.29566e-07\n",
      "7788 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.56948e-07\n",
      "7789 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.11314e-07\n",
      "7790 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.27409e-07\n",
      "7791 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.10611e-07\n",
      "7792 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.31655e-07\n",
      "7793 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 1.72618e-07\n",
      "7794 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.82854e-07\n",
      "7795 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.02165e-07\n",
      "7796 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.31127e-07\n",
      "7797 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.37412e-07\n",
      "7798 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 1.62283e-07\n",
      "7799 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.1283e-07\n",
      "7800 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.21309e-07\n",
      "7801 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.95243e-07\n",
      "7802 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.27111e-07\n",
      "7803 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.6864e-07\n",
      "7804 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.9604e-07\n",
      "7805 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.63481e-07\n",
      "7806 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.43603e-07\n",
      "7807 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.03534e-07\n",
      "7808 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.31754e-07\n",
      "7809 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 6.75677e-07\n",
      "7810 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.50435e-07\n",
      "7811 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.50561e-07\n",
      "7812 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.7478e-07\n",
      "7813 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.26386e-07\n",
      "7814 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.13025e-07\n",
      "7815 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 2.21404e-07\n",
      "7816 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.00202e-07\n",
      "7817 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.66406e-07\n",
      "7818 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.30839e-07\n",
      "7819 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.4079e-07\n",
      "7820 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.28021e-07\n",
      "7821 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.72793e-07\n",
      "7822 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.99789e-07\n",
      "7823 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.02967e-07\n",
      "7824 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.0552e-07\n",
      "7825 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.67192e-07\n",
      "7826 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.3609e-07\n",
      "7827 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.20576e-07\n",
      "7828 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.05697e-07\n",
      "7829 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.70006e-07\n",
      "7830 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.55406e-07\n",
      "7831 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.52164e-07\n",
      "7832 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.75976e-07\n",
      "7833 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.74691e-07\n",
      "7834 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.66785e-07\n",
      "7835 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.03904e-07\n",
      "7836 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 9.04416e-08\n",
      "7837 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.01917e-07\n",
      "7838 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.51884e-08\n",
      "7839 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 1.58841e-07\n",
      "7840 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.69214e-07\n",
      "7841 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.81596e-07\n",
      "7842 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.26333e-07\n",
      "7843 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.84265e-07\n",
      "7844 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.21629e-07\n",
      "7845 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.75649e-07\n",
      "7846 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.36378e-07\n",
      "7847 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.14088e-07\n",
      "7848 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.04359e-07\n",
      "7849 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.28966e-07\n",
      "7850 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.11278e-07\n",
      "7851 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.03531e-07\n",
      "7852 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.98992e-07\n",
      "7853 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.28182e-07\n",
      "7854 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.44591e-07\n",
      "7855 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 3.38831e-07\n",
      "7856 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.13492e-07\n",
      "7857 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.32949e-07\n",
      "7858 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 3.00452e-07\n",
      "7859 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.45775e-07\n",
      "7860 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.48577e-07\n",
      "7861 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.47616e-07\n",
      "7862 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.0751e-07\n",
      "7863 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.08634e-07\n",
      "7864 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.35083e-07\n",
      "7865 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.47283e-07\n",
      "7866 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.4992e-07\n",
      "7867 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.28434e-07\n",
      "7868 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.66222e-07\n",
      "7869 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.68644e-07\n",
      "7870 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.07967e-07\n",
      "7871 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.07684e-07\n",
      "7872 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.47976e-07\n",
      "7873 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.97051e-07\n",
      "7874 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.80692e-07\n",
      "7875 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.75022e-08\n",
      "7876 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.3563e-07\n",
      "7877 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.61396e-07\n",
      "7878 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.0096e-07\n",
      "7879 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.14802e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7880 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.53699e-07\n",
      "7881 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.49859e-07\n",
      "7882 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.9901e-07\n",
      "7883 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.40334e-07\n",
      "7884 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.44484e-07\n",
      "7885 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.34954e-07\n",
      "7886 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.63411e-07\n",
      "7887 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.49805e-07\n",
      "7888 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.49313e-07\n",
      "7889 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.92139e-07\n",
      "7890 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.0871e-06\n",
      "7891 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.48249e-07\n",
      "7892 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 2.38944e-07\n",
      "7893 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.90731e-07\n",
      "7894 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.19153e-07\n",
      "7895 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.30092e-07\n",
      "7896 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.29896e-08\n",
      "7897 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.05635e-07\n",
      "7898 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 3.57232e-07\n",
      "7899 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.0978e-07\n",
      "7900 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.47411e-07\n",
      "7901 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.70336e-08\n",
      "7902 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.36036e-07\n",
      "7903 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.45772e-07\n",
      "7904 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.08522e-07\n",
      "7905 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.23646e-07\n",
      "7906 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.183e-07\n",
      "7907 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.66029e-07\n",
      "7908 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.53573e-07\n",
      "7909 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 9.20758e-08\n",
      "7910 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.85138e-07\n",
      "7911 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.68636e-07\n",
      "7912 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.53363e-07\n",
      "7913 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.12636e-08\n",
      "7914 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.248e-07\n",
      "7915 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.44542e-07\n",
      "7916 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.65057e-07\n",
      "7917 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.44527e-07\n",
      "7918 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.11385e-07\n",
      "7919 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.859e-07\n",
      "7920 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.8683e-07\n",
      "7921 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.40627e-07\n",
      "7922 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.29416e-07\n",
      "7923 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.66241e-07\n",
      "7924 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.31971e-07\n",
      "7925 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.13776e-07\n",
      "7926 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.48366e-07\n",
      "7927 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.23363e-07\n",
      "7928 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 1.59489e-07\n",
      "7929 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.33731e-07\n",
      "7930 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.70889e-07\n",
      "7931 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.70279e-07\n",
      "7932 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 8.35664e-07\n",
      "7933 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.80626e-07\n",
      "7934 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.58833e-07\n",
      "7935 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 6.46655e-07\n",
      "7936 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.53315e-07\n",
      "7937 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.15801e-07\n",
      "7938 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.34496e-07\n",
      "7939 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.90674e-07\n",
      "7940 Train total loss: 3.03982 \tReconstruction loss: 3.03982 \tLatent loss: 1.65109e-07\n",
      "7941 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.0425e-07\n",
      "7942 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.83605e-08\n",
      "7943 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.84331e-08\n",
      "7944 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.19241e-07\n",
      "7945 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.58066e-07\n",
      "7946 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.50193e-07\n",
      "7947 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 1.57971e-07\n",
      "7948 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.07738e-07\n",
      "7949 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.13111e-07\n",
      "7950 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.88699e-07\n",
      "7951 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 8.31325e-07\n",
      "7952 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.52866e-07\n",
      "7953 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 1.3219e-07\n",
      "7954 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.82707e-07\n",
      "7955 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.68062e-07\n",
      "7956 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.46392e-07\n",
      "7957 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.84057e-07\n",
      "7958 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.49634e-07\n",
      "7959 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 2.74967e-07\n",
      "7960 Train total loss: 3.0398 \tReconstruction loss: 3.03979 \tLatent loss: 1.45115e-07\n",
      "7961 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.41604e-07\n",
      "7962 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.14802e-07\n",
      "7963 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.87813e-07\n",
      "7964 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.17109e-07\n",
      "7965 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.05211e-07\n",
      "7966 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.74645e-07\n",
      "7967 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 6.49503e-07\n",
      "7968 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.02536e-06\n",
      "7969 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.877e-06\n",
      "7970 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.74206e-07\n",
      "7971 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.37955e-07\n",
      "7972 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 8.2214e-08\n",
      "7973 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.15205e-07\n",
      "7974 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 9.2329e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7975 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.1981e-07\n",
      "7976 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.91072e-08\n",
      "7977 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.35295e-08\n",
      "7978 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.0414e-07\n",
      "7979 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.33237e-07\n",
      "7980 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.87212e-07\n",
      "7981 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.03955e-07\n",
      "7982 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.18154e-07\n",
      "7983 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.25186e-07\n",
      "7984 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.41669e-07\n",
      "7985 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.06309e-07\n",
      "7986 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.93171e-07\n",
      "7987 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 6.90605e-08\n",
      "7988 Train total loss: 2.85423 \tReconstruction loss: 2.85421 \tLatent loss: 2.0677e-05\n",
      "7989 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.59441e-06\n",
      "7990 Train total loss: 2.98598 \tReconstruction loss: 2.98598 \tLatent loss: 1.96476e-07\n",
      "7991 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 7.09319e-08\n",
      "7992 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 6.97473e-08\n",
      "7993 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 6.28541e-08\n",
      "7994 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.50062e-08\n",
      "7995 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.24698e-08\n",
      "7996 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.87169e-08\n",
      "7997 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 7.69913e-08\n",
      "7998 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 6.41943e-08\n",
      "7999 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.26161e-07\n",
      "8000 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.49936e-07\n",
      "8001 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.69836e-08\n",
      "8002 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.21062e-08\n",
      "8003 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 5.06654e-08\n",
      "8004 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 6.32484e-08\n",
      "8005 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 7.41959e-08\n",
      "8006 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 5.36063e-08\n",
      "8007 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 7.97649e-08\n",
      "8008 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 5.67059e-08\n",
      "8009 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.40186e-07\n",
      "8010 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.67242e-07\n",
      "8011 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 9.3678e-08\n",
      "8012 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.10126e-07\n",
      "8013 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.44818e-07\n",
      "8014 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.03831e-07\n",
      "8015 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.76783e-08\n",
      "8016 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.11017e-07\n",
      "8017 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.67436e-07\n",
      "8018 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 1.71107e-07\n",
      "8019 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.56837e-07\n",
      "8020 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.12489e-07\n",
      "8021 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.27022e-07\n",
      "8022 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.90525e-07\n",
      "8023 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.32046e-07\n",
      "8024 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 7.34172e-07\n",
      "8025 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.88049e-07\n",
      "8026 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 9.86052e-08\n",
      "8027 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.20394e-07\n",
      "8028 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.17035e-07\n",
      "8029 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 9.74047e-08\n",
      "8030 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.16667e-07\n",
      "8031 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.18787e-07\n",
      "8032 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.42514e-07\n",
      "8033 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.3653e-07\n",
      "8034 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 8.65257e-08\n",
      "8035 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.31025e-07\n",
      "8036 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.33741e-07\n",
      "8037 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.60211e-07\n",
      "8038 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.86216e-07\n",
      "8039 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 2.36642e-07\n",
      "8040 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.28109e-06\n",
      "8041 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.41247e-07\n",
      "8042 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.61544e-07\n",
      "8043 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.45189e-07\n",
      "8044 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.35177e-07\n",
      "8045 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.02603e-07\n",
      "8046 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.31386e-07\n",
      "8047 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 5.38246e-08\n",
      "8048 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.10437e-07\n",
      "8049 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 2.53046e-07\n",
      "8050 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.94774e-07\n",
      "8051 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.69776e-07\n",
      "8052 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.74114e-07\n",
      "8053 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.11682e-07\n",
      "8054 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.94888e-07\n",
      "8055 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.60109e-07\n",
      "8056 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.01029e-07\n",
      "8057 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.35085e-07\n",
      "8058 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.79804e-07\n",
      "8059 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.00056e-07\n",
      "8060 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.16284e-07\n",
      "8061 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.77632e-07\n",
      "8062 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.94579e-07\n",
      "8063 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.51464e-07\n",
      "8064 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.97758e-07\n",
      "8065 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.64028e-07\n",
      "8066 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.73253e-07\n",
      "8067 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.30363e-07\n",
      "8068 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.97483e-07\n",
      "8069 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.84368e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8070 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.03117e-07\n",
      "8071 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.97929e-07\n",
      "8072 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.70821e-07\n",
      "8073 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 4.79826e-07\n",
      "8074 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.37374e-07\n",
      "8075 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.65556e-07\n",
      "8076 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 1.35982e-07\n",
      "8077 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 1.62658e-07\n",
      "8078 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.35136e-07\n",
      "8079 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.03423e-07\n",
      "8080 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.25399e-07\n",
      "8081 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.98526e-07\n",
      "8082 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.71571e-07\n",
      "8083 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.64201e-07\n",
      "8084 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.07764e-07\n",
      "8085 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 4.55414e-07\n",
      "8086 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.18915e-07\n",
      "8087 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.07737e-07\n",
      "8088 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.21663e-07\n",
      "8089 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.56507e-07\n",
      "8090 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.39243e-07\n",
      "8091 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.6796e-07\n",
      "8092 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.46707e-07\n",
      "8093 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 3.56926e-07\n",
      "8094 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.86837e-07\n",
      "8095 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.04893e-07\n",
      "8096 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.47142e-07\n",
      "8097 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.10519e-07\n",
      "8098 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.59765e-07\n",
      "8099 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 5.15451e-07\n",
      "8100 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 6.04409e-07\n",
      "8101 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.84073e-08\n",
      "8102 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.87708e-07\n",
      "8103 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.33488e-07\n",
      "8104 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.68778e-07\n",
      "8105 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 2.05312e-07\n",
      "8106 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.93443e-07\n",
      "8107 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.80602e-07\n",
      "8108 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.14457e-07\n",
      "8109 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.01306e-07\n",
      "8110 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 8.07663e-07\n",
      "8111 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.1593e-07\n",
      "8112 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.23672e-07\n",
      "8113 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.64733e-07\n",
      "8114 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.50916e-07\n",
      "8115 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.48179e-07\n",
      "8116 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.27774e-07\n",
      "8117 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.30161e-07\n",
      "8118 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 1.80153e-07\n",
      "8119 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.18645e-07\n",
      "8120 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.12276e-07\n",
      "8121 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.67775e-07\n",
      "8122 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.83747e-07\n",
      "8123 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.82596e-07\n",
      "8124 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.96202e-07\n",
      "8125 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.65255e-07\n",
      "8126 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 5.27171e-07\n",
      "8127 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.16351e-07\n",
      "8128 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.57733e-07\n",
      "8129 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.2113e-07\n",
      "8130 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 7.17814e-07\n",
      "8131 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.22195e-07\n",
      "8132 Train total loss: 2.92838 \tReconstruction loss: 2.92838 \tLatent loss: 2.89353e-07\n",
      "8133 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.74251e-07\n",
      "8134 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.72372e-07\n",
      "8135 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.48782e-08\n",
      "8136 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.33077e-07\n",
      "8137 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.3763e-07\n",
      "8138 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.298e-07\n",
      "8139 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.68366e-07\n",
      "8140 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 1.29989e-07\n",
      "8141 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.24254e-07\n",
      "8142 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.86142e-07\n",
      "8143 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.89179e-07\n",
      "8144 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.33374e-07\n",
      "8145 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.46298e-07\n",
      "8146 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.52482e-07\n",
      "8147 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 4.56996e-07\n",
      "8148 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 1.40522e-07\n",
      "8149 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.38008e-07\n",
      "8150 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.91637e-07\n",
      "8151 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.07941e-07\n",
      "8152 Train total loss: 2.92842 \tReconstruction loss: 2.92841 \tLatent loss: 4.59619e-07\n",
      "8153 Train total loss: 2.97586 \tReconstruction loss: 2.97586 \tLatent loss: 7.49045e-07\n",
      "8154 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 7.85527e-07\n",
      "8155 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.46571e-07\n",
      "8156 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.49827e-07\n",
      "8157 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 2.86919e-07\n",
      "8158 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.75197e-07\n",
      "8159 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 9.78209e-07\n",
      "8160 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 3.24007e-07\n",
      "8161 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.32778e-07\n",
      "8162 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.57858e-07\n",
      "8163 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.08505e-07\n",
      "8164 Train total loss: 3.09079 \tReconstruction loss: 3.09079 \tLatent loss: 2.53193e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8165 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.22177e-07\n",
      "8166 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.40314e-07\n",
      "8167 Train total loss: 2.92516 \tReconstruction loss: 2.92516 \tLatent loss: 7.68897e-07\n",
      "8168 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 6.66357e-07\n",
      "8169 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.43902e-07\n",
      "8170 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 6.7417e-07\n",
      "8171 Train total loss: 2.94648 \tReconstruction loss: 2.94648 \tLatent loss: 6.57824e-07\n",
      "8172 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.53459e-07\n",
      "8173 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 1.06698e-05\n",
      "8174 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 5.83046e-07\n",
      "8175 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.36807e-07\n",
      "8176 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.71055e-07\n",
      "8177 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 3.34891e-07\n",
      "8178 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.74849e-07\n",
      "8179 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.9205e-07\n",
      "8180 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 2.45459e-07\n",
      "8181 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.3476e-07\n",
      "8182 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.70029e-08\n",
      "8183 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 7.10206e-08\n",
      "8184 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.41197e-07\n",
      "8185 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.6281e-07\n",
      "8186 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 7.00849e-08\n",
      "8187 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.62275e-08\n",
      "8188 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.40709e-08\n",
      "8189 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 8.81555e-08\n",
      "8190 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.60778e-07\n",
      "8191 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.00685e-07\n",
      "8192 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.71081e-07\n",
      "8193 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.43707e-07\n",
      "8194 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.11743e-07\n",
      "8195 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 6.4254e-08\n",
      "8196 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.90272e-08\n",
      "8197 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.79718e-07\n",
      "8198 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 6.09696e-08\n",
      "8199 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 9.76608e-08\n",
      "8200 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 2.2013e-07\n",
      "8201 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.24279e-07\n",
      "8202 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.65169e-07\n",
      "8203 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.42655e-08\n",
      "8204 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 7.27625e-08\n",
      "8205 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 5.8666e-08\n",
      "8206 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 6.34126e-07\n",
      "8207 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.32463e-07\n",
      "8208 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 6.41143e-08\n",
      "8209 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 9.94813e-08\n",
      "8210 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.76328e-07\n",
      "8211 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.46055e-07\n",
      "8212 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 5.65924e-08\n",
      "8213 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.16945e-08\n",
      "8214 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.55682e-08\n",
      "8215 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.90284e-07\n",
      "8216 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.08854e-08\n",
      "8217 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.29663e-07\n",
      "8218 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 1.7728e-07\n",
      "8219 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.20064e-07\n",
      "8220 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.74371e-07\n",
      "8221 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.01785e-07\n",
      "8222 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.33934e-07\n",
      "8223 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 6.22545e-08\n",
      "8224 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 6.2109e-08\n",
      "8225 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.06524e-07\n",
      "8226 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.64149e-07\n",
      "8227 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.60759e-07\n",
      "8228 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.91303e-08\n",
      "8229 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.19185e-08\n",
      "8230 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 6.8167e-08\n",
      "8231 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.43674e-07\n",
      "8232 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.37245e-07\n",
      "8233 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.43913e-07\n",
      "8234 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 8.67774e-08\n",
      "8235 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.27303e-08\n",
      "8236 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 7.27989e-08\n",
      "8237 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.32355e-07\n",
      "8238 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.16193e-07\n",
      "8239 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.09483e-07\n",
      "8240 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.2025e-07\n",
      "8241 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 7.83242e-08\n",
      "8242 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.81436e-08\n",
      "8243 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.18327e-07\n",
      "8244 Train total loss: 3.09078 \tReconstruction loss: 3.09077 \tLatent loss: 2.05803e-07\n",
      "8245 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.04217e-07\n",
      "8246 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.88936e-07\n",
      "8247 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.0908e-07\n",
      "8248 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.01295e-07\n",
      "8249 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.65292e-07\n",
      "8250 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.18793e-07\n",
      "8251 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.26094e-07\n",
      "8252 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.81026e-07\n",
      "8253 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.75576e-07\n",
      "8254 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.37642e-07\n",
      "8255 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.23225e-07\n",
      "8256 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.23261e-07\n",
      "8257 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.84467e-07\n",
      "8258 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.96509e-07\n",
      "8259 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.1159e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8260 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.02554e-07\n",
      "8261 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 5.63219e-07\n",
      "8262 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.7641e-07\n",
      "8263 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.06281e-07\n",
      "8264 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.49611e-07\n",
      "8265 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.16265e-07\n",
      "8266 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.9567e-07\n",
      "8267 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.17796e-07\n",
      "8268 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.65497e-07\n",
      "8269 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.87776e-07\n",
      "8270 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.9486e-07\n",
      "8271 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 9.90185e-08\n",
      "8272 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.27174e-07\n",
      "8273 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.89923e-07\n",
      "8274 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.67271e-07\n",
      "8275 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.98151e-07\n",
      "8276 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.62443e-07\n",
      "8277 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.07398e-07\n",
      "8278 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.08685e-07\n",
      "8279 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.65966e-07\n",
      "8280 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.27932e-07\n",
      "8281 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.93163e-07\n",
      "8282 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.7928e-07\n",
      "8283 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.4103e-07\n",
      "8284 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.51349e-07\n",
      "8285 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.75198e-07\n",
      "8286 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.88062e-07\n",
      "8287 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 9.68736e-08\n",
      "8288 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.67614e-07\n",
      "8289 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.57298e-07\n",
      "8290 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.14262e-07\n",
      "8291 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.82939e-07\n",
      "8292 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 2.41294e-07\n",
      "8293 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.31171e-07\n",
      "8294 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 3.26673e-07\n",
      "8295 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.83762e-07\n",
      "8296 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 2.43401e-07\n",
      "8297 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.50034e-07\n",
      "8298 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.15985e-07\n",
      "8299 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 2.26035e-07\n",
      "8300 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.65869e-07\n",
      "8301 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.45664e-07\n",
      "8302 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.94497e-07\n",
      "8303 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.66332e-07\n",
      "8304 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.83943e-07\n",
      "8305 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.99341e-07\n",
      "8306 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 6.68035e-07\n",
      "8307 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.59264e-07\n",
      "8308 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.38696e-07\n",
      "8309 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 9.57574e-08\n",
      "8310 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.80933e-07\n",
      "8311 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 8.82923e-07\n",
      "8312 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.99219e-07\n",
      "8313 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.43058e-07\n",
      "8314 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.58363e-07\n",
      "8315 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.62303e-07\n",
      "8316 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.09382e-07\n",
      "8317 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.06927e-07\n",
      "8318 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.47814e-07\n",
      "8319 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.06399e-07\n",
      "8320 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.74873e-07\n",
      "8321 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.56656e-07\n",
      "8322 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.8594e-07\n",
      "8323 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.25401e-07\n",
      "8324 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.54029e-07\n",
      "8325 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.13614e-07\n",
      "8326 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.52086e-07\n",
      "8327 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.87603e-07\n",
      "8328 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.84946e-08\n",
      "8329 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.11207e-07\n",
      "8330 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.58439e-07\n",
      "8331 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.63738e-07\n",
      "8332 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.82186e-07\n",
      "8333 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.64891e-07\n",
      "8334 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.60812e-07\n",
      "8335 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.82221e-07\n",
      "8336 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.41338e-07\n",
      "8337 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.00393e-07\n",
      "8338 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.3795e-07\n",
      "8339 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.41098e-07\n",
      "8340 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.38307e-07\n",
      "8341 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.18936e-07\n",
      "8342 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.03545e-07\n",
      "8343 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.24508e-07\n",
      "8344 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.91366e-07\n",
      "8345 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.19116e-07\n",
      "8346 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.00763e-07\n",
      "8347 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.30979e-07\n",
      "8348 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.32858e-07\n",
      "8349 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.61657e-07\n",
      "8350 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 5.31939e-07\n",
      "8351 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.54689e-07\n",
      "8352 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.17901e-07\n",
      "8353 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.59565e-07\n",
      "8354 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.62113e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8355 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.50557e-07\n",
      "8356 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.99355e-07\n",
      "8357 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.94868e-07\n",
      "8358 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.8896e-07\n",
      "8359 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.68064e-07\n",
      "8360 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.63456e-07\n",
      "8361 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.29381e-07\n",
      "8362 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.07537e-07\n",
      "8363 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.04003e-07\n",
      "8364 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.29049e-07\n",
      "8365 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.91058e-07\n",
      "8366 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.86353e-07\n",
      "8367 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.75404e-07\n",
      "8368 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.30124e-07\n",
      "8369 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.12783e-07\n",
      "8370 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.91163e-07\n",
      "8371 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.07561e-07\n",
      "8372 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 2.36668e-07\n",
      "8373 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.12538e-07\n",
      "8374 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.03407e-07\n",
      "8375 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.48691e-07\n",
      "8376 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.11623e-07\n",
      "8377 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.82651e-07\n",
      "8378 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.45235e-07\n",
      "8379 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 8.67702e-08\n",
      "8380 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.14243e-07\n",
      "8381 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.26756e-07\n",
      "8382 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.25025e-07\n",
      "8383 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.38221e-07\n",
      "8384 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.49969e-07\n",
      "8385 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.56492e-07\n",
      "8386 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.25247e-07\n",
      "8387 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.94553e-07\n",
      "8388 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.12004e-07\n",
      "8389 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 9.89661e-08\n",
      "8390 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.90406e-07\n",
      "8391 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.54437e-07\n",
      "8392 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.6908e-07\n",
      "8393 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.53355e-07\n",
      "8394 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.93676e-07\n",
      "8395 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.79018e-07\n",
      "8396 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.05876e-07\n",
      "8397 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.96326e-07\n",
      "8398 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.08771e-07\n",
      "8399 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.20699e-07\n",
      "8400 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.72618e-07\n",
      "8401 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.36652e-07\n",
      "8402 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.66397e-07\n",
      "8403 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.56873e-07\n",
      "8404 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.15455e-07\n",
      "8405 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.82258e-07\n",
      "8406 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.89574e-07\n",
      "8407 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.20008e-07\n",
      "8408 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 4.37186e-07\n",
      "8409 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.43742e-07\n",
      "8410 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.30814e-07\n",
      "8411 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.46755e-07\n",
      "8412 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.67838e-07\n",
      "8413 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.79763e-07\n",
      "8414 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.24913e-07\n",
      "8415 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.33947e-07\n",
      "8416 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.35246e-07\n",
      "8417 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.83097e-07\n",
      "8418 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.18429e-07\n",
      "8419 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.18384e-07\n",
      "8420 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.50388e-07\n",
      "8421 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.13749e-07\n",
      "8422 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.3141e-07\n",
      "8423 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.88944e-07\n",
      "8424 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.77621e-07\n",
      "8425 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.12876e-07\n",
      "8426 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 2.0951e-07\n",
      "8427 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.30671e-07\n",
      "8428 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.05643e-07\n",
      "8429 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.09775e-07\n",
      "8430 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.40934e-07\n",
      "8431 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.37214e-07\n",
      "8432 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.55972e-07\n",
      "8433 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.63435e-07\n",
      "8434 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.85282e-07\n",
      "8435 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.49943e-07\n",
      "8436 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.85138e-07\n",
      "8437 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.32041e-07\n",
      "8438 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.47292e-07\n",
      "8439 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.82556e-07\n",
      "8440 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.9271e-07\n",
      "8441 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.2148e-07\n",
      "8442 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.983e-07\n",
      "8443 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.27394e-07\n",
      "8444 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.11053e-07\n",
      "8445 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.47501e-07\n",
      "8446 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.4062e-07\n",
      "8447 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.2787e-07\n",
      "8448 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.78783e-07\n",
      "8449 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.54955e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8450 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.75591e-07\n",
      "8451 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.46954e-07\n",
      "8452 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.49289e-07\n",
      "8453 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.94021e-07\n",
      "8454 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.20041e-07\n",
      "8455 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.43705e-08\n",
      "8456 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.1889e-07\n",
      "8457 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.35642e-07\n",
      "8458 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.61581e-07\n",
      "8459 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.52368e-07\n",
      "8460 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.65585e-07\n",
      "8461 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.03216e-07\n",
      "8462 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.47078e-07\n",
      "8463 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.07646e-07\n",
      "8464 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.64e-07\n",
      "8465 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.16404e-07\n",
      "8466 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.15923e-07\n",
      "8467 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.6458e-07\n",
      "8468 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 6.81738e-07\n",
      "8469 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.93912e-07\n",
      "8470 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.108e-07\n",
      "8471 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.1122e-07\n",
      "8472 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.15415e-07\n",
      "8473 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.21334e-07\n",
      "8474 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.05908e-07\n",
      "8475 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.16965e-07\n",
      "8476 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.00271e-07\n",
      "8477 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.04416e-08\n",
      "8478 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.44039e-07\n",
      "8479 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.0252e-07\n",
      "8480 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.6547e-07\n",
      "8481 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 4.49114e-07\n",
      "8482 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.78461e-07\n",
      "8483 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.01038e-07\n",
      "8484 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 7.80914e-08\n",
      "8485 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.16717e-07\n",
      "8486 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.06196e-07\n",
      "8487 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.11191e-07\n",
      "8488 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.01949e-07\n",
      "8489 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.41818e-07\n",
      "8490 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.76104e-07\n",
      "8491 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.23127e-07\n",
      "8492 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.85578e-07\n",
      "8493 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.2866e-07\n",
      "8494 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.45988e-07\n",
      "8495 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.93987e-07\n",
      "8496 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.84523e-07\n",
      "8497 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.04769e-07\n",
      "8498 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.15352e-07\n",
      "8499 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 3.6177e-06\n",
      "8500 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.96581e-07\n",
      "8501 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.98069e-07\n",
      "8502 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 8.58956e-08\n",
      "8503 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.27494e-07\n",
      "8504 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.00206e-07\n",
      "8505 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.26008e-07\n",
      "8506 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 6.61574e-08\n",
      "8507 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 6.52799e-08\n",
      "8508 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.72985e-08\n",
      "8509 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 9.77394e-08\n",
      "8510 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.30208e-07\n",
      "8511 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.20197e-07\n",
      "8512 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.1407e-07\n",
      "8513 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.49758e-07\n",
      "8514 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.81393e-07\n",
      "8515 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.37694e-07\n",
      "8516 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.16907e-07\n",
      "8517 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.99037e-07\n",
      "8518 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.82057e-07\n",
      "8519 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.28345e-07\n",
      "8520 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.15321e-07\n",
      "8521 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 5.86922e-07\n",
      "8522 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.70084e-07\n",
      "8523 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 8.59669e-08\n",
      "8524 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.57544e-08\n",
      "8525 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.55957e-07\n",
      "8526 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.58456e-07\n",
      "8527 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.33345e-08\n",
      "8528 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.02346e-07\n",
      "8529 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 2.21892e-07\n",
      "8530 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.11978e-07\n",
      "8531 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.45564e-07\n",
      "8532 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.24564e-07\n",
      "8533 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.54741e-07\n",
      "8534 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.16895e-07\n",
      "8535 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.15085e-07\n",
      "8536 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.70845e-07\n",
      "8537 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.96529e-07\n",
      "8538 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.92322e-07\n",
      "8539 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.40615e-07\n",
      "8540 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.54944e-07\n",
      "8541 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.07331e-07\n",
      "8542 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.26743e-07\n",
      "8543 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 7.79488e-08\n",
      "8544 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.20484e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8545 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.46199e-07\n",
      "8546 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.90122e-07\n",
      "8547 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.34868e-07\n",
      "8548 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 1.68761e-06\n",
      "8549 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.28311e-07\n",
      "8550 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.56397e-07\n",
      "8551 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.80406e-07\n",
      "8552 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.11928e-07\n",
      "8553 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.1474e-07\n",
      "8554 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.25302e-07\n",
      "8555 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.80523e-08\n",
      "8556 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.58932e-07\n",
      "8557 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.1926e-07\n",
      "8558 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.87812e-08\n",
      "8559 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 9.35412e-08\n",
      "8560 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.71424e-07\n",
      "8561 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.68507e-07\n",
      "8562 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.04972e-07\n",
      "8563 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.01009e-07\n",
      "8564 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.75726e-07\n",
      "8565 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.60343e-07\n",
      "8566 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.00793e-07\n",
      "8567 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.68582e-07\n",
      "8568 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 3.69697e-07\n",
      "8569 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.09488e-07\n",
      "8570 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.56347e-07\n",
      "8571 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.87278e-07\n",
      "8572 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.54363e-07\n",
      "8573 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.80824e-07\n",
      "8574 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.17016e-07\n",
      "8575 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.85333e-07\n",
      "8576 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.0641e-07\n",
      "8577 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.86876e-07\n",
      "8578 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.82752e-07\n",
      "8579 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.5498e-07\n",
      "8580 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.71264e-07\n",
      "8581 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.52043e-07\n",
      "8582 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.72385e-07\n",
      "8583 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.32863e-07\n",
      "8584 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.09844e-07\n",
      "8585 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.0574e-07\n",
      "8586 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 7.78455e-08\n",
      "8587 Train total loss: 2.92515 \tReconstruction loss: 2.92514 \tLatent loss: 2.26531e-07\n",
      "8588 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.02738e-07\n",
      "8589 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.10324e-07\n",
      "8590 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.33077e-07\n",
      "8591 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.78076e-07\n",
      "8592 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.31915e-07\n",
      "8593 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.75576e-07\n",
      "8594 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.87247e-07\n",
      "8595 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.24557e-07\n",
      "8596 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.50282e-07\n",
      "8597 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.20668e-07\n",
      "8598 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.38633e-07\n",
      "8599 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.59743e-07\n",
      "8600 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.80826e-07\n",
      "8601 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 8.82792e-08\n",
      "8602 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.4311e-07\n",
      "8603 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.41682e-07\n",
      "8604 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.89504e-07\n",
      "8605 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.0169e-07\n",
      "8606 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.22397e-07\n",
      "8607 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.10181e-07\n",
      "8608 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.82208e-07\n",
      "8609 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.75794e-07\n",
      "8610 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.2311e-07\n",
      "8611 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.37598e-07\n",
      "8612 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.0419e-07\n",
      "8613 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.20175e-07\n",
      "8614 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.69176e-07\n",
      "8615 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.7924e-07\n",
      "8616 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.08581e-07\n",
      "8617 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.20665e-07\n",
      "8618 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.01957e-07\n",
      "8619 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.00995e-07\n",
      "8620 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.40184e-07\n",
      "8621 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.95966e-07\n",
      "8622 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.67089e-07\n",
      "8623 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.46397e-07\n",
      "8624 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 3.47523e-07\n",
      "8625 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.85436e-07\n",
      "8626 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.74616e-07\n",
      "8627 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.65642e-07\n",
      "8628 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.29372e-07\n",
      "8629 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.81871e-07\n",
      "8630 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.99339e-07\n",
      "8631 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.15495e-07\n",
      "8632 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.15721e-07\n",
      "8633 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.46429e-07\n",
      "8634 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.7393e-07\n",
      "8635 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.1954e-07\n",
      "8636 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.51896e-07\n",
      "8637 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.59667e-07\n",
      "8638 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.74729e-07\n",
      "8639 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.07979e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8640 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.41641e-07\n",
      "8641 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.84262e-07\n",
      "8642 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.35555e-07\n",
      "8643 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.74256e-07\n",
      "8644 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.3566e-07\n",
      "8645 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.92496e-07\n",
      "8646 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 6.72262e-07\n",
      "8647 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.00272e-07\n",
      "8648 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.16018e-07\n",
      "8649 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.19839e-07\n",
      "8650 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.29924e-07\n",
      "8651 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.11327e-07\n",
      "8652 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.32703e-07\n",
      "8653 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.15071e-07\n",
      "8654 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.17638e-07\n",
      "8655 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.26489e-07\n",
      "8656 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.15969e-07\n",
      "8657 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.04686e-07\n",
      "8658 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.62693e-07\n",
      "8659 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.42462e-07\n",
      "8660 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.10272e-07\n",
      "8661 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.6352e-07\n",
      "8662 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.63351e-07\n",
      "8663 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.80132e-07\n",
      "8664 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.55769e-07\n",
      "8665 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.00626e-07\n",
      "8666 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.78178e-07\n",
      "8667 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.82241e-07\n",
      "8668 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.73921e-07\n",
      "8669 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.28504e-07\n",
      "8670 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.53896e-07\n",
      "8671 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.53497e-07\n",
      "8672 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.82779e-07\n",
      "8673 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.62084e-07\n",
      "8674 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.46062e-07\n",
      "8675 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.45355e-07\n",
      "8676 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.43793e-07\n",
      "8677 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.69132e-07\n",
      "8678 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.61187e-07\n",
      "8679 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.67722e-07\n",
      "8680 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.11314e-08\n",
      "8681 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.49568e-07\n",
      "8682 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.2488e-07\n",
      "8683 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.50215e-07\n",
      "8684 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.34332e-07\n",
      "8685 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.60402e-07\n",
      "8686 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.7237e-07\n",
      "8687 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 5.25248e-07\n",
      "8688 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.14954e-07\n",
      "8689 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.20427e-07\n",
      "8690 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.52418e-07\n",
      "8691 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.17434e-06\n",
      "8692 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.48991e-07\n",
      "8693 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.15737e-08\n",
      "8694 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.16811e-08\n",
      "8695 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.54792e-08\n",
      "8696 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.11551e-07\n",
      "8697 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.62827e-07\n",
      "8698 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.92673e-08\n",
      "8699 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 8.90912e-08\n",
      "8700 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.03521e-07\n",
      "8701 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.54578e-07\n",
      "8702 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.77123e-07\n",
      "8703 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.44414e-07\n",
      "8704 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.27174e-07\n",
      "8705 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.93586e-07\n",
      "8706 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.5899e-07\n",
      "8707 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.36605e-07\n",
      "8708 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.20084e-07\n",
      "8709 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.15081e-07\n",
      "8710 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.22254e-07\n",
      "8711 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.44325e-07\n",
      "8712 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.33661e-07\n",
      "8713 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.32541e-07\n",
      "8714 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.96e-07\n",
      "8715 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.11706e-07\n",
      "8716 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.6163e-07\n",
      "8717 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.16637e-07\n",
      "8718 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.02309e-07\n",
      "8719 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.3269e-07\n",
      "8720 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.38363e-07\n",
      "8721 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.05966e-07\n",
      "8722 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.72419e-07\n",
      "8723 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.87286e-07\n",
      "8724 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.30828e-07\n",
      "8725 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.17667e-07\n",
      "8726 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 9.66829e-08\n",
      "8727 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.06776e-07\n",
      "8728 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.97023e-07\n",
      "8729 Train total loss: 2.85855 \tReconstruction loss: 2.85854 \tLatent loss: 2.43147e-07\n",
      "8730 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.42471e-07\n",
      "8731 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.28426e-07\n",
      "8732 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.57554e-07\n",
      "8733 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.0295e-07\n",
      "8734 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.25552e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8735 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.16239e-07\n",
      "8736 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.10887e-07\n",
      "8737 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.54455e-06\n",
      "8738 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.54657e-07\n",
      "8739 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.28977e-08\n",
      "8740 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 8.35571e-08\n",
      "8741 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.52185e-07\n",
      "8742 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 9.66524e-08\n",
      "8743 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 3.73519e-07\n",
      "8744 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.29825e-07\n",
      "8745 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.64799e-07\n",
      "8746 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.95159e-07\n",
      "8747 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.40091e-07\n",
      "8748 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.34065e-07\n",
      "8749 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.41076e-07\n",
      "8750 Train total loss: 2.98601 \tReconstruction loss: 2.98601 \tLatent loss: 3.65041e-07\n",
      "8751 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.93561e-06\n",
      "8752 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.01355e-06\n",
      "8753 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.45689e-07\n",
      "8754 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 9.27015e-08\n",
      "8755 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.50612e-08\n",
      "8756 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 6.37415e-07\n",
      "8757 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.06405e-07\n",
      "8758 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.92442e-07\n",
      "8759 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 8.43851e-08\n",
      "8760 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.91575e-07\n",
      "8761 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 4.50134e-08\n",
      "8762 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.6648e-07\n",
      "8763 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.00512e-08\n",
      "8764 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.48228e-08\n",
      "8765 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 6.10453e-08\n",
      "8766 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.85044e-07\n",
      "8767 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.41436e-08\n",
      "8768 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.26837e-08\n",
      "8769 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.20056e-08\n",
      "8770 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.00471e-07\n",
      "8771 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.76591e-08\n",
      "8772 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.22488e-07\n",
      "8773 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.94199e-07\n",
      "8774 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 8.80376e-08\n",
      "8775 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 6.7873e-08\n",
      "8776 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 7.09493e-08\n",
      "8777 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.08637e-07\n",
      "8778 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.32686e-07\n",
      "8779 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.13634e-07\n",
      "8780 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.08207e-07\n",
      "8781 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.62968e-07\n",
      "8782 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 6.0536e-08\n",
      "8783 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.05923e-07\n",
      "8784 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.26121e-07\n",
      "8785 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.48482e-07\n",
      "8786 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.65156e-07\n",
      "8787 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.64405e-07\n",
      "8788 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.55991e-07\n",
      "8789 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 6.34009e-07\n",
      "8790 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.32621e-07\n",
      "8791 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.60883e-07\n",
      "8792 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.69921e-07\n",
      "8793 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.34138e-07\n",
      "8794 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.52497e-07\n",
      "8795 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.38118e-08\n",
      "8796 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.12362e-07\n",
      "8797 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.86663e-08\n",
      "8798 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.29724e-07\n",
      "8799 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.64498e-07\n",
      "8800 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.75529e-07\n",
      "8801 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.15611e-07\n",
      "8802 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.35925e-07\n",
      "8803 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 5.4039e-07\n",
      "8804 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.30795e-07\n",
      "8805 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.29803e-07\n",
      "8806 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.78468e-07\n",
      "8807 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.74218e-07\n",
      "8808 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.18116e-07\n",
      "8809 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.74435e-07\n",
      "8810 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.17056e-07\n",
      "8811 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.61775e-07\n",
      "8812 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 6.39461e-07\n",
      "8813 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.80448e-07\n",
      "8814 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.84584e-06\n",
      "8815 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 4.70117e-07\n",
      "8816 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 7.01315e-08\n",
      "8817 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 7.15125e-08\n",
      "8818 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.4058e-07\n",
      "8819 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.0598e-07\n",
      "8820 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.42887e-08\n",
      "8821 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 7.37855e-08\n",
      "8822 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.57427e-08\n",
      "8823 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 7.61967e-08\n",
      "8824 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.27399e-07\n",
      "8825 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 8.50458e-08\n",
      "8826 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 4.14236e-07\n",
      "8827 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.35824e-07\n",
      "8828 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 1.35173e-07\n",
      "8829 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.00356e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8830 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.97771e-07\n",
      "8831 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.40159e-07\n",
      "8832 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.02443e-07\n",
      "8833 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.8728e-07\n",
      "8834 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.42489e-07\n",
      "8835 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.4646e-07\n",
      "8836 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.51184e-07\n",
      "8837 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.96749e-07\n",
      "8838 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.47708e-07\n",
      "8839 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.95361e-07\n",
      "8840 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.10352e-07\n",
      "8841 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.08878e-07\n",
      "8842 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.71385e-07\n",
      "8843 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.90832e-07\n",
      "8844 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.68927e-07\n",
      "8845 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.90159e-07\n",
      "8846 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.98145e-07\n",
      "8847 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.30873e-07\n",
      "8848 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 6.8076e-07\n",
      "8849 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.13331e-07\n",
      "8850 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 8.79605e-08\n",
      "8851 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.36119e-07\n",
      "8852 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 4.85657e-07\n",
      "8853 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.50554e-07\n",
      "8854 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.31656e-07\n",
      "8855 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.22889e-07\n",
      "8856 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.2409e-08\n",
      "8857 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.45737e-07\n",
      "8858 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 9.56017e-08\n",
      "8859 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 4.02899e-08\n",
      "8860 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.43398e-08\n",
      "8861 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.30318e-07\n",
      "8862 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.81788e-07\n",
      "8863 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 5.19009e-08\n",
      "8864 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.88904e-08\n",
      "8865 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.75819e-07\n",
      "8866 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.20822e-07\n",
      "8867 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.99312e-07\n",
      "8868 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.42326e-07\n",
      "8869 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.38187e-07\n",
      "8870 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.43194e-07\n",
      "8871 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.70621e-07\n",
      "8872 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.40767e-07\n",
      "8873 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.7214e-07\n",
      "8874 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.84565e-07\n",
      "8875 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.21427e-07\n",
      "8876 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.77111e-07\n",
      "8877 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.18634e-07\n",
      "8878 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 2.69522e-07\n",
      "8879 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.0521e-07\n",
      "8880 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.53084e-07\n",
      "8881 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.82411e-07\n",
      "8882 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.44124e-07\n",
      "8883 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.09358e-07\n",
      "8884 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.27439e-07\n",
      "8885 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.43903e-07\n",
      "8886 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 3.06545e-07\n",
      "8887 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.91364e-08\n",
      "8888 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.35572e-07\n",
      "8889 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.78075e-07\n",
      "8890 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.42187e-07\n",
      "8891 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.58432e-07\n",
      "8892 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.03886e-07\n",
      "8893 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.30673e-07\n",
      "8894 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.63551e-07\n",
      "8895 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.47834e-07\n",
      "8896 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.90471e-07\n",
      "8897 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.4555e-07\n",
      "8898 Train total loss: 2.77429 \tReconstruction loss: 2.77428 \tLatent loss: 2.81626e-06\n",
      "8899 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 6.95907e-07\n",
      "8900 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.90186e-07\n",
      "8901 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.5618e-07\n",
      "8902 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 8.38307e-08\n",
      "8903 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.08394e-07\n",
      "8904 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.57672e-08\n",
      "8905 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.60774e-07\n",
      "8906 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.51084e-07\n",
      "8907 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.40432e-08\n",
      "8908 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.37289e-07\n",
      "8909 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.18122e-08\n",
      "8910 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.28983e-07\n",
      "8911 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.86529e-07\n",
      "8912 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 7.55506e-08\n",
      "8913 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.37959e-07\n",
      "8914 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.18047e-07\n",
      "8915 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.12625e-07\n",
      "8916 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.34586e-07\n",
      "8917 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 6.47808e-08\n",
      "8918 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.61895e-08\n",
      "8919 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.81577e-07\n",
      "8920 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.664e-07\n",
      "8921 Train total loss: 2.80587 \tReconstruction loss: 2.80586 \tLatent loss: 9.88592e-07\n",
      "8922 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.48256e-07\n",
      "8923 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 8.44695e-08\n",
      "8924 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.40481e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8925 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.65847e-07\n",
      "8926 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.7272e-07\n",
      "8927 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.0564e-07\n",
      "8928 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.02262e-07\n",
      "8929 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.03201e-07\n",
      "8930 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.15233e-07\n",
      "8931 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.11703e-07\n",
      "8932 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 2.55591e-07\n",
      "8933 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.71735e-07\n",
      "8934 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.50019e-07\n",
      "8935 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.36805e-07\n",
      "8936 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.72286e-07\n",
      "8937 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 7.02334e-08\n",
      "8938 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.55839e-07\n",
      "8939 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.67545e-07\n",
      "8940 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.42732e-07\n",
      "8941 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.24282e-07\n",
      "8942 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.14423e-07\n",
      "8943 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.98532e-07\n",
      "8944 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.01248e-07\n",
      "8945 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.74792e-07\n",
      "8946 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.79923e-07\n",
      "8947 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.49829e-07\n",
      "8948 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.0732e-07\n",
      "8949 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.28853e-07\n",
      "8950 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.25972e-07\n",
      "8951 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.40191e-07\n",
      "8952 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.36356e-07\n",
      "8953 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.02089e-07\n",
      "8954 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 7.58989e-07\n",
      "8955 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.25546e-07\n",
      "8956 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 6.71338e-08\n",
      "8957 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.61956e-07\n",
      "8958 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.92788e-08\n",
      "8959 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 8.60338e-08\n",
      "8960 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.47007e-07\n",
      "8961 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.90229e-08\n",
      "8962 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.65807e-07\n",
      "8963 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.1121e-07\n",
      "8964 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.558e-07\n",
      "8965 Train total loss: 2.91475 \tReconstruction loss: 2.91474 \tLatent loss: 4.88348e-07\n",
      "8966 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.87691e-07\n",
      "8967 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.41121e-07\n",
      "8968 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 6.80915e-07\n",
      "8969 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.14136e-07\n",
      "8970 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.25772e-07\n",
      "8971 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.57562e-07\n",
      "8972 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.03158e-07\n",
      "8973 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.33548e-07\n",
      "8974 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.66945e-07\n",
      "8975 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 8.76462e-08\n",
      "8976 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.46816e-07\n",
      "8977 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.17969e-07\n",
      "8978 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.75847e-07\n",
      "8979 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.07377e-07\n",
      "8980 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.12194e-07\n",
      "8981 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.03868e-07\n",
      "8982 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.37179e-07\n",
      "8983 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.07667e-07\n",
      "8984 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.79296e-07\n",
      "8985 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.58595e-07\n",
      "8986 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.54111e-07\n",
      "8987 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.20423e-07\n",
      "8988 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.62947e-07\n",
      "8989 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.55514e-07\n",
      "8990 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.6561e-07\n",
      "8991 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 6.66891e-07\n",
      "8992 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.44397e-07\n",
      "8993 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.45552e-07\n",
      "8994 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.86576e-07\n",
      "8995 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.70669e-08\n",
      "8996 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.77269e-07\n",
      "8997 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.53812e-07\n",
      "8998 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.18088e-07\n",
      "8999 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.73591e-07\n",
      "9000 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.25625e-07\n",
      "9001 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.28085e-07\n",
      "9002 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.08516e-07\n",
      "9003 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.41895e-07\n",
      "9004 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.10375e-07\n",
      "9005 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.2587e-07\n",
      "9006 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.78414e-07\n",
      "9007 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.22542e-07\n",
      "9008 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.9862e-07\n",
      "9009 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.17205e-07\n",
      "9010 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 1.5813e-07\n",
      "9011 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.03556e-07\n",
      "9012 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.02045e-07\n",
      "9013 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.94232e-07\n",
      "9014 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 3.88363e-07\n",
      "9015 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.81634e-07\n",
      "9016 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.80064e-07\n",
      "9017 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.99991e-07\n",
      "9018 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.20288e-07\n",
      "9019 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.0699e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9020 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 8.80228e-07\n",
      "9021 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.72076e-07\n",
      "9022 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.38824e-07\n",
      "9023 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.20987e-07\n",
      "9024 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.12002e-07\n",
      "9025 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.06104e-08\n",
      "9026 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.86668e-07\n",
      "9027 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.02512e-07\n",
      "9028 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.76797e-07\n",
      "9029 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.89395e-07\n",
      "9030 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.50997e-07\n",
      "9031 Train total loss: 2.94646 \tReconstruction loss: 2.94645 \tLatent loss: 4.80769e-07\n",
      "9032 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.87645e-07\n",
      "9033 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.60268e-07\n",
      "9034 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.54965e-07\n",
      "9035 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.26382e-07\n",
      "9036 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.1656e-07\n",
      "9037 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.9678e-07\n",
      "9038 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.58322e-07\n",
      "9039 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.68329e-06\n",
      "9040 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 5.27616e-07\n",
      "9041 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.15181e-07\n",
      "9042 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 6.32281e-08\n",
      "9043 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 7.58257e-08\n",
      "9044 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.91215e-07\n",
      "9045 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.83316e-07\n",
      "9046 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 6.13509e-08\n",
      "9047 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 5.27507e-08\n",
      "9048 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 7.34042e-08\n",
      "9049 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.89383e-08\n",
      "9050 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.66651e-07\n",
      "9051 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.45409e-07\n",
      "9052 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 8.23071e-08\n",
      "9053 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.25481e-07\n",
      "9054 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.26548e-07\n",
      "9055 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.34172e-08\n",
      "9056 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.03964e-08\n",
      "9057 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 7.67352e-08\n",
      "9058 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.14778e-07\n",
      "9059 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.35502e-07\n",
      "9060 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.03077e-08\n",
      "9061 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.57231e-07\n",
      "9062 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.34687e-07\n",
      "9063 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.22949e-07\n",
      "9064 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.47551e-07\n",
      "9065 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.70413e-07\n",
      "9066 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.96e-07\n",
      "9067 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.28604e-07\n",
      "9068 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.11649e-07\n",
      "9069 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.19814e-07\n",
      "9070 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.56529e-07\n",
      "9071 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.9069e-07\n",
      "9072 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.26041e-07\n",
      "9073 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.80968e-07\n",
      "9074 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.20267e-07\n",
      "9075 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 9.74635e-07\n",
      "9076 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.34201e-07\n",
      "9077 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.71035e-08\n",
      "9078 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.04599e-07\n",
      "9079 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 8.79387e-08\n",
      "9080 Train total loss: 3.03981 \tReconstruction loss: 3.0398 \tLatent loss: 2.02423e-07\n",
      "9081 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.32907e-07\n",
      "9082 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.93166e-07\n",
      "9083 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.41206e-07\n",
      "9084 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.71248e-07\n",
      "9085 Train total loss: 2.91475 \tReconstruction loss: 2.91474 \tLatent loss: 7.11481e-07\n",
      "9086 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.32078e-07\n",
      "9087 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.41864e-07\n",
      "9088 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 9.82196e-08\n",
      "9089 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.47465e-07\n",
      "9090 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.64857e-07\n",
      "9091 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.44743e-07\n",
      "9092 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.482e-07\n",
      "9093 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.45013e-07\n",
      "9094 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 9.98902e-08\n",
      "9095 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.16861e-07\n",
      "9096 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.9053e-07\n",
      "9097 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.28266e-07\n",
      "9098 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.85857e-07\n",
      "9099 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.7635e-07\n",
      "9100 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 4.77879e-07\n",
      "9101 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.25503e-07\n",
      "9102 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.06837e-07\n",
      "9103 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 9.08825e-08\n",
      "9104 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.09265e-07\n",
      "9105 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.24766e-07\n",
      "9106 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.03208e-07\n",
      "9107 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.23351e-07\n",
      "9108 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.70008e-07\n",
      "9109 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.35171e-07\n",
      "9110 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.26178e-07\n",
      "9111 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 7.20353e-07\n",
      "9112 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.93126e-07\n",
      "9113 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.94922e-07\n",
      "9114 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 2.59171e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9115 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.53236e-07\n",
      "9116 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.19084e-07\n",
      "9117 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 7.96517e-07\n",
      "9118 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.83228e-07\n",
      "9119 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.84766e-07\n",
      "9120 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 5.29012e-07\n",
      "9121 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.8847e-07\n",
      "9122 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.3001e-07\n",
      "9123 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.42369e-07\n",
      "9124 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.33224e-07\n",
      "9125 Train total loss: 2.91475 \tReconstruction loss: 2.91474 \tLatent loss: 4.19892e-06\n",
      "9126 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 6.66198e-07\n",
      "9127 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.82276e-07\n",
      "9128 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.90672e-07\n",
      "9129 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.05702e-07\n",
      "9130 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 9.13758e-08\n",
      "9131 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.37024e-08\n",
      "9132 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.10169e-07\n",
      "9133 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.58533e-07\n",
      "9134 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.47908e-07\n",
      "9135 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.72737e-07\n",
      "9136 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 5.13735e-07\n",
      "9137 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.75179e-07\n",
      "9138 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.08684e-07\n",
      "9139 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.91136e-07\n",
      "9140 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 7.63664e-07\n",
      "9141 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.62722e-07\n",
      "9142 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.46455e-07\n",
      "9143 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.59776e-07\n",
      "9144 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.38928e-07\n",
      "9145 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.42903e-07\n",
      "9146 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.48253e-07\n",
      "9147 Train total loss: 2.92518 \tReconstruction loss: 2.92518 \tLatent loss: 1.83094e-07\n",
      "9148 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.32191e-07\n",
      "9149 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.44544e-07\n",
      "9150 Train total loss: 2.986 \tReconstruction loss: 2.98599 \tLatent loss: 1.21839e-07\n",
      "9151 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.50529e-07\n",
      "9152 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.34485e-07\n",
      "9153 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.30348e-07\n",
      "9154 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 1.70576e-07\n",
      "9155 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 8.77315e-07\n",
      "9156 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.68077e-07\n",
      "9157 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.31567e-08\n",
      "9158 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.51109e-08\n",
      "9159 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 7.00034e-08\n",
      "9160 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.79598e-07\n",
      "9161 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.19382e-07\n",
      "9162 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.18806e-08\n",
      "9163 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.9753e-08\n",
      "9164 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 1.21785e-07\n",
      "9165 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.18033e-07\n",
      "9166 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 5.54865e-08\n",
      "9167 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.03473e-07\n",
      "9168 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.23763e-07\n",
      "9169 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 4.15866e-07\n",
      "9170 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 1.73649e-07\n",
      "9171 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 8.20161e-08\n",
      "9172 Train total loss: 2.92841 \tReconstruction loss: 2.92841 \tLatent loss: 7.62186e-08\n",
      "9173 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 8.70845e-08\n",
      "9174 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.34364e-08\n",
      "9175 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 8.91741e-08\n",
      "9176 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.51574e-07\n",
      "9177 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.20154e-07\n",
      "9178 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 6.91896e-07\n",
      "9179 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.94173e-07\n",
      "9180 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.80976e-07\n",
      "9181 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.17718e-07\n",
      "9182 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.75322e-07\n",
      "9183 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.08293e-07\n",
      "9184 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.39028e-07\n",
      "9185 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 3.4134e-07\n",
      "9186 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.05333e-07\n",
      "9187 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.10239e-07\n",
      "9188 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.55126e-07\n",
      "9189 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.30326e-07\n",
      "9190 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.67618e-07\n",
      "9191 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.12658e-07\n",
      "9192 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.69585e-07\n",
      "9193 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.70595e-07\n",
      "9194 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.53172e-07\n",
      "9195 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 8.53237e-08\n",
      "9196 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.39818e-07\n",
      "9197 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.29926e-08\n",
      "9198 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.81006e-07\n",
      "9199 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.55076e-07\n",
      "9200 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.16188e-07\n",
      "9201 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.0368e-07\n",
      "9202 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.43975e-07\n",
      "9203 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.08627e-07\n",
      "9204 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.10774e-07\n",
      "9205 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.38851e-07\n",
      "9206 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 2.12886e-07\n",
      "9207 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 5.67266e-07\n",
      "9208 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.34981e-07\n",
      "9209 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.45715e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9210 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.38974e-07\n",
      "9211 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.82618e-07\n",
      "9212 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.34464e-07\n",
      "9213 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.32255e-07\n",
      "9214 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.8594e-07\n",
      "9215 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.30901e-07\n",
      "9216 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.60013e-07\n",
      "9217 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.25219e-07\n",
      "9218 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.49956e-06\n",
      "9219 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.43631e-07\n",
      "9220 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.32988e-07\n",
      "9221 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.24714e-07\n",
      "9222 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.8556e-07\n",
      "9223 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.50847e-07\n",
      "9224 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 9.13118e-08\n",
      "9225 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 8.51956e-08\n",
      "9226 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.58259e-07\n",
      "9227 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 7.72925e-08\n",
      "9228 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.08907e-07\n",
      "9229 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.0082e-07\n",
      "9230 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.07274e-07\n",
      "9231 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.66889e-07\n",
      "9232 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.26337e-07\n",
      "9233 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.5375e-07\n",
      "9234 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.14876e-07\n",
      "9235 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.36507e-07\n",
      "9236 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.27643e-07\n",
      "9237 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.99764e-07\n",
      "9238 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.95127e-07\n",
      "9239 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.4982e-07\n",
      "9240 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.56584e-07\n",
      "9241 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.88335e-07\n",
      "9242 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 4.14603e-07\n",
      "9243 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.93017e-07\n",
      "9244 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.3502e-07\n",
      "9245 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.92401e-07\n",
      "9246 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.37594e-07\n",
      "9247 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.08714e-07\n",
      "9248 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.1808e-07\n",
      "9249 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.23266e-07\n",
      "9250 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.51808e-07\n",
      "9251 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.02098e-07\n",
      "9252 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.48086e-07\n",
      "9253 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.53798e-07\n",
      "9254 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.4843e-07\n",
      "9255 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.60871e-07\n",
      "9256 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.63799e-07\n",
      "9257 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.21518e-07\n",
      "9258 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.78211e-07\n",
      "9259 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 9.47417e-08\n",
      "9260 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.33768e-07\n",
      "9261 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.75195e-07\n",
      "9262 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.11945e-07\n",
      "9263 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.27698e-07\n",
      "9264 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.42933e-07\n",
      "9265 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.99569e-07\n",
      "9266 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.28971e-07\n",
      "9267 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.70775e-07\n",
      "9268 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.49589e-07\n",
      "9269 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.67565e-07\n",
      "9270 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 9.19221e-07\n",
      "9271 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.74345e-07\n",
      "9272 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.65106e-07\n",
      "9273 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.21785e-07\n",
      "9274 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.3122e-07\n",
      "9275 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.86036e-07\n",
      "9276 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.49115e-07\n",
      "9277 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.47743e-07\n",
      "9278 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.48379e-07\n",
      "9279 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.30421e-07\n",
      "9280 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.40202e-07\n",
      "9281 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 8.42963e-08\n",
      "9282 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.94169e-07\n",
      "9283 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.83875e-07\n",
      "9284 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.58606e-07\n",
      "9285 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.62199e-07\n",
      "9286 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.35716e-07\n",
      "9287 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.28808e-07\n",
      "9288 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.46548e-07\n",
      "9289 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.82671e-07\n",
      "9290 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.88725e-07\n",
      "9291 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.94452e-07\n",
      "9292 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.68973e-07\n",
      "9293 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.22766e-07\n",
      "9294 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.078e-07\n",
      "9295 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.04937e-07\n",
      "9296 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.01532e-07\n",
      "9297 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.19645e-07\n",
      "9298 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.18104e-07\n",
      "9299 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.19639e-07\n",
      "9300 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.48523e-07\n",
      "9301 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 5.86631e-08\n",
      "9302 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.75268e-07\n",
      "9303 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.25149e-07\n",
      "9304 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 9.46166e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9305 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.13828e-07\n",
      "9306 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.69043e-07\n",
      "9307 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.89341e-08\n",
      "9308 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 9.6162e-08\n",
      "9309 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.22986e-07\n",
      "9310 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.73683e-07\n",
      "9311 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.85114e-07\n",
      "9312 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.6879e-07\n",
      "9313 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.02846e-07\n",
      "9314 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.65254e-07\n",
      "9315 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.8649e-07\n",
      "9316 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.24893e-07\n",
      "9317 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.45419e-07\n",
      "9318 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.78296e-07\n",
      "9319 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.39938e-07\n",
      "9320 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.03206e-07\n",
      "9321 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 8.05681e-08\n",
      "9322 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.28339e-07\n",
      "9323 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.77755e-07\n",
      "9324 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.08054e-07\n",
      "9325 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.33902e-07\n",
      "9326 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 9.98727e-08\n",
      "9327 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.88491e-07\n",
      "9328 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.65151e-07\n",
      "9329 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.55979e-07\n",
      "9330 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.29875e-07\n",
      "9331 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.06369e-07\n",
      "9332 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.36399e-07\n",
      "9333 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.03914e-07\n",
      "9334 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.49663e-07\n",
      "9335 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.41715e-07\n",
      "9336 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.57195e-07\n",
      "9337 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.98656e-07\n",
      "9338 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.83419e-07\n",
      "9339 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.07875e-07\n",
      "9340 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 8.52961e-08\n",
      "9341 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.37736e-07\n",
      "9342 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.32091e-07\n",
      "9343 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.48513e-07\n",
      "9344 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.79852e-08\n",
      "9345 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.29266e-07\n",
      "9346 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.97656e-07\n",
      "9347 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.70116e-07\n",
      "9348 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 3.03272e-07\n",
      "9349 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.86841e-07\n",
      "9350 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.19476e-07\n",
      "9351 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.02937e-07\n",
      "9352 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 4.35417e-07\n",
      "9353 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.46299e-07\n",
      "9354 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.75386e-07\n",
      "9355 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.07325e-07\n",
      "9356 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.64752e-07\n",
      "9357 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.33065e-07\n",
      "9358 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.22543e-07\n",
      "9359 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 3.20835e-07\n",
      "9360 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 6.37403e-07\n",
      "9361 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 4.75715e-07\n",
      "9362 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.4431e-07\n",
      "9363 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.17572e-06\n",
      "9364 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.81076e-07\n",
      "9365 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.78922e-08\n",
      "9366 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.53408e-07\n",
      "9367 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 5.73936e-07\n",
      "9368 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.41077e-07\n",
      "9369 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.01445e-06\n",
      "9370 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.87576e-07\n",
      "9371 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.12268e-07\n",
      "9372 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.71337e-08\n",
      "9373 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.66975e-07\n",
      "9374 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 6.79167e-08\n",
      "9375 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.79485e-08\n",
      "9376 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 4.58604e-08\n",
      "9377 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 7.2681e-08\n",
      "9378 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 9.29169e-08\n",
      "9379 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.19425e-07\n",
      "9380 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 9.31468e-08\n",
      "9381 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 7.97387e-08\n",
      "9382 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.92264e-07\n",
      "9383 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.55771e-07\n",
      "9384 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.68634e-07\n",
      "9385 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.51391e-07\n",
      "9386 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 8.71587e-08\n",
      "9387 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.50395e-07\n",
      "9388 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.2969e-07\n",
      "9389 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.31314e-07\n",
      "9390 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 2.22065e-07\n",
      "9391 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.07796e-07\n",
      "9392 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 9.20525e-08\n",
      "9393 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 1.39825e-07\n",
      "9394 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.76001e-07\n",
      "9395 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 5.22076e-07\n",
      "9396 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.57884e-07\n",
      "9397 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.54221e-07\n",
      "9398 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 7.03571e-08\n",
      "9399 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.52606e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9400 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.30212e-07\n",
      "9401 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 6.5488e-08\n",
      "9402 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.11917e-07\n",
      "9403 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.55496e-07\n",
      "9404 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.77686e-07\n",
      "9405 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 8.88394e-08\n",
      "9406 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.29064e-07\n",
      "9407 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.95453e-07\n",
      "9408 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.32825e-07\n",
      "9409 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.80007e-07\n",
      "9410 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.24878e-07\n",
      "9411 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.16669e-07\n",
      "9412 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.74512e-07\n",
      "9413 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.36618e-08\n",
      "9414 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.16717e-07\n",
      "9415 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 1.42056e-07\n",
      "9416 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.43246e-07\n",
      "9417 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.04131e-07\n",
      "9418 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.35951e-07\n",
      "9419 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.97349e-07\n",
      "9420 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.7439e-07\n",
      "9421 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.78178e-07\n",
      "9422 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.12222e-07\n",
      "9423 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.77455e-07\n",
      "9424 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.38759e-07\n",
      "9425 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.6747e-07\n",
      "9426 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.24742e-07\n",
      "9427 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.32042e-07\n",
      "9428 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 3.25685e-07\n",
      "9429 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.67487e-07\n",
      "9430 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.55047e-07\n",
      "9431 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 4.47896e-07\n",
      "9432 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 3.74649e-07\n",
      "9433 Train total loss: 2.97582 \tReconstruction loss: 2.97582 \tLatent loss: 4.76593e-07\n",
      "9434 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 6.08021e-07\n",
      "9435 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.69523e-07\n",
      "9436 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.42874e-07\n",
      "9437 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.04605e-07\n",
      "9438 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 4.60802e-07\n",
      "9439 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.95982e-07\n",
      "9440 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.07056e-07\n",
      "9441 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 4.68324e-07\n",
      "9442 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.60503e-07\n",
      "9443 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.95918e-07\n",
      "9444 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.29027e-07\n",
      "9445 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.02429e-07\n",
      "9446 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 2.14337e-07\n",
      "9447 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.46526e-07\n",
      "9448 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 4.79298e-07\n",
      "9449 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.33573e-07\n",
      "9450 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.40455e-07\n",
      "9451 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.71235e-07\n",
      "9452 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.78778e-07\n",
      "9453 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 2.35373e-07\n",
      "9454 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 5.41843e-07\n",
      "9455 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.70476e-07\n",
      "9456 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.24883e-07\n",
      "9457 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.71064e-08\n",
      "9458 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.81482e-08\n",
      "9459 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.29147e-07\n",
      "9460 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.51033e-07\n",
      "9461 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 5.51212e-08\n",
      "9462 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.81946e-07\n",
      "9463 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.02084e-07\n",
      "9464 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.22308e-07\n",
      "9465 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.16727e-07\n",
      "9466 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.44518e-07\n",
      "9467 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.04665e-07\n",
      "9468 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.95819e-07\n",
      "9469 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 6.12781e-08\n",
      "9470 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.43127e-07\n",
      "9471 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.37331e-07\n",
      "9472 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.33599e-07\n",
      "9473 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.85063e-07\n",
      "9474 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.16273e-07\n",
      "9475 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.87113e-07\n",
      "9476 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.3995e-07\n",
      "9477 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.17243e-06\n",
      "9478 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 3.90866e-07\n",
      "9479 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 5.94977e-07\n",
      "9480 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.68732e-07\n",
      "9481 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.01843e-07\n",
      "9482 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.64517e-07\n",
      "9483 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.39824e-07\n",
      "9484 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.07363e-07\n",
      "9485 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.32481e-07\n",
      "9486 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 6.21512e-07\n",
      "9487 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.02667e-07\n",
      "9488 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.33341e-07\n",
      "9489 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.92153e-07\n",
      "9490 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.76212e-07\n",
      "9491 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.27828e-06\n",
      "9492 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 3.1504e-07\n",
      "9493 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.38427e-07\n",
      "9494 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.56143e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9495 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.86014e-07\n",
      "9496 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.08132e-07\n",
      "9497 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 8.58985e-08\n",
      "9498 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.16041e-07\n",
      "9499 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.65944e-07\n",
      "9500 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 1.64079e-07\n",
      "9501 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 7.62826e-08\n",
      "9502 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.50608e-07\n",
      "9503 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.5831e-07\n",
      "9504 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.69163e-07\n",
      "9505 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.6594e-07\n",
      "9506 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 9.8499e-08\n",
      "9507 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 3.55688e-07\n",
      "9508 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.43232e-07\n",
      "9509 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.64532e-07\n",
      "9510 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.81652e-07\n",
      "9511 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 3.15584e-07\n",
      "9512 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.40509e-06\n",
      "9513 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.26669e-07\n",
      "9514 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.15875e-07\n",
      "9515 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.22731e-07\n",
      "9516 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.72407e-07\n",
      "9517 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.89021e-08\n",
      "9518 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.71964e-07\n",
      "9519 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.58109e-07\n",
      "9520 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.92254e-07\n",
      "9521 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.46589e-07\n",
      "9522 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.59863e-07\n",
      "9523 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.57596e-07\n",
      "9524 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.09715e-07\n",
      "9525 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.96682e-07\n",
      "9526 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.70677e-06\n",
      "9527 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.90651e-07\n",
      "9528 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 5.88152e-07\n",
      "9529 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 7.5257e-07\n",
      "9530 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.475e-07\n",
      "9531 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.32002e-07\n",
      "9532 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 5.79811e-07\n",
      "9533 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 5.13115e-07\n",
      "9534 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.12509e-07\n",
      "9535 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.56322e-07\n",
      "9536 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.20739e-07\n",
      "9537 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 8.91698e-08\n",
      "9538 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 9.0299e-08\n",
      "9539 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.12323e-07\n",
      "9540 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 8.74119e-08\n",
      "9541 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 6.19938e-07\n",
      "9542 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.72046e-07\n",
      "9543 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 6.64222e-08\n",
      "9544 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 8.72213e-08\n",
      "9545 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 6.65299e-08\n",
      "9546 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.46995e-07\n",
      "9547 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.559e-07\n",
      "9548 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 6.75223e-08\n",
      "9549 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.2953e-07\n",
      "9550 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.42858e-07\n",
      "9551 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.92892e-07\n",
      "9552 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 4.40209e-07\n",
      "9553 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.3791e-07\n",
      "9554 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.22689e-07\n",
      "9555 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.40337e-07\n",
      "9556 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 5.87752e-08\n",
      "9557 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.73887e-07\n",
      "9558 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 2.98329e-07\n",
      "9559 Train total loss: 2.80521 \tReconstruction loss: 2.8052 \tLatent loss: 2.85094e-07\n",
      "9560 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.35011e-07\n",
      "9561 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 6.85017e-08\n",
      "9562 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.35791e-07\n",
      "9563 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.53455e-07\n",
      "9564 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.15185e-07\n",
      "9565 Train total loss: 2.91475 \tReconstruction loss: 2.91474 \tLatent loss: 4.30369e-07\n",
      "9566 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 2.38112e-07\n",
      "9567 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.73078e-07\n",
      "9568 Train total loss: 2.8542 \tReconstruction loss: 2.8542 \tLatent loss: 5.63162e-07\n",
      "9569 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.52443e-07\n",
      "9570 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.87649e-07\n",
      "9571 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.23508e-06\n",
      "9572 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 7.93595e-07\n",
      "9573 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.85679e-07\n",
      "9574 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 2.02497e-07\n",
      "9575 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 2.63478e-07\n",
      "9576 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.84144e-07\n",
      "9577 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.34756e-07\n",
      "9578 Train total loss: 2.77425 \tReconstruction loss: 2.77425 \tLatent loss: 1.50989e-07\n",
      "9579 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.66517e-07\n",
      "9580 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.1875e-06\n",
      "9581 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 3.81605e-07\n",
      "9582 Train total loss: 2.83695 \tReconstruction loss: 2.83694 \tLatent loss: 3.08837e-06\n",
      "9583 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.98208e-06\n",
      "9584 Train total loss: 3.09076 \tReconstruction loss: 3.09076 \tLatent loss: 2.00892e-07\n",
      "9585 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.25432e-07\n",
      "9586 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 5.58313e-08\n",
      "9587 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 4.96715e-08\n",
      "9588 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.22821e-08\n",
      "9589 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 9.98349e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590 Train total loss: 2.98598 \tReconstruction loss: 2.98598 \tLatent loss: 6.06888e-08\n",
      "9591 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.44515e-07\n",
      "9592 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.01784e-07\n",
      "9593 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.25173e-07\n",
      "9594 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.57812e-07\n",
      "9595 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 6.70894e-07\n",
      "9596 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.04734e-07\n",
      "9597 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.20043e-07\n",
      "9598 Train total loss: 2.77431 \tReconstruction loss: 2.7743 \tLatent loss: 1.20337e-05\n",
      "9599 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.94713e-06\n",
      "9600 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 2.34916e-07\n",
      "9601 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.70983e-07\n",
      "9602 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 6.14309e-08\n",
      "9603 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.37764e-08\n",
      "9604 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.49784e-08\n",
      "9605 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.18832e-08\n",
      "9606 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 5.23563e-08\n",
      "9607 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 4.17e-08\n",
      "9608 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.4427e-08\n",
      "9609 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 3.77258e-08\n",
      "9610 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.63142e-08\n",
      "9611 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 5.98069e-08\n",
      "9612 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.18006e-07\n",
      "9613 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.19989e-07\n",
      "9614 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.08601e-07\n",
      "9615 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.68266e-08\n",
      "9616 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.84069e-08\n",
      "9617 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 9.7329e-08\n",
      "9618 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 5.97414e-08\n",
      "9619 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.0459e-07\n",
      "9620 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 7.08023e-08\n",
      "9621 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 4.15035e-08\n",
      "9622 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.62082e-08\n",
      "9623 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.20347e-08\n",
      "9624 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.01766e-07\n",
      "9625 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.02492e-07\n",
      "9626 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 9.3237e-08\n",
      "9627 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 6.75704e-08\n",
      "9628 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 5.34666e-08\n",
      "9629 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.01358e-07\n",
      "9630 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.09532e-07\n",
      "9631 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 9.09378e-08\n",
      "9632 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 9.89807e-08\n",
      "9633 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.86691e-08\n",
      "9634 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 4.91561e-07\n",
      "9635 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 1.71727e-07\n",
      "9636 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.09449e-07\n",
      "9637 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 1.22008e-07\n",
      "9638 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 9.29009e-08\n",
      "9639 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.20042e-07\n",
      "9640 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.55141e-07\n",
      "9641 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 3.96729e-08\n",
      "9642 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 5.29501e-08\n",
      "9643 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 5.20507e-08\n",
      "9644 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 6.74379e-08\n",
      "9645 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.20816e-08\n",
      "9646 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 9.63206e-08\n",
      "9647 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.68302e-07\n",
      "9648 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.03349e-07\n",
      "9649 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.77822e-07\n",
      "9650 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 8.64064e-08\n",
      "9651 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.86119e-07\n",
      "9652 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.95562e-07\n",
      "9653 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.2238e-07\n",
      "9654 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.19819e-07\n",
      "9655 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 9.55071e-08\n",
      "9656 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.09116e-07\n",
      "9657 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.17256e-07\n",
      "9658 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.83718e-07\n",
      "9659 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 6.59634e-07\n",
      "9660 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.79266e-07\n",
      "9661 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.36545e-07\n",
      "9662 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.06342e-07\n",
      "9663 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.53431e-07\n",
      "9664 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.52351e-07\n",
      "9665 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.63011e-07\n",
      "9666 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.08583e-07\n",
      "9667 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 2.45546e-07\n",
      "9668 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.14022e-07\n",
      "9669 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.79964e-07\n",
      "9670 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.49361e-07\n",
      "9671 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.8435e-07\n",
      "9672 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.51862e-07\n",
      "9673 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 9.28922e-08\n",
      "9674 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 2.38484e-07\n",
      "9675 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.07923e-08\n",
      "9676 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.62142e-07\n",
      "9677 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.89508e-07\n",
      "9678 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.12952e-07\n",
      "9679 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.12011e-07\n",
      "9680 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 7.87026e-08\n",
      "9681 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 5.51881e-08\n",
      "9682 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.67706e-07\n",
      "9683 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 8.76651e-08\n",
      "9684 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.95806e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9685 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.01578e-08\n",
      "9686 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 6.4474e-07\n",
      "9687 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.56135e-07\n",
      "9688 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.81317e-07\n",
      "9689 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.64956e-07\n",
      "9690 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.82344e-07\n",
      "9691 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.94852e-07\n",
      "9692 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.88105e-07\n",
      "9693 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 7.14998e-06\n",
      "9694 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 5.75316e-07\n",
      "9695 Train total loss: 2.79368 \tReconstruction loss: 2.79367 \tLatent loss: 1.86921e-07\n",
      "9696 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.83403e-08\n",
      "9697 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.10522e-07\n",
      "9698 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 8.15678e-08\n",
      "9699 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.35314e-07\n",
      "9700 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.15883e-07\n",
      "9701 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 7.39223e-08\n",
      "9702 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 6.43515e-08\n",
      "9703 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.16907e-07\n",
      "9704 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.28481e-07\n",
      "9705 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.36154e-07\n",
      "9706 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.85601e-07\n",
      "9707 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.11736e-08\n",
      "9708 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.09298e-07\n",
      "9709 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.29612e-07\n",
      "9710 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.27831e-07\n",
      "9711 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 1.38383e-07\n",
      "9712 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.08099e-07\n",
      "9713 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.62365e-07\n",
      "9714 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.51023e-07\n",
      "9715 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.71142e-07\n",
      "9716 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.32785e-07\n",
      "9717 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.11175e-07\n",
      "9718 Train total loss: 2.77428 \tReconstruction loss: 2.77428 \tLatent loss: 1.31025e-07\n",
      "9719 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.97225e-07\n",
      "9720 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.24431e-07\n",
      "9721 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 2.3803e-07\n",
      "9722 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.35745e-07\n",
      "9723 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 4.22017e-07\n",
      "9724 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.64953e-07\n",
      "9725 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.27002e-06\n",
      "9726 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.63033e-07\n",
      "9727 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.96821e-08\n",
      "9728 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.45566e-07\n",
      "9729 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 9.40522e-07\n",
      "9730 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 3.14201e-07\n",
      "9731 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.76144e-07\n",
      "9732 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.79396e-07\n",
      "9733 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.78067e-07\n",
      "9734 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.97374e-08\n",
      "9735 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 8.83141e-08\n",
      "9736 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 8.45568e-08\n",
      "9737 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.0409e-07\n",
      "9738 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 4.23481e-07\n",
      "9739 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.27189e-07\n",
      "9740 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 7.2323e-08\n",
      "9741 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 8.19258e-08\n",
      "9742 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 6.86821e-08\n",
      "9743 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.03781e-07\n",
      "9744 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.32118e-07\n",
      "9745 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.04749e-07\n",
      "9746 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.67187e-07\n",
      "9747 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.2673e-07\n",
      "9748 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 4.81392e-08\n",
      "9749 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 8.12332e-08\n",
      "9750 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.75854e-07\n",
      "9751 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.92792e-07\n",
      "9752 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.64135e-07\n",
      "9753 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.52858e-07\n",
      "9754 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 1.54872e-07\n",
      "9755 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.41523e-07\n",
      "9756 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.0204e-07\n",
      "9757 Train total loss: 2.79716 \tReconstruction loss: 2.79716 \tLatent loss: 4.45016e-07\n",
      "9758 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 8.25738e-07\n",
      "9759 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.55596e-06\n",
      "9760 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.78842e-07\n",
      "9761 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.59524e-07\n",
      "9762 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.05392e-07\n",
      "9763 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.71812e-07\n",
      "9764 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.86092e-07\n",
      "9765 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.21026e-07\n",
      "9766 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.88998e-07\n",
      "9767 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.03135e-08\n",
      "9768 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.07397e-07\n",
      "9769 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.24617e-07\n",
      "9770 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.27779e-07\n",
      "9771 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.35338e-07\n",
      "9772 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.68497e-07\n",
      "9773 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.7941e-07\n",
      "9774 Train total loss: 3.02182 \tReconstruction loss: 3.02182 \tLatent loss: 3.37091e-07\n",
      "9775 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 2.70086e-07\n",
      "9776 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.99475e-07\n",
      "9777 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.06324e-07\n",
      "9778 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.43409e-07\n",
      "9779 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.3053e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9780 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 3.22906e-07\n",
      "9781 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 9.59303e-07\n",
      "9782 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.08682e-07\n",
      "9783 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.0534e-07\n",
      "9784 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 8.15897e-08\n",
      "9785 Train total loss: 2.91476 \tReconstruction loss: 2.91476 \tLatent loss: 7.25428e-08\n",
      "9786 Train total loss: 2.80908 \tReconstruction loss: 2.80907 \tLatent loss: 1.62305e-07\n",
      "9787 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.12105e-07\n",
      "9788 Train total loss: 2.85422 \tReconstruction loss: 2.85421 \tLatent loss: 1.92324e-07\n",
      "9789 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 2.14581e-06\n",
      "9790 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.5629e-07\n",
      "9791 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 8.11997e-08\n",
      "9792 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.25177e-07\n",
      "9793 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 1.24359e-07\n",
      "9794 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 9.46238e-08\n",
      "9795 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 8.04008e-08\n",
      "9796 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.87463e-08\n",
      "9797 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.51765e-07\n",
      "9798 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.52352e-07\n",
      "9799 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.86051e-07\n",
      "9800 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 8.15271e-08\n",
      "9801 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 5.15443e-08\n",
      "9802 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 3.07712e-07\n",
      "9803 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 2.58585e-07\n",
      "9804 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.4189e-07\n",
      "9805 Train total loss: 2.91475 \tReconstruction loss: 2.91475 \tLatent loss: 1.75161e-07\n",
      "9806 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.42593e-07\n",
      "9807 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.74857e-08\n",
      "9808 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 8.44011e-08\n",
      "9809 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.36258e-07\n",
      "9810 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 8.27771e-08\n",
      "9811 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.00539e-07\n",
      "9812 Train total loss: 2.9284 \tReconstruction loss: 2.92839 \tLatent loss: 4.29436e-07\n",
      "9813 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.34905e-07\n",
      "9814 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.35547e-07\n",
      "9815 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 9.44186e-08\n",
      "9816 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.7201e-07\n",
      "9817 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.36506e-07\n",
      "9818 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 3.19548e-07\n",
      "9819 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.04563e-07\n",
      "9820 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 2.50472e-07\n",
      "9821 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 2.49133e-07\n",
      "9822 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 7.80066e-07\n",
      "9823 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 2.17135e-07\n",
      "9824 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.34556e-07\n",
      "9825 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.35311e-07\n",
      "9826 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.84203e-07\n",
      "9827 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 2.39756e-07\n",
      "9828 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.95413e-07\n",
      "9829 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.46718e-07\n",
      "9830 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.61229e-07\n",
      "9831 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.93277e-07\n",
      "9832 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 5.38715e-07\n",
      "9833 Train total loss: 2.97584 \tReconstruction loss: 2.97584 \tLatent loss: 2.63902e-07\n",
      "9834 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 2.04808e-07\n",
      "9835 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 3.20077e-07\n",
      "9836 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.47394e-07\n",
      "9837 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.52443e-07\n",
      "9838 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.02506e-07\n",
      "9839 Train total loss: 2.8052 \tReconstruction loss: 2.8052 \tLatent loss: 1.64349e-07\n",
      "9840 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.81214e-07\n",
      "9841 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.22823e-07\n",
      "9842 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.14679e-07\n",
      "9843 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.66683e-07\n",
      "9844 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.88209e-07\n",
      "9845 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.55546e-07\n",
      "9846 Train total loss: 2.80907 \tReconstruction loss: 2.80907 \tLatent loss: 1.63061e-07\n",
      "9847 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.73446e-07\n",
      "9848 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.7515e-07\n",
      "9849 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 8.81599e-08\n",
      "9850 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.93686e-07\n",
      "9851 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.68038e-07\n",
      "9852 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.33992e-07\n",
      "9853 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 7.69505e-08\n",
      "9854 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.81422e-07\n",
      "9855 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.65752e-07\n",
      "9856 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.67429e-07\n",
      "9857 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 5.4822e-07\n",
      "9858 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.34328e-07\n",
      "9859 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.87976e-07\n",
      "9860 Train total loss: 3.03978 \tReconstruction loss: 3.03978 \tLatent loss: 2.19286e-07\n",
      "9861 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.41478e-07\n",
      "9862 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.01749e-07\n",
      "9863 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 3.27188e-07\n",
      "9864 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.21365e-07\n",
      "9865 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 9.68357e-08\n",
      "9866 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.73945e-07\n",
      "9867 Train total loss: 2.92515 \tReconstruction loss: 2.92515 \tLatent loss: 1.87667e-07\n",
      "9868 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.89689e-07\n",
      "9869 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.54204e-07\n",
      "9870 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.24933e-07\n",
      "9871 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 2.91612e-07\n",
      "9872 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.50678e-07\n",
      "9873 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.74739e-07\n",
      "9874 Train total loss: 3.02181 \tReconstruction loss: 3.02181 \tLatent loss: 1.22056e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9875 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 3.7502e-07\n",
      "9876 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.64384e-07\n",
      "9877 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 4.01649e-07\n",
      "9878 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.88661e-07\n",
      "9879 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.83182e-07\n",
      "9880 Train total loss: 3.0398 \tReconstruction loss: 3.0398 \tLatent loss: 1.43696e-07\n",
      "9881 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.97339e-07\n",
      "9882 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.1917e-07\n",
      "9883 Train total loss: 2.86597 \tReconstruction loss: 2.86597 \tLatent loss: 1.87023e-07\n",
      "9884 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 3.2191e-07\n",
      "9885 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 3.0964e-07\n",
      "9886 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 1.87306e-07\n",
      "9887 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.32698e-07\n",
      "9888 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 1.9492e-07\n",
      "9889 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.48857e-07\n",
      "9890 Train total loss: 2.986 \tReconstruction loss: 2.986 \tLatent loss: 3.70563e-07\n",
      "9891 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.98183e-07\n",
      "9892 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.76555e-07\n",
      "9893 Train total loss: 2.97585 \tReconstruction loss: 2.97584 \tLatent loss: 2.4675e-07\n",
      "9894 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 6.44286e-07\n",
      "9895 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 4.94932e-07\n",
      "9896 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 1.66237e-07\n",
      "9897 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.29433e-07\n",
      "9898 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.31108e-07\n",
      "9899 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.19205e-06\n",
      "9900 Train total loss: 3.0398 \tReconstruction loss: 3.03979 \tLatent loss: 4.26156e-07\n",
      "9901 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.99932e-07\n",
      "9902 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.01201e-07\n",
      "9903 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.09406e-07\n",
      "9904 Train total loss: 3.09078 \tReconstruction loss: 3.09078 \tLatent loss: 2.30085e-07\n",
      "9905 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.66236e-07\n",
      "9906 Train total loss: 2.80906 \tReconstruction loss: 2.80906 \tLatent loss: 1.88488e-07\n",
      "9907 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.75618e-07\n",
      "9908 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 8.17148e-08\n",
      "9909 Train total loss: 2.85854 \tReconstruction loss: 2.85854 \tLatent loss: 1.34094e-07\n",
      "9910 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 6.1942e-07\n",
      "9911 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 2.19316e-07\n",
      "9912 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 1.72183e-07\n",
      "9913 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 3.65738e-07\n",
      "9914 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.84475e-07\n",
      "9915 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 6.7252e-07\n",
      "9916 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 5.95966e-07\n",
      "9917 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 2.96949e-07\n",
      "9918 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.24704e-07\n",
      "9919 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 4.16083e-07\n",
      "9920 Train total loss: 3.03981 \tReconstruction loss: 3.03981 \tLatent loss: 2.45297e-07\n",
      "9921 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 1.81892e-07\n",
      "9922 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.54789e-07\n",
      "9923 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 9.83404e-08\n",
      "9924 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 1.80959e-07\n",
      "9925 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 4.75568e-07\n",
      "9926 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 2.14038e-07\n",
      "9927 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 4.33055e-07\n",
      "9928 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.29106e-07\n",
      "9929 Train total loss: 2.85856 \tReconstruction loss: 2.85856 \tLatent loss: 1.59872e-07\n",
      "9930 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.76037e-07\n",
      "9931 Train total loss: 2.94645 \tReconstruction loss: 2.94645 \tLatent loss: 3.72672e-07\n",
      "9932 Train total loss: 2.92839 \tReconstruction loss: 2.92839 \tLatent loss: 2.96949e-07\n",
      "9933 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 1.82127e-07\n",
      "9934 Train total loss: 3.02179 \tReconstruction loss: 3.02179 \tLatent loss: 8.84742e-08\n",
      "9935 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.45962e-07\n",
      "9936 Train total loss: 2.77373 \tReconstruction loss: 2.77373 \tLatent loss: 3.72568e-07\n",
      "9937 Train total loss: 2.79714 \tReconstruction loss: 2.79714 \tLatent loss: 2.16387e-07\n",
      "9938 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.54313e-07\n",
      "9939 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 2.08593e-07\n",
      "9940 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.63604e-07\n",
      "9941 Train total loss: 2.80587 \tReconstruction loss: 2.80587 \tLatent loss: 8.33563e-08\n",
      "9942 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 2.26162e-07\n",
      "9943 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.67142e-07\n",
      "9944 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.01901e-07\n",
      "9945 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.74818e-07\n",
      "9946 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 3.33544e-07\n",
      "9947 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 9.96879e-08\n",
      "9948 Train total loss: 2.85422 \tReconstruction loss: 2.85422 \tLatent loss: 2.71865e-07\n",
      "9949 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 1.08442e-07\n",
      "9950 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 1.68447e-07\n",
      "9951 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.95521e-07\n",
      "9952 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.20312e-06\n",
      "9953 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 8.43981e-07\n",
      "9954 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.61251e-07\n",
      "9955 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 7.5897e-08\n",
      "9956 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 2.20429e-07\n",
      "9957 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.37741e-07\n",
      "9958 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.37166e-07\n",
      "9959 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.808e-07\n",
      "9960 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 1.30667e-07\n",
      "9961 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.21893e-07\n",
      "9962 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 9.18473e-08\n",
      "9963 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.46874e-07\n",
      "9964 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 2.00325e-07\n",
      "9965 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 2.26819e-07\n",
      "9966 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 4.284e-07\n",
      "9967 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 3.21084e-07\n",
      "9968 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 2.48905e-07\n",
      "9969 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.2086e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9970 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 2.77402e-07\n",
      "9971 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.10231e-07\n",
      "9972 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 1.3268e-07\n",
      "9973 Train total loss: 2.97583 \tReconstruction loss: 2.97583 \tLatent loss: 2.24708e-07\n",
      "9974 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 1.93839e-07\n",
      "9975 Train total loss: 2.79368 \tReconstruction loss: 2.79368 \tLatent loss: 9.20554e-08\n",
      "9976 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 8.52873e-08\n",
      "9977 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 1.82331e-07\n",
      "9978 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 1.66881e-07\n",
      "9979 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 3.62916e-07\n",
      "9980 Train total loss: 3.03979 \tReconstruction loss: 3.03979 \tLatent loss: 7.90285e-08\n",
      "9981 Train total loss: 2.80586 \tReconstruction loss: 2.80586 \tLatent loss: 1.5416e-07\n",
      "9982 Train total loss: 2.83694 \tReconstruction loss: 2.83694 \tLatent loss: 1.74496e-07\n",
      "9983 Train total loss: 2.86596 \tReconstruction loss: 2.86596 \tLatent loss: 1.84814e-07\n",
      "9984 Train total loss: 3.09077 \tReconstruction loss: 3.09077 \tLatent loss: 4.90005e-07\n",
      "9985 Train total loss: 2.91474 \tReconstruction loss: 2.91474 \tLatent loss: 1.86771e-07\n",
      "9986 Train total loss: 2.80908 \tReconstruction loss: 2.80908 \tLatent loss: 1.32063e-07\n",
      "9987 Train total loss: 2.92514 \tReconstruction loss: 2.92514 \tLatent loss: 1.87676e-07\n",
      "9988 Train total loss: 2.85421 \tReconstruction loss: 2.85421 \tLatent loss: 1.11932e-07\n",
      "9989 Train total loss: 2.85855 \tReconstruction loss: 2.85855 \tLatent loss: 2.24181e-07\n",
      "9990 Train total loss: 2.98599 \tReconstruction loss: 2.98599 \tLatent loss: 4.89711e-07\n",
      "9991 Train total loss: 2.94646 \tReconstruction loss: 2.94646 \tLatent loss: 1.63053e-07\n",
      "9992 Train total loss: 2.9284 \tReconstruction loss: 2.9284 \tLatent loss: 2.799e-07\n",
      "9993 Train total loss: 2.97584 \tReconstruction loss: 2.97583 \tLatent loss: 1.94054e-07\n",
      "9994 Train total loss: 3.0218 \tReconstruction loss: 3.0218 \tLatent loss: 7.00908e-08\n",
      "9995 Train total loss: 2.79367 \tReconstruction loss: 2.79367 \tLatent loss: 1.29208e-07\n",
      "9996 Train total loss: 2.77374 \tReconstruction loss: 2.77374 \tLatent loss: 3.67266e-07\n",
      "9997 Train total loss: 2.79715 \tReconstruction loss: 2.79715 \tLatent loss: 3.22712e-07\n",
      "9998 Train total loss: 2.77429 \tReconstruction loss: 2.77429 \tLatent loss: 2.45929e-07\n",
      "9999 Train total loss: 2.80521 \tReconstruction loss: 2.80521 \tLatent loss: 1.10849e-07\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "batch_size = 100\n",
    "n_faces = 100\n",
    "ff_reader = data_reader(ff, batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = ff_reader.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch = ff_reader.next_batch().astype(np.float32)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_val, reconstruction_loss_val, latent_loss_val = sess.run([cost,\n",
    "         reconstruction_loss, latent_loss], feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train total loss:\", loss_val / X_batch.shape[0],\n",
    "         \"\\tReconstruction loss:\", reconstruction_loss_val / X_batch.shape[0],\n",
    "          \"\\tLatent loss:\", latent_loss_val / X_batch.shape[0])\n",
    "    \n",
    "    # generating digits\n",
    "    codings_rnd = np.random.normal(size=[n_faces, n_hidden3])\n",
    "    outputs_val = outputs.eval(feed_dict={hidden3: codings_rnd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 13.49620819,   8.39196682,  22.28470612, ...,  44.02404022,\n",
       "          29.25331116,  18.60301781],\n",
       "        [ 12.38797951,   7.50776911,  22.57164955, ...,  22.14555931,\n",
       "          15.48232651,   4.06478071],\n",
       "        [ 36.60523224,  12.11432266,  12.59864712, ...,   5.95402908,\n",
       "          15.01677704,  13.85228252],\n",
       "        ..., \n",
       "        [ 62.53839493,  11.08270359,   5.9053998 , ...,  38.06702042,\n",
       "          22.3343277 ,  -3.1355617 ],\n",
       "        [  9.52103996,  14.96702385,  28.14153481, ...,   2.00306368,\n",
       "          10.19797611,   5.78598118],\n",
       "        [ 61.08483505,   4.42576313,   6.92634153, ...,   9.226017  ,\n",
       "          17.65270424,  23.27662849]],\n",
       "\n",
       "       [[  0.32231557,   4.32793379,   5.92375994, ...,   8.59908772,\n",
       "           9.1932497 ,   4.82758141],\n",
       "        [  2.81827641,  -0.92397451,  -2.2294929 , ...,   9.65980625,\n",
       "           2.90895605,   1.22812152],\n",
       "        [ 15.77140713,   0.59766448,   1.48807418, ...,   1.48326576,\n",
       "           2.0353229 ,   3.00262785],\n",
       "        ..., \n",
       "        [ 14.71610928,   5.51572466,   3.320014  , ...,  12.08424664,\n",
       "           3.64460182,   0.31750596],\n",
       "        [  2.42277527,  10.1381464 ,   9.96121407, ...,   0.92915994,\n",
       "           2.98139596,   2.69756961],\n",
       "        [ 16.88814163,  -0.69339156,   3.44741464, ...,   0.98672473,\n",
       "           2.58650064,   7.3928442 ]],\n",
       "\n",
       "       [[ 14.33849144,  14.94644165,  30.21382141, ...,  53.63539124,\n",
       "          41.21853638,  31.31706619],\n",
       "        [ 13.02887917,  -5.87618446, -15.0197525 , ...,  36.68484116,\n",
       "          37.0130043 ,  -2.79253817],\n",
       "        [ 65.5402298 ,   6.13478851,  24.55173492, ...,   9.89687634,\n",
       "          11.78360653,  25.13774872],\n",
       "        ..., \n",
       "        [ 77.53543854,   0.63366032,  30.24692535, ...,  48.41702271,\n",
       "           5.22532845,   7.56974697],\n",
       "        [  6.95213699,  39.01576996,  38.50658417, ...,  16.51955032,\n",
       "          36.47447968,  13.16081333],\n",
       "        [ 90.57801819,   3.19530964,  12.58292484, ...,  15.15588379,\n",
       "          49.53890991,  16.13121796]],\n",
       "\n",
       "       ..., \n",
       "       [[  1.09373105,   3.46410418,   6.11445045, ...,   9.30023289,\n",
       "           7.83453035,   3.35974503],\n",
       "        [  6.3837924 ,   1.24897337,   3.23636389, ...,   8.23586941,\n",
       "           3.85174704,  -2.76534891],\n",
       "        [ 11.20261574,   1.10804129,   2.95816565, ...,   4.23661423,\n",
       "           3.2274797 ,   2.41785598],\n",
       "        ..., \n",
       "        [ 20.33382797,   2.68968654,   0.16504091, ...,   7.25431824,\n",
       "           4.88595772,   1.69509625],\n",
       "        [  1.7410748 ,   3.24139977,   8.34293556, ...,   1.99051845,\n",
       "           4.21331501,   0.67001778],\n",
       "        [ 17.083992  ,  -2.06601882,  -0.61086386, ...,   7.92471981,\n",
       "           5.74479055,   3.55567527]],\n",
       "\n",
       "       [[ -1.38706875,  -4.99206638,  -6.19468975, ..., -13.14803505,\n",
       "         -12.92248917,  -7.9561758 ],\n",
       "        [ -7.04369164,  -2.70577049,  -1.74838805, ...,  -7.31811905,\n",
       "          -4.93509579,  -0.45955798],\n",
       "        [-16.7628727 ,  -2.21152639,  -2.96581411, ...,  -0.14770384,\n",
       "          -4.81446934,  -2.34544563],\n",
       "        ..., \n",
       "        [-22.48855591,  -3.81735444,  -4.77592659, ..., -15.91142368,\n",
       "          -6.54172564,  -3.24858928],\n",
       "        [ -0.74749458,  -9.55321503, -10.06148148, ...,  -2.76476932,\n",
       "          -5.73671913,  -2.09007835],\n",
       "        [-22.48642731,  -3.39678574,  -4.48209667, ...,  -4.57614136,\n",
       "          -9.38733196,  -7.53852081]],\n",
       "\n",
       "       [[  3.2170651 ,   2.84352779,  12.99876213, ...,  32.14235687,\n",
       "          17.78741455,  12.01233292],\n",
       "        [ 10.37003899,  -0.56636333,   6.35059881, ...,  23.03159332,\n",
       "          14.46610069,   4.46930695],\n",
       "        [ 29.66949463,   8.84158325,   7.68791103, ...,   2.4001503 ,\n",
       "           7.94044781,   5.29518986],\n",
       "        ..., \n",
       "        [ 48.34876633,  10.26702213,   0.3935895 , ...,  30.70071983,\n",
       "          15.90163136,   4.18654442],\n",
       "        [  2.37578821,  14.80141926,  22.69629097, ...,   2.51272178,\n",
       "           8.84301376,   4.34935617],\n",
       "        [ 52.02032852,   2.91563535,   4.99210072, ...,   9.19224739,\n",
       "          15.46245575,  16.02850723]]], dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_val.reshape((-1, img_rows, img_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.34962082e+01,   8.39196682e+00,   2.22847061e+01, ...,\n",
       "         -1.59085321e+01,  -1.52422237e+01,  -7.94355917e+00],\n",
       "       [  1.23879795e+01,   7.50776911e+00,   2.25716496e+01, ...,\n",
       "         -1.27879267e+01,  -1.03633480e+01,   3.12566757e-03],\n",
       "       [  3.66052322e+01,   1.21143227e+01,   1.25986471e+01, ...,\n",
       "          2.16905177e-01,  -3.74290466e+00,  -4.44116163e+00],\n",
       "       ..., \n",
       "       [ -1.87553997e+01,  -6.28125858e+00,  -6.61096513e-01, ...,\n",
       "          3.07007198e+01,   1.59016314e+01,   4.18654442e+00],\n",
       "       [ -4.33136761e-01,  -7.97760105e+00,  -9.96455002e+00, ...,\n",
       "          2.51272178e+00,   8.84301376e+00,   4.34935617e+00],\n",
       "       [ -2.12468204e+01,   2.27107957e-01,  -4.61287975e+00, ...,\n",
       "          9.19224739e+00,   1.54624557e+01,   1.60285072e+01]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f795d496eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_examples(outputs_val.reshape((-1, img_rows, img_cols)), n=100, n_cols=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1965000"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000*1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
