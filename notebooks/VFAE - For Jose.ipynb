{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructed_digits(X, outputs, model_path = None, n_test_digits = 2):\n",
    "    with tf.Session() as sess:\n",
    "        if model_path:\n",
    "            saver.restore(sess, model_path)\n",
    "        X_test = mnist.test.images[:n_test_digits]\n",
    "        outputs_val = outputs.eval(feed_dict={X: X_test})\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 3 * n_test_digits))\n",
    "    for digit_index in range(n_test_digits):\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "        plot_image(X_test[digit_index])\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "        plot_image(outputs_val[digit_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct the graph for the VFAE architecture:\n",
    "\n",
    "- Input: X = [X_without_s, s], where s is the sensitive feature\n",
    "- Middle Encodings: We're learning the parameters for the distribution of the encodings. What's different here is that we inject both the response y and the sensitive features in the middle layers.\n",
    "- Output: X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction phase\n",
    "n_s = 10 # number of sensitive features\n",
    "n_inputs = 28*28 - n_s # number of non-sensitive features\n",
    "\n",
    "# encoders\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 20 # codings\n",
    "n_hidden3 = 500\n",
    "n_hidden4 = 20\n",
    "\n",
    "# decoders\n",
    "n_hidden5 = 500\n",
    "n_hidden6 = 20\n",
    "n_hidden7 = 500\n",
    "\n",
    "# final output can take a random sample from the posterior\n",
    "n_outputs = n_inputs + n_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training rates\n",
    "alpha = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up the graph\n",
    "with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected],\n",
    "        activation_fn = tf.nn.elu,\n",
    "        weights_initializer = tf.contrib.layers.variance_scaling_initializer()):\n",
    "    X = tf.placeholder(tf.float32, shape = [None, n_inputs], name=\"X_wo_s\")\n",
    "    s = tf.placeholder(tf.float32, shape = [None, n_s], name=\"s\")\n",
    "    X_full = tf.concat([X,s], axis=1)\n",
    "    y = tf.placeholder(tf.int32, shape = [None, 1], name=\"y\") # for your example, switch this to tf.float32 bc you'll be doing reg\n",
    "    is_unlabelled = tf.placeholder(tf.bool, shape=(), name='is_training') # don't worry about this\n",
    "    with tf.name_scope(\"X_encoder\"):\n",
    "        hidden1 = fully_connected(tf.concat([X, s], axis=1), n_hidden1)\n",
    "        hidden2_mean = fully_connected(hidden1, n_hidden2, activation_fn = None)\n",
    "        hidden2_gamma = fully_connected(hidden1, n_hidden2, activation_fn = None)\n",
    "        hidden2_sigma = tf.exp(0.5 * hidden2_gamma)\n",
    "    noise1 = tf.random_normal(tf.shape(hidden2_sigma), dtype=tf.float32)\n",
    "    hidden2 = hidden2_mean + hidden2_sigma * noise1         # z1\n",
    "    with tf.name_scope(\"Z1_encoder\"):\n",
    "        hidden3_ygz1 = fully_connected(hidden2, n_hidden4, activation_fn = tf.nn.tanh)\n",
    "        hidden4_softmax_mean = fully_connected(hidden3_ygz1, 10, activation_fn = tf.nn.softmax)\n",
    "        if is_unlabelled == True:\n",
    "            # impute by sampling from q(y|z1)\n",
    "            y = tf.assign(y, tf.multinomial(hidden4_softmax_mean, 1,\n",
    "                                output_type = tf.int32))\n",
    "        hidden3 = fully_connected(tf.concat([hidden2, tf.cast(y, tf.float32)], axis=1),\n",
    "                        n_hidden3, activation_fn=tf.nn.tanh)\n",
    "        hidden4_mean = fully_connected(hidden3, n_hidden4, activation_fn = None)\n",
    "        hidden4_gamma = fully_connected(hidden3, n_hidden4, activation_fn = None)\n",
    "        hidden4_sigma = tf.exp(0.5 * hidden4_gamma)\n",
    "    noise2 = tf.random_normal(tf.shape(hidden4_sigma), dtype=tf.float32)\n",
    "    hidden4 = hidden4_mean + hidden4_sigma * noise2     # z2\n",
    "    with tf.name_scope(\"Z1_decoder\"):\n",
    "        hidden5 = fully_connected(tf.concat([hidden4, tf.cast(y, tf.float32)], axis=1 ),\n",
    "                    n_hidden5, activation_fn = tf.nn.tanh)\n",
    "        hidden6_mean = fully_connected(hidden5, n_hidden6, activation_fn = None)\n",
    "        hidden6_gamma = fully_connected(hidden5, n_hidden6, activation_fn = None)\n",
    "        hidden6_sigma = tf.exp(0.5 * hidden6_gamma)\n",
    "    noise3 = tf.random_normal(tf.shape(hidden6_sigma), dtype=tf.float32)\n",
    "    hidden6 = hidden6_mean + hidden6_sigma * noise3     # z1 (decoded)\n",
    "    with tf.name_scope(\"X_decoder\"):\n",
    "        hidden7 = fully_connected(tf.concat([hidden6, s], axis=1), n_hidden7,\n",
    "                                 activation_fn = tf.nn.tanh)\n",
    "        hidden8 = fully_connected(hidden7, n_outputs, activation_fn = None)\n",
    "    outputs = tf.sigmoid(hidden8, name=\"decoded_X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected lower bound\n",
    "with tf.name_scope(\"ELB\"):\n",
    "    kl_z2 = 0.5 * tf.reduce_sum(\n",
    "                    tf.exp(hidden4_gamma)\n",
    "                    + tf.square(hidden4_mean)\n",
    "                    - 1\n",
    "                    - hidden4_gamma\n",
    "                    )\n",
    "\n",
    "    kl_z1 = 0.5 * (tf.reduce_sum(\n",
    "                    (1 / (1e-10 + tf.exp(hidden6_gamma))) * tf.exp(hidden2_gamma)\n",
    "                    - 1\n",
    "                    + hidden6_gamma\n",
    "                    - hidden2_gamma\n",
    "                    ) + tf.einsum('ij,ji -> i', # this might not work for you depending on version of tflow\n",
    "                        (hidden6_mean-hidden2_mean) * (1 / (1e-10 + tf.exp(hidden6_gamma))),\n",
    "                        tf.transpose((hidden6_mean-hidden2_mean))))\n",
    "\n",
    "    indices = tf.range(tf.shape(y)[0])\n",
    "    indices = tf.concat([indices[:, tf.newaxis], y], axis=1)\n",
    "    eps = 1e-10\n",
    "    log_q_y_z1 = tf.reduce_sum(tf.log(eps + tf.gather_nd(hidden4_softmax_mean, indices)))\n",
    "\n",
    "    # Bernoulli log-likelihood\n",
    "    reconstruction_loss = -(tf.reduce_sum(X_full * tf.log(outputs)\n",
    "                            + (1 - X_full) * tf.log(1 - outputs)))\n",
    "    cost = kl_z2 + kl_z1 + reconstruction_loss + alpha * log_q_y_z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Graph & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train total loss: 15808.6 \tReconstruction loss: 16510.4 \tKL-z1: 544.288 \tKL-z2: 767.143 \tlog_q(y|z1): -2013.19\n",
      "1 Train total loss: 14580.0 \tReconstruction loss: 15223.1 \tKL-z1: 548.365 \tKL-z2: 786.87 \tlog_q(y|z1): -1978.38\n",
      "2 Train total loss: 13775.5 \tReconstruction loss: 14337.7 \tKL-z1: 552.086 \tKL-z2: 842.155 \tlog_q(y|z1): -1956.4\n",
      "3 Train total loss: 13613.7 \tReconstruction loss: 14258.4 \tKL-z1: 563.264 \tKL-z2: 840.809 \tlog_q(y|z1): -2048.83\n",
      "4 Train total loss: 12805.3 \tReconstruction loss: 13532.6 \tKL-z1: 485.266 \tKL-z2: 836.376 \tlog_q(y|z1): -2049.03\n",
      "5 Train total loss: 12624.4 \tReconstruction loss: 13176.3 \tKL-z1: 504.406 \tKL-z2: 877.674 \tlog_q(y|z1): -1933.99\n",
      "6 Train total loss: 11958.7 \tReconstruction loss: 12733.8 \tKL-z1: 474.23 \tKL-z2: 845.952 \tlog_q(y|z1): -2095.22\n",
      "7 Train total loss: 11915.2 \tReconstruction loss: 12431.5 \tKL-z1: 606.83 \tKL-z2: 833.997 \tlog_q(y|z1): -1957.11\n",
      "8 Train total loss: 12707.2 \tReconstruction loss: 13417.9 \tKL-z1: 550.244 \tKL-z2: 857.368 \tlog_q(y|z1): -2118.3\n",
      "9 Train total loss: 12194.7 \tReconstruction loss: 12970.5 \tKL-z1: 436.428 \tKL-z2: 883.124 \tlog_q(y|z1): -2095.3\n",
      "10 Train total loss: 12542.4 \tReconstruction loss: 13201.6 \tKL-z1: 513.078 \tKL-z2: 900.041 \tlog_q(y|z1): -2072.29\n",
      "11 Train total loss: 11871.0 \tReconstruction loss: 12475.6 \tKL-z1: 503.172 \tKL-z2: 895.458 \tlog_q(y|z1): -2003.22\n",
      "12 Train total loss: 12076.7 \tReconstruction loss: 12781.3 \tKL-z1: 465.89 \tKL-z2: 878.845 \tlog_q(y|z1): -2049.28\n",
      "13 Train total loss: 11972.4 \tReconstruction loss: 12641.2 \tKL-z1: 447.913 \tKL-z2: 932.53 \tlog_q(y|z1): -2049.29\n",
      "14 Train total loss: 12224.6 \tReconstruction loss: 13063.0 \tKL-z1: 431.989 \tKL-z2: 871.024 \tlog_q(y|z1): -2141.39\n",
      "15 Train total loss: 12102.9 \tReconstruction loss: 12755.4 \tKL-z1: 489.554 \tKL-z2: 930.334 \tlog_q(y|z1): -2072.32\n",
      "16 Train total loss: 12242.2 \tReconstruction loss: 13103.6 \tKL-z1: 406.573 \tKL-z2: 919.451 \tlog_q(y|z1): -2187.45\n",
      "17 Train total loss: 12035.5 \tReconstruction loss: 12708.1 \tKL-z1: 442.737 \tKL-z2: 887.884 \tlog_q(y|z1): -2003.24\n",
      "18 Train total loss: 11483.8 \tReconstruction loss: 12144.8 \tKL-z1: 442.23 \tKL-z2: 876.983 \tlog_q(y|z1): -1980.22\n",
      "19 Train total loss: 11300.8 \tReconstruction loss: 11963.8 \tKL-z1: 441.635 \tKL-z2: 898.589 \tlog_q(y|z1): -2003.25\n",
      "20 Train total loss: 11800.9 \tReconstruction loss: 12584.6 \tKL-z1: 410.764 \tKL-z2: 900.88 \tlog_q(y|z1): -2095.35\n",
      "21 Train total loss: 11348.2 \tReconstruction loss: 11994.0 \tKL-z1: 423.236 \tKL-z2: 888.093 \tlog_q(y|z1): -1957.2\n",
      "22 Train total loss: 11102.8 \tReconstruction loss: 11811.4 \tKL-z1: 452.28 \tKL-z2: 911.403 \tlog_q(y|z1): -2072.33\n",
      "23 Train total loss: 11235.7 \tReconstruction loss: 11977.3 \tKL-z1: 366.715 \tKL-z2: 895.008 \tlog_q(y|z1): -2003.25\n",
      "24 Train total loss: 11587.3 \tReconstruction loss: 12512.8 \tKL-z1: 362.16 \tKL-z2: 899.765 \tlog_q(y|z1): -2187.45\n",
      "25 Train total loss: 11251.7 \tReconstruction loss: 11949.5 \tKL-z1: 404.47 \tKL-z2: 900.986 \tlog_q(y|z1): -2003.25\n",
      "26 Train total loss: 11363.5 \tReconstruction loss: 12106.2 \tKL-z1: 369.752 \tKL-z2: 913.838 \tlog_q(y|z1): -2026.27\n",
      "27 Train total loss: 11698.9 \tReconstruction loss: 12426.6 \tKL-z1: 432.647 \tKL-z2: 865.974 \tlog_q(y|z1): -2026.27\n",
      "28 Train total loss: 11329.5 \tReconstruction loss: 11923.6 \tKL-z1: 477.538 \tKL-z2: 885.562 \tlog_q(y|z1): -1957.2\n",
      "29 Train total loss: 12167.3 \tReconstruction loss: 12879.0 \tKL-z1: 464.003 \tKL-z2: 896.613 \tlog_q(y|z1): -2072.33\n",
      "30 Train total loss: 11247.1 \tReconstruction loss: 11888.4 \tKL-z1: 441.533 \tKL-z2: 920.416 \tlog_q(y|z1): -2003.25\n",
      "31 Train total loss: 11813.7 \tReconstruction loss: 12492.1 \tKL-z1: 463.269 \tKL-z2: 884.585 \tlog_q(y|z1): -2026.27\n",
      "32 Train total loss: 11350.4 \tReconstruction loss: 12129.9 \tKL-z1: 411.455 \tKL-z2: 904.394 \tlog_q(y|z1): -2095.35\n",
      "33 Train total loss: 12326.9 \tReconstruction loss: 13099.4 \tKL-z1: 442.528 \tKL-z2: 972.397 \tlog_q(y|z1): -2187.46\n",
      "34 Train total loss: 11054.8 \tReconstruction loss: 11791.0 \tKL-z1: 454.253 \tKL-z2: 904.922 \tlog_q(y|z1): -2095.35\n",
      "35 Train total loss: 11613.0 \tReconstruction loss: 12357.6 \tKL-z1: 379.79 \tKL-z2: 947.891 \tlog_q(y|z1): -2072.33\n",
      "36 Train total loss: 11306.7 \tReconstruction loss: 12053.0 \tKL-z1: 440.073 \tKL-z2: 908.976 \tlog_q(y|z1): -2095.35\n",
      "37 Train total loss: 11419.9 \tReconstruction loss: 12130.2 \tKL-z1: 463.804 \tKL-z2: 898.219 \tlog_q(y|z1): -2072.33\n",
      "38 Train total loss: 11904.4 \tReconstruction loss: 12697.0 \tKL-z1: 404.371 \tKL-z2: 921.429 \tlog_q(y|z1): -2118.38\n",
      "39 Train total loss: 12091.7 \tReconstruction loss: 12892.6 \tKL-z1: 419.112 \tKL-z2: 898.424 \tlog_q(y|z1): -2118.38\n",
      "40 Train total loss: 11470.4 \tReconstruction loss: 12115.1 \tKL-z1: 523.732 \tKL-z2: 903.862 \tlog_q(y|z1): -2072.33\n",
      "41 Train total loss: 10988.4 \tReconstruction loss: 11812.7 \tKL-z1: 375.982 \tKL-z2: 895.0 \tlog_q(y|z1): -2095.35\n",
      "42 Train total loss: 11116.6 \tReconstruction loss: 12106.4 \tKL-z1: 377.183 \tKL-z2: 924.07 \tlog_q(y|z1): -2291.12\n",
      "43 Train total loss: 11705.2 \tReconstruction loss: 12529.6 \tKL-z1: 534.664 \tKL-z2: 938.507 \tlog_q(y|z1): -2297.53\n",
      "44 Train total loss: 10470.2 \tReconstruction loss: 11375.1 \tKL-z1: 430.108 \tKL-z2: 944.507 \tlog_q(y|z1): -2279.56\n",
      "45 Train total loss: 11925.1 \tReconstruction loss: 12826.0 \tKL-z1: 482.043 \tKL-z2: 919.634 \tlog_q(y|z1): -2302.56\n",
      "46 Train total loss: 10482.1 \tReconstruction loss: 11527.7 \tKL-z1: 346.24 \tKL-z2: 910.471 \tlog_q(y|z1): -2302.29\n",
      "47 Train total loss: 10779.0 \tReconstruction loss: 11674.1 \tKL-z1: 443.438 \tKL-z2: 963.965 \tlog_q(y|z1): -2302.58\n",
      "48 Train total loss: 10585.6 \tReconstruction loss: 11586.4 \tKL-z1: 392.708 \tKL-z2: 909.074 \tlog_q(y|z1): -2302.58\n",
      "49 Train total loss: 10686.3 \tReconstruction loss: 11671.9 \tKL-z1: 402.437 \tKL-z2: 914.562 \tlog_q(y|z1): -2302.58\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_digits = 60\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch[:,:-n_s],\n",
    "                                    s: X_batch[:,-n_s:],\n",
    "                                    y: y_batch[:,np.newaxis],\n",
    "                                    is_unlabelled: False})\n",
    "        kl_z2_val, kl_z1_val, log_q_y_z1_val, reconstruction_loss_val, loss_val = sess.run([\n",
    "                kl_z2,\n",
    "                kl_z1,\n",
    "                log_q_y_z1,\n",
    "                reconstruction_loss,\n",
    "                cost],\n",
    "                feed_dict={X: X_batch[:,:-n_s],\n",
    "                        s: X_batch[:,-n_s:],\n",
    "                        y: y_batch[:,np.newaxis]})\n",
    "        print(\"\\r{}\".format(epoch), \"Train total loss:\", loss_val,\n",
    "         \"\\tReconstruction loss:\", reconstruction_loss_val,\n",
    "          \"\\tKL-z1:\", kl_z1_val,\n",
    "          \"\\tKL-z2:\", kl_z2_val,\n",
    "          \"\\tlog_q(y|z1):\", log_q_y_z1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
